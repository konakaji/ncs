{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -1.0429962745401\n"
     ]
    }
   ],
   "source": [
    "from gqe.operator_pool.uccsd import UCCSD, generate_molecule\n",
    "# distances = [0.5, 0.6, 0.7, 0.7414, 0.8, 0.9, 1.0, 1.5, 2.0]\n",
    "distance = 0.5\n",
    "transformation = 'jordan-wigner'\n",
    "is_bravyi = transformation == 'bravyi-kitaev'\n",
    "molecule = generate_molecule(\"H\", \"H\", distance, \"sto-3g\", bravyi_kitaev=is_bravyi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50257,\n",
       " 'n_positions': 1024,\n",
       " 'n_embd': 768,\n",
       " 'n_layer': 12,\n",
       " 'n_head': 12,\n",
       " 'n_inner': None,\n",
       " 'activation_function': 'gelu_new',\n",
       " 'resid_pdrop': 0.1,\n",
       " 'embd_pdrop': 0.1,\n",
       " 'attn_pdrop': 0.1,\n",
       " 'layer_norm_epsilon': 1e-05,\n",
       " 'initializer_range': 0.02,\n",
       " 'summary_type': 'cls_index',\n",
       " 'summary_use_proj': True,\n",
       " 'summary_activation': None,\n",
       " 'summary_first_dropout': 0.1,\n",
       " 'summary_proj_to_labels': True,\n",
       " 'scale_attn_weights': True,\n",
       " 'use_cache': True,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'reorder_and_upcast_attn': False,\n",
       " 'bos_token_id': 50256,\n",
       " 'eos_token_id': 50256,\n",
       " 'return_dict': True,\n",
       " 'output_hidden_states': False,\n",
       " 'output_attentions': False,\n",
       " 'torchscript': False,\n",
       " 'torch_dtype': None,\n",
       " 'use_bfloat16': False,\n",
       " 'tf_legacy_loss': False,\n",
       " 'pruned_heads': {},\n",
       " 'tie_word_embeddings': True,\n",
       " 'is_encoder_decoder': False,\n",
       " 'is_decoder': False,\n",
       " 'cross_attention_hidden_size': None,\n",
       " 'add_cross_attention': False,\n",
       " 'tie_encoder_decoder': False,\n",
       " 'max_length': 20,\n",
       " 'min_length': 0,\n",
       " 'do_sample': False,\n",
       " 'early_stopping': False,\n",
       " 'num_beams': 1,\n",
       " 'num_beam_groups': 1,\n",
       " 'diversity_penalty': 0.0,\n",
       " 'temperature': 1.0,\n",
       " 'top_k': 50,\n",
       " 'top_p': 1.0,\n",
       " 'typical_p': 1.0,\n",
       " 'repetition_penalty': 1.0,\n",
       " 'length_penalty': 1.0,\n",
       " 'no_repeat_ngram_size': 0,\n",
       " 'encoder_no_repeat_ngram_size': 0,\n",
       " 'bad_words_ids': None,\n",
       " 'num_return_sequences': 1,\n",
       " 'chunk_size_feed_forward': 0,\n",
       " 'output_scores': False,\n",
       " 'return_dict_in_generate': False,\n",
       " 'forced_bos_token_id': None,\n",
       " 'forced_eos_token_id': None,\n",
       " 'remove_invalid_values': False,\n",
       " 'exponential_decay_length_penalty': None,\n",
       " 'suppress_tokens': None,\n",
       " 'begin_suppress_tokens': None,\n",
       " 'architectures': None,\n",
       " 'finetuning_task': None,\n",
       " 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
       " 'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       " 'tokenizer_class': None,\n",
       " 'prefix': None,\n",
       " 'pad_token_id': None,\n",
       " 'sep_token_id': None,\n",
       " 'decoder_start_token_id': None,\n",
       " 'task_specific_params': None,\n",
       " 'problem_type': None,\n",
       " '_name_or_path': '',\n",
       " 'transformers_version': '4.33.2',\n",
       " 'model_type': 'gpt2'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "GPT2Config().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse FermionOperator using 4 qubits...\n",
      "\n",
      "Operator t:  0.3798313517809532 [] +\n",
      "-0.04221755692243385 [X0 X1 Y2 Y3] +\n",
      "0.04221755692243385 [X0 Y1 Y2 X3] +\n",
      "0.04221755692243385 [Y0 X1 X2 Y3] +\n",
      "-0.04221755692243385 [Y0 Y1 X2 X3] +\n",
      "0.21393531024521326 [Z0] +\n",
      "0.17992650976405974 [Z0 Z1] +\n",
      "0.13459240346368836 [Z0 Z2] +\n",
      "0.17680996038612218 [Z0 Z3] +\n",
      "0.21393531024521334 [Z1] +\n",
      "0.17680996038612218 [Z1 Z2] +\n",
      "0.13459240346368836 [Z1 Z3] +\n",
      "-0.3691443152437658 [Z2] +\n",
      "0.18620984259247125 [Z2 Z3] +\n",
      "-0.3691443152437658 [Z3]\n",
      "Term, coeff:  () 0.3798313517809532\n",
      "Term, coeff:  ((0, 'Z'),) 0.21393531024521326\n",
      "Index, p_char:  0 Z\n",
      "Term, coeff:  ((1, 'Z'),) 0.21393531024521334\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((2, 'Z'),) -0.3691443152437658\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((3, 'Z'),) -0.3691443152437658\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((0, 'Z'), (1, 'Z')) 0.17992650976405974\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((0, 'Y'), (1, 'X'), (2, 'X'), (3, 'Y')) 0.04221755692243385\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'Y'), (1, 'Y'), (2, 'X'), (3, 'X')) -0.04221755692243385\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'X'), (1, 'X'), (2, 'Y'), (3, 'Y')) -0.04221755692243385\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'X'), (1, 'Y'), (2, 'Y'), (3, 'X')) 0.04221755692243385\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'Z'), (2, 'Z')) 0.13459240346368836\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((0, 'Z'), (3, 'Z')) 0.17680996038612218\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((1, 'Z'), (2, 'Z')) 0.17680996038612218\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((1, 'Z'), (3, 'Z')) 0.13459240346368836\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((2, 'Z'), (3, 'Z')) 0.18620984259247125\n",
      "Index, p_char:  2 Z\n",
      "Index, p_char:  3 Z\n",
      "ground state: -1.0551597944706244\n"
     ]
    }
   ],
   "source": [
    "from benchmark.molecule import DiatomicMolecularHamiltonian\n",
    "from qwrapper.hamiltonian import compute_ground_state\n",
    "nqubit = 4\n",
    "hamiltonian = DiatomicMolecularHamiltonian(nqubit, molecule, bravyi_kitaev=is_bravyi)\n",
    "ge = compute_ground_state(hamiltonian)\n",
    "print(\"ground state:\", ge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf state: -1.0429962745400951\n"
     ]
    }
   ],
   "source": [
    "from qwrapper.obs import PauliObservable\n",
    "from gqe.common.initializer import HFStateInitializer\n",
    "from qswift.compiler import DefaultOperatorPool\n",
    "from gqe.mingpt.cost import EnergyCost\n",
    "\n",
    "# prepare operator_pool\n",
    "uccsd = UCCSD(4, molecule)\n",
    "paulis = uccsd.paulis\n",
    "paulis.append(PauliObservable(\"IIII\"))\n",
    "initializer = HFStateInitializer(n_electrons=2)\n",
    "print(\"hf state:\", hamiltonian.exact_value(initializer.init_circuit(4, [], \"qulacs\")))\n",
    "\n",
    "pool = DefaultOperatorPool(paulis)\n",
    "cost = EnergyCost(hamiltonian, initializer, pool,\n",
    "                    [0.00625, -0.00625, 0.0125, -0.0125, 0.025, -0.025, 0.05, -0.05, 0.1, -0.1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.modeling_gpt2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        gpt2config = cfg\n",
    "        self.transformer = GPT2LMHeadModel(gpt2config)        \n",
    "    \n",
    "    def generate_logits(self, idx):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)  # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        logits = self.transformer(idx)[0]\n",
    "        return logits\n",
    "\n",
    "    def cost(self, idx):\n",
    "        idx_output, logits_tensor = self.generate(idx, self.n_gates)\n",
    "        energies = self._cost.energy(idx_output)\n",
    "        mean_logits = torch.mean(logits_tensor, 1)\n",
    "        print(\"mean_logits\", mean_logits * self.energy_scaling)\n",
    "        print(\"energies:\", energies)\n",
    "        print(\"mean:\", torch.mean(energies))\n",
    "        loss = torch.nn.MSELoss()\n",
    "        return loss(torch.exp(-mean_logits), torch.exp(-energies / self.energy_scaling))\n",
    "        # return loss(mean_logits, energies)\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        b_size = idx.size(dim=0)\n",
    "        condition_length = idx.size(dim=1)\n",
    "        # logits_tensor = torch.empty((max_new_tokens, b_size))\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits_base = self.generate_logits(idx_cond)\n",
    "            logits = logits_base[:, -1, :]\n",
    "            probs = F.softmax(-self.temperature * logits, dim=-1)\n",
    "            # either sample from the distribution or take the most likely element\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # logits_tensor[pos] = torch.gather(logits, 1, idx_next).flatten()\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        idx = idx[:, condition_length:]\n",
    "        # print(logits_tensor.T)\n",
    "        return idx, torch.gather(logits_base, 2, idx.reshape(b_size, -1, 1)).reshape(b_size, -1)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        loss = self.cost(idx)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "fabric = L.Fabric()\n",
    "fabric.launch()\n",
    "\n",
    "model = Transformer()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "model, optimizer = fabric.setup(model, optimizer)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input, target)\n",
    "    loss = torch.nn.functional.nll_loss(output, target.view(-1))\n",
    "    fabric.backward(loss)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_collections import config_dict, ConfigDict\n",
    "\n",
    "# placeholder = config_dict.FieldReference(0)\n",
    "cfg = ConfigDict()\n",
    "# setup parameters\n",
    "cfg.sparams = sparams = ConfigDict()\n",
    "sparams.nqubit = 4\n",
    "sparams.ngates = config_dict.FieldReference(0)\n",
    "sparams.seed = 3047\n",
    "sparams.transformation = 'jordan-wigner'\n",
    "sparams.distances = [0.5, 0.6, 0.7, 0.7414, 0.8, 0.9, 1.0, 1.5, 2.0]  # choices of the distance between two atoms\n",
    "# hyperparameters\n",
    "cfg.hparams = hparams = ConfigDict()\n",
    "hparams.iter = 100\n",
    "cfg.gptparams = gptparams = ConfigDict()\n",
    "gptparams.ngates = sparams.ngates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.sparams.ngates = 40\n",
    "cfg.gptparams.ngates\n",
    "cfg.sparams.ngates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not override field 'optional' (reference). tom is of type <class 'str'> but should be of type <class 'int'>\n",
      "nested:\n",
      "  placeholder: 1\n",
      "optional: 1555\n",
      "placeholder: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ml_collections import config_dict\n",
    "\n",
    "placeholder = config_dict.FieldReference(0)\n",
    "cfg = config_dict.ConfigDict()\n",
    "cfg.placeholder = placeholder\n",
    "cfg.optional = config_dict.placeholder(int)\n",
    "cfg.nested = config_dict.ConfigDict()\n",
    "cfg.nested.placeholder = placeholder\n",
    "\n",
    "try:\n",
    "  cfg.optional = 'tom'  # Raises Type error as this field is an integer.\n",
    "except TypeError as e:\n",
    "  print(e)\n",
    "\n",
    "cfg.optional = 1555  # Works fine.\n",
    "cfg.placeholder = 1  # Changes the value of both placeholder and\n",
    "                     # nested.placeholder fields.\n",
    "\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.nested.placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nested:\n",
       "  placeholder: &id002 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "    _field_type: &id001 !!python/name:builtins.int ''\n",
       "    _ops: []\n",
       "    _required: false\n",
       "    _value: 1\n",
       "optional: !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "  _field_type: *id001\n",
       "  _ops: []\n",
       "  _required: false\n",
       "  _value: 1555\n",
       "placeholder: *id002"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "\n",
    "# Get the first 100 characters of Lorem Ipsum\n",
    "lorem_ipsum_100 = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut.\"\n",
    "\n",
    "# Create an image\n",
    "width, height = 600, 60\n",
    "image = Image.new('RGB', (width, height), color = 'white')\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "# Use a truetype font (This uses the default font. For a different font, provide the path)\n",
    "font = ImageFont.truetype(\"Arial\",15)\n",
    "\n",
    "# Draw the text onto the image\n",
    "draw.text((10, 10), lorem_ipsum_100, font=font, fill='black')\n",
    "\n",
    "# Save the image\n",
    "image.save('lorem_ipsum_100.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImageDraw' object has no attribute 'textsize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/marko/Documents/PhD/gqe/lightning_gptqe/playground.ipynb Cell 15\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marko/Documents/PhD/gqe/lightning_gptqe/playground.ipynb#X20sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m bar_str\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marko/Documents/PhD/gqe/lightning_gptqe/playground.ipynb#X20sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# Example of using the progress bar\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/marko/Documents/PhD/gqe/lightning_gptqe/playground.ipynb#X20sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(callbacks\u001b[39m=\u001b[39m[RollingLyricsProgressBar()], max_epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marko/Documents/PhD/gqe/lightning_gptqe/playground.ipynb#X20sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mSimpleCNN\u001b[39;00m(pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marko/Documents/PhD/gqe/lightning_gptqe/playground.ipynb#X20sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "\u001b[1;32m/Users/marko/Documents/PhD/gqe/lightning_gptqe/playground.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marko/Documents/PhD/gqe/lightning_gptqe/playground.ipynb#X20sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m fnt \u001b[39m=\u001b[39m ImageFont\u001b[39m.\u001b[39mload_default()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marko/Documents/PhD/gqe/lightning_gptqe/playground.ipynb#X20sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m dctx \u001b[39m=\u001b[39m ImageDraw\u001b[39m.\u001b[39mDraw(Image\u001b[39m.\u001b[39mnew(\u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m, (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/marko/Documents/PhD/gqe/lightning_gptqe/playground.ipynb#X20sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m text_width, text_height \u001b[39m=\u001b[39m dctx\u001b[39m.\u001b[39;49mtextsize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull_lyrics, font\u001b[39m=\u001b[39mfnt)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marko/Documents/PhD/gqe/lightning_gptqe/playground.ipynb#X20sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mnew(\u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m, (text_width, text_height), color\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marko/Documents/PhD/gqe/lightning_gptqe/playground.ipynb#X20sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m dctx \u001b[39m=\u001b[39m ImageDraw\u001b[39m.\u001b[39mDraw(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageDraw' object has no attribute 'textsize'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "\n",
    "class RollingLyricsProgressBar(pl.callbacks.ProgressBar):\n",
    "\n",
    "    def __init__(self, width=30):\n",
    "        super(RollingLyricsProgressBar, self).__init__()\n",
    "        \n",
    "        # Define lyrics and join them for a continuous flow\n",
    "        self.lyrics = [\n",
    "            \"We're no strangers to love\",\n",
    "            \"You know the rules and so do I\",\n",
    "            # ... Add the rest of the lyrics.\n",
    "            \"Never gonna give you up\",\n",
    "            \"Never gonna let you down\",\n",
    "        ]\n",
    "        self.full_lyrics = ' | '.join(self.lyrics) + ' | '\n",
    "        \n",
    "        self.width = width\n",
    "        self.position = 0\n",
    "        \n",
    "        # Create an image of the lyrics\n",
    "        fnt = ImageFont.load_default()\n",
    "        dctx = ImageDraw.Draw(Image.new('1', (1, 1)))\n",
    "        text_width, text_height = dctx.textsize(self.full_lyrics, font=fnt)\n",
    "        self.image = Image.new('1', (text_width, text_height), color=0)\n",
    "        dctx = ImageDraw.Draw(self.image)\n",
    "        dctx.text((0, 0), self.full_lyrics, font=fnt, fill=1)\n",
    "\n",
    "        \n",
    "    def init_sanity_tqdm(self):\n",
    "        bar = tqdm(position=self.position, leave=True)\n",
    "        bar.set_postfix_str(self.get_next_pixels())\n",
    "        return bar\n",
    "\n",
    "    def init_train_tqdm(self):\n",
    "        bar = tqdm(position=self.position, leave=True)\n",
    "        bar.set_postfix_str(self.get_next_pixels())\n",
    "        return bar\n",
    "\n",
    "    def get_next_pixels(self):\n",
    "        # Extract a portion of the image to represent current progress bar state\n",
    "        segment = self.image.crop((self.position, 0, self.position+self.width, self.image.height))\n",
    "        np_segment = np.asarray(segment).astype(int)\n",
    "        \n",
    "        # Convert binary image to a string of pixel representation (e.g., using `█` for filled pixel)\n",
    "        bar_str = ''\n",
    "        for row in np_segment:\n",
    "            if np.any(row):  # if any pixel in row is ON\n",
    "                bar_str += '█'\n",
    "            else:\n",
    "                bar_str += ' '\n",
    "\n",
    "        self.position = (self.position + 1) % (self.image.width - self.width)\n",
    "\n",
    "        return bar_str\n",
    "\n",
    "# Example of using the progress bar\n",
    "trainer = pl.Trainer(callbacks=[RollingLyricsProgressBar()], max_epochs=100)\n",
    "\n",
    "\n",
    "class SimpleCNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(8 * 8 * 64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 8 * 8 * 64)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        dataset = datasets.CIFAR10('~/.cache/', train=True, download=True, transform=transform)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "        return loader\n",
    "\n",
    "def main():\n",
    "    model = SimpleCNN()\n",
    "    trainer = pl.Trainer(callbacks=[RollingLyricsProgressBar(width=50)], max_epochs=3, progress_bar_refresh_rate=1)\n",
    "    trainer.fit(model)\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kohei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
