{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from hrr import *\n",
    "from argparse import Namespace\n",
    "from experiments.deep_heisenberg.setup_dataloader import setup_dataloader\n",
    "cfg = Namespace(**dict(\n",
    "    ngates = 8000,\n",
    "    npaulis = 128,\n",
    "))\n",
    "dataloader = setup_dataloader(cfg)\n",
    "for circuit in dataloader: break # (8000, 128)\n",
    "\n",
    "def transpose_for_scores(x: torch.Tensor, nh, hhs) -> torch.Tensor:\n",
    "    attention_head_size = hhs//nh\n",
    "    new_x_shape = x.size()[:-1] + (nh, attention_head_size)\n",
    "    x = x.view(new_x_shape)\n",
    "    return x.permute(0, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # symmetric encoder\n",
    "# qubits = torch.tensor([0,1,2,2]) # YZ the same in symmetric encoder\n",
    "# all_paulis = F.one_hot(torch.cartesian_prod(qubits,qubits,qubits),num_classes=3)\n",
    "# all_paulis = torch.cat([all_paulis, -all_paulis])\n",
    "# all_paulis.view(all_paulis.shape[0],-1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEncoding(nn.Module):\n",
    "\n",
    "#     def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "#         super().__init__()\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "#         position = torch.arange(max_len).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "#         pe = torch.zeros(max_len, 1, d_model)\n",
    "#         pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Arguments:\n",
    "#             x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "#         \"\"\"\n",
    "#         x = x + self.pe[:x.size(0)]\n",
    "#         return self.dropout(x)\n",
    "\n",
    "# import math\n",
    "# max_len = 8000\n",
    "# d_model = 8\n",
    "\n",
    "# position = torch.arange(max_len).unsqueeze(1)\n",
    "# div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "# pe = torch.zeros(max_len, 1, d_model)\n",
    "# pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "# pe[:, 0, 1::2] = torch.cos(position * div_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 9])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mygqe.network import PauliEnergy\n",
    "\n",
    "model = PauliEnergy(9, 8, 4, 8000, 128)\n",
    "model(circuit.permute(1,0,2))[:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000e+09])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8000, 128, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden_size, hidden_hidden_size, num_heads, max_length, batch_size\n",
    "# hidden_size == num_gate_features, hidden_hidden_size == attention_hidden_size\n",
    "# number of paulis == bs\n",
    "hs, hhs, nh, ml, bs = 9, 8, 4, 8000, 128\n",
    "gate_embeddings = nn.Embedding(bs, hs) # original embedding is one hot\n",
    "positional_bias = nn.Parameter(torch.empty(1, ml, hs))\n",
    "xavier_uniform_(positional_bias)\n",
    "# pauli_embs = gate_embeddings(circuit)\n",
    "pauli_embs = circuit.permute(1,0,2).float()\n",
    "pauli_embs += positional_bias\n",
    "x = pauli_embs.transpose_(0, 1)\n",
    "\n",
    "query = nn.Linear(hs, hhs)\n",
    "key = nn.Linear(hs, hhs)\n",
    "value = nn.Linear(hs, hhs)\n",
    "\n",
    "# k, q, v = torch.randn(bs, nh, ml, hs//nh), torch.randn(bs, nh, ml, hs//nh), torch.randn(bs, nh, ml, hs//nh)\n",
    "q = transpose_for_scores(query(x), nh, hhs)\n",
    "k = transpose_for_scores(key(x), nh, hhs)\n",
    "v = transpose_for_scores(value(x), nh, hhs)\n",
    "\n",
    "bind = binding(k, v, dim=-1).sum(dim=-2, keepdims=True)  # (B, h, 1, H')\n",
    "vp = unbinding(bind, q, dim=-1)  # (B, h, T, H')\n",
    "scale = cosine_similarity(v, vp, dim=-1, keepdim=True)  # (B, h, T, 1)\n",
    "\n",
    "# scale = scale + (1. - mask) * (-1e9)\n",
    "weight = nn.Softmax(dim=-2)(scale)\n",
    "weighted_value = weight * v\n",
    "\n",
    "# weighted_value = merge(weighted_value)\n",
    "context_layer = weighted_value.permute(0, 2, 1, 3).contiguous()\n",
    "new_context_layer_shape = context_layer.size()[:-2] + (hhs,)\n",
    "context_layer = context_layer.view(new_context_layer_shape)\n",
    "out = nn.Linear(hhs, hs)(context_layer)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IXYZ'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "''.join(map(lambda x: {0:\"I\",1:\"X\",2:\"Y\",3:\"Z\"}[x], torch.arange(4).tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.distributions.Categorical(torch.tensor([0.9,0.1,0.3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_grads(self, sampler: Sampler):\n",
    "    indices = []\n",
    "    seed = random.randint(0, sys.maxsize)\n",
    "\n",
    "    def get_operator():\n",
    "        index = sampler.sample_indices(1)[0]\n",
    "        indices.append(index)\n",
    "        return sampler.get(index)\n",
    "\n",
    "    def get_operator_inv():\n",
    "        index = sampler.sample_indices(1)[0]\n",
    "        return sampler.get(index)\n",
    "\n",
    "    t_1 = np.array(self.ancilla_mes_method.get_values(self.my_get_prepare(sampler, get_operator, False), ntotal=self.shot, seed=seed))\n",
    "    t_2 = np.array(self.ancilla_mes_method.get_values(self.my_get_prepare(sampler, get_operator_inv, True),ntotal=self.shot, seed=seed))\n",
    "    \n",
    "    return (t_1 + t_2) / 2, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_get_prepare(self, sampler, get_operator, inverse):\n",
    "    def prepare():\n",
    "        qc = self.initializer.initialize(init_circuit(self.nqubit + 1, tool=self.tool), targets=self._targets)\n",
    "        qc.h(self._ancilla)\n",
    "\n",
    "        operator = get_operator()\n",
    "        self._add_swift_operator(qc, operator, inverse)\n",
    "        return qc\n",
    "    return prepare"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.init import xavier_uniform_\n",
    "npaulis = 128\n",
    "ngates = 8000\n",
    "nfeatures = 12\n",
    "nheads = 4\n",
    "paulis = torch.arange(npaulis).repeat(ngates, 1)\n",
    "gate_embeddings = nn.Embedding(npaulis, nfeatures) # original embedding is one hot\n",
    "positional_bias = nn.Parameter(torch.empty(ngates, 1, nfeatures))\n",
    "xavier_uniform_(positional_bias)\n",
    "attn = nn.MultiheadAttention(\n",
    "    embed_dim=nfeatures,\n",
    "    num_heads=nheads,\n",
    "    batch_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 73, 125, 125, 105, 121, 105,  19, 109,  77])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxx = torch.tensor([ 73, 125, 125, 105, 121, 105,  19, 109,  77])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 8000, 12])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hrr import *\n",
    "hs, hhs, nh, ml, bs = 12, 24, 4, 8000, 128\n",
    "\n",
    "k, q, v = torch.randn(bs, nh, ml, hs//nh), torch.randn(bs, nh, ml, hs//nh), torch.randn(bs, nh, ml, hs//nh)\n",
    "\n",
    "bind = binding(k, v, dim=-1).sum(dim=-2, keepdims=True)  # (B, h, 1, H')\n",
    "\n",
    "vp = unbinding(bind, q, dim=-1)  # (B, h, T, H')\n",
    "scale = cosine_similarity(v, vp, dim=-1, keepdim=True)  # (B, h, T, 1)\n",
    "\n",
    "# scale = scale + (1. - mask) * (-1e9)\n",
    "weight = nn.Softmax(dim=-2)(scale)\n",
    "weighted_value = weight * v\n",
    "\n",
    "# weighted_value = merge(weighted_value)\n",
    "context_layer = weighted_value.permute(0, 2, 1, 3).contiguous()\n",
    "new_context_layer_shape = context_layer.size()[:-2] + (hs,)\n",
    "context_layer = context_layer.view(new_context_layer_shape)\n",
    "out = nn.Linear(hs, hs)(context_layer)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 131072000000 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m pauli_embs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m positional_bias\n\u001b[1;32m      3\u001b[0m x \u001b[39m=\u001b[39m pauli_embs\u001b[39m.\u001b[39mtranspose_(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m pauli_embs \u001b[39m=\u001b[39m attn(x, x, x)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtranspose_(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m pauli_embs\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/ringmaster/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ringmaster/lib/python3.11/site-packages/torch/nn/modules/activation.py:1205\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1192\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1193\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1202\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1203\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1205\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1206\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1207\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1208\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1209\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1210\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1211\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1212\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1213\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1214\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1215\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1217\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/ringmaster/lib/python3.11/site-packages/torch/nn/functional.py:5338\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5336\u001b[0m     attn_output_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbaddbmm(attn_mask, q_scaled, k\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m   5337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 5338\u001b[0m     attn_output_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(q_scaled, k\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m   5339\u001b[0m attn_output_weights \u001b[39m=\u001b[39m softmax(attn_output_weights, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m   5340\u001b[0m \u001b[39mif\u001b[39;00m dropout_p \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 131072000000 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "pauli_embs = gate_embeddings(paulis)\n",
    "pauli_embs += positional_bias\n",
    "x = pauli_embs.transpose_(0, 1)\n",
    "pauli_embs = attn(x, x, x)[0].transpose_(0, 1)\n",
    "pauli_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "for pauli in paulis:\n",
    "    result = []\n",
    "    for c in pauli.p_string:\n",
    "        r = [0] * 3\n",
    "        if c == 'I':\n",
    "            r[0] = 1\n",
    "        elif c == 'X':\n",
    "            r[1] = 1\n",
    "        else:\n",
    "            r[2] = 1\n",
    "        result.append(r)\n",
    "    results.append(result)\n",
    "torch.tensor(results, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertConfig\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
