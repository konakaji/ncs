{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A demo using Hydrogen Hamiltonian with GPT-QE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import json, os, torch\n",
    "from qwrapper.hamiltonian import compute_ground_state\n",
    "from qwrapper.obs import PauliObservable\n",
    "from gqe.mingpt.model import GPT\n",
    "from gqe.mingpt.trainer import Trainer\n",
    "from gqe.mingpt.utils import set_seed\n",
    "from qswift.compiler import DefaultOperatorPool\n",
    "from benchmark.molecule import DiatomicMolecularHamiltonian\n",
    "from gqe.mingpt.cost import EnergyCost\n",
    "from gqe.operator_pool.uccsd import UCCSD, generate_molecule\n",
    "from gqe.common.initializer import HFStateInitializer\n",
    "\n",
    "nqubit = 4\n",
    "n_gates = 20\n",
    "iter = 100\n",
    "n_sample = 5\n",
    "seed = 3047\n",
    "distances = [0.5, 0.6, 0.7, 0.7414, 0.8, 0.9, 1.0, 1.5, 2.0]  # choices of the distance between two atoms\n",
    "model_type = 'gpt2'\n",
    "transformation = 'jordan-wigner'\n",
    "is_bravyi = transformation == 'bravyi-kitaev'\n",
    "MODEL_FILEBASE = '../saved_models/{}_model_h2_sto3g_{}_{}_{}.json'\n",
    "ENERGY_FILEBASE = '../output/{}_energy_h2_sto3g_{}_{}_{}.txt'\n",
    "OTHER_FILEBASE = '../output/{}_other_h2_sto3g_{}_{}_{}.json'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T18:40:22.422579Z",
     "start_time": "2023-09-25T18:40:17.661108Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "def find_ground_state_energy(distance, seed, ignore_cache=False):\n",
    "    molecule = generate_molecule(\"H\", \"H\", distance, \"sto-3g\", bravyi_kitaev=is_bravyi)\n",
    "    set_seed(seed)\n",
    "    # prepare file\n",
    "    model_output = MODEL_FILEBASE.format(model_type, str(distance), transformation, seed)\n",
    "    other_output = OTHER_FILEBASE.format(model_type, str(distance), transformation, seed)\n",
    "\n",
    "    if not ignore_cache and os.path.exists(model_output):\n",
    "        return\n",
    "\n",
    "    # prepare Hamiltonian\n",
    "    hamiltonian = DiatomicMolecularHamiltonian(nqubit, molecule, bravyi_kitaev=is_bravyi)\n",
    "\n",
    "    ge = compute_ground_state(hamiltonian)\n",
    "    print(\"ground state:\", ge)\n",
    "\n",
    "    # prepare operator_pool\n",
    "    uccsd = UCCSD(4, molecule)\n",
    "    paulis = uccsd.paulis\n",
    "    paulis.append(PauliObservable(\"IIII\"))\n",
    "    initializer = HFStateInitializer(n_electrons=2)\n",
    "    print(\"hf state:\", hamiltonian.exact_value(initializer.init_circuit(4, [], \"qulacs\")))\n",
    "\n",
    "    pool = DefaultOperatorPool(paulis)\n",
    "    cost = EnergyCost(hamiltonian, initializer, pool,\n",
    "                      [0.00625, -0.00625, 0.0125, -0.0125, 0.025, -0.025, 0.05, -0.05, 0.1, -0.1])\n",
    "\n",
    "    model_config = GPT.get_default_config()\n",
    "    model_config.model_type = model_type\n",
    "    model_config.vocab_size = cost.vocab_size()\n",
    "    model_config.block_size = cost.vocab_size()\n",
    "    model_config.n_gates = n_gates  # The number of gates for each circuit\n",
    "    model_config.temperature = 5  # Each gate is generated with probability exp(-temperature * logit)\n",
    "    model_config.embd_pdrop = 0\n",
    "    model_config.resid_pdrop = 0\n",
    "    model_config.attn_pdrop = 0\n",
    "    model = GPT(model_config, cost)\n",
    "\n",
    "    # create a Trainer object\n",
    "\n",
    "    train_config = Trainer.get_default_config()\n",
    "    train_config.learning_rate = 5e-7  # the model we're using is so small that we can go a bit faster\n",
    "    train_config.max_iters = iter\n",
    "    train_config.num_workers = 0\n",
    "    train_config.n_samples = n_sample\n",
    "\n",
    "    trainer = Trainer(train_config, model)\n",
    "\n",
    "    def batch_end_callback(trainer, detail):\n",
    "        if trainer.iter_num % 1 == 0:\n",
    "            print(\n",
    "                f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f} temperature: {model.temperature}\")\n",
    "            model.temperature += 0.05\n",
    "            print(\"mean_logits\", torch.mean(detail.logits, 1) * model.energy_scaling)\n",
    "            print(\"energies:\", detail.energies)\n",
    "            print(\"mean:\", torch.mean(detail.energies))\n",
    "\n",
    "    trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "    trainer.run()\n",
    "    torch.save(model.state_dict(), model_output)\n",
    "\n",
    "    model.temperature = 20\n",
    "    indices, logits = model.generate(torch.zeros(10, 1, dtype=torch.int), model_config.n_gates)\n",
    "    index = torch.argmin(cost.energy(indices)).item()\n",
    "    target = indices.numpy()[index]\n",
    "    computed_energy = cost.energy(torch.tensor([target])).item()\n",
    "\n",
    "    m = {\"distance\": distance,\n",
    "         \"exact_energy\": ge,\n",
    "         \"computed_energy\": computed_energy,\n",
    "         \"n_gates\": n_gates, \"seed\": seed}\n",
    "    with open(other_output, \"w\") as f:\n",
    "        f.write(json.dumps(m))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T16:28:19.095552Z",
     "start_time": "2023-09-17T16:28:19.082039Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -1.0429962745401\n",
      "Starting to parse FermionOperator using 4 qubits...\n",
      "\n",
      "Operator t:  0.379831351780953 [] +\n",
      "-0.042217556922433806 [X0 X1 Y2 Y3] +\n",
      "0.042217556922433806 [X0 Y1 Y2 X3] +\n",
      "0.042217556922433806 [Y0 X1 X2 Y3] +\n",
      "-0.042217556922433806 [Y0 Y1 X2 X3] +\n",
      "0.21393531024521328 [Z0] +\n",
      "0.17992650976405974 [Z0 Z1] +\n",
      "0.13459240346368848 [Z0 Z2] +\n",
      "0.17680996038612226 [Z0 Z3] +\n",
      "0.2139353102452133 [Z1] +\n",
      "0.17680996038612226 [Z1 Z2] +\n",
      "0.13459240346368848 [Z1 Z3] +\n",
      "-0.36914431524376606 [Z2] +\n",
      "0.18620984259247156 [Z2 Z3] +\n",
      "-0.369144315243766 [Z3]\n",
      "Term, coeff:  () 0.379831351780953\n",
      "Term, coeff:  ((0, 'Z'),) 0.21393531024521328\n",
      "Index, p_char:  0 Z\n",
      "Term, coeff:  ((1, 'Z'),) 0.2139353102452133\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((2, 'Z'),) -0.36914431524376606\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((3, 'Z'),) -0.369144315243766\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((0, 'Z'), (1, 'Z')) 0.17992650976405974\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((0, 'Y'), (1, 'X'), (2, 'X'), (3, 'Y')) 0.042217556922433806\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'Y'), (1, 'Y'), (2, 'X'), (3, 'X')) -0.042217556922433806\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'X'), (1, 'X'), (2, 'Y'), (3, 'Y')) -0.042217556922433806\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'X'), (1, 'Y'), (2, 'Y'), (3, 'X')) 0.042217556922433806\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'Z'), (2, 'Z')) 0.13459240346368848\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((0, 'Z'), (3, 'Z')) 0.17680996038612226\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((1, 'Z'), (2, 'Z')) 0.17680996038612226\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((1, 'Z'), (3, 'Z')) 0.13459240346368848\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((2, 'Z'), (3, 'Z')) 0.18620984259247156\n",
      "Index, p_char:  2 Z\n",
      "Index, p_char:  3 Z\n",
      "ground state: -1.0551597944706248\n",
      "hf state: -1.0429962745400956\n",
      "number of parameters: 85.29M\n",
      "running on device cpu\n",
      "mean_logits tensor([-1.1676, -0.9894, -1.0338, -1.0321, -1.2728], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0325, -1.0313, -0.9601, -0.9797, -0.8476])\n",
      "mean: tensor(-0.9703)\n",
      "iter_dt 0.00ms; iter 0: train loss 0.35365 temperature: 5\n",
      "mean_logits tensor([-0.9149, -1.4337, -0.9579, -0.9189, -0.9366], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9042, -1.0530, -1.0422, -0.9574, -1.0165])\n",
      "mean: tensor(-0.9947)\n",
      "iter_dt 1694968107621.52ms; iter 1: train loss 0.37423 temperature: 5.05\n",
      "mean_logits tensor([-0.9990, -0.9663, -1.1767, -1.2484, -0.9713], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.5757, -0.7414, -0.9927, -0.5282, -1.0146])\n",
      "mean: tensor(-0.7705)\n",
      "iter_dt 6525.53ms; iter 2: train loss 0.93396 temperature: 5.1\n",
      "mean_logits tensor([-1.0575, -0.9106, -1.0167, -1.0680, -1.0113], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.7891, -0.9889, -0.9057, -1.0212, -0.9258])\n",
      "mean: tensor(-0.9261)\n",
      "iter_dt 6470.22ms; iter 3: train loss 0.13061 temperature: 5.1499999999999995\n",
      "mean_logits tensor([-0.7829, -1.0565, -1.0841, -1.0530, -1.1221], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8777, -0.6817, -0.8088, -0.9107, -0.5649])\n",
      "mean: tensor(-0.7688)\n",
      "iter_dt 6480.25ms; iter 4: train loss 0.64558 temperature: 5.199999999999999\n",
      "mean_logits tensor([-0.8433, -0.8226, -0.9657, -1.0284, -0.8384], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9118, -1.0239, -0.9908, -0.8955, -0.6782])\n",
      "mean: tensor(-0.9001)\n",
      "iter_dt 6481.81ms; iter 5: train loss 0.10553 temperature: 5.249999999999999\n",
      "mean_logits tensor([-1.0457, -0.8829, -0.9145, -0.9489, -0.8310], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8963, -1.0286, -0.8421, -0.6334, -0.5683])\n",
      "mean: tensor(-0.7937)\n",
      "iter_dt 6481.11ms; iter 6: train loss 0.21997 temperature: 5.299999999999999\n",
      "mean_logits tensor([-0.8676, -0.8502, -0.9673, -0.9619, -0.8131], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0040, -0.9324, -0.8393, -1.0366, -0.9133])\n",
      "mean: tensor(-0.9451)\n",
      "iter_dt 6464.44ms; iter 7: train loss 0.07177 temperature: 5.349999999999999\n",
      "mean_logits tensor([-0.9892, -0.8572, -0.8318, -0.7543, -0.9075], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9677, -0.8497, -1.0374, -0.9028, -0.8879])\n",
      "mean: tensor(-0.9291)\n",
      "iter_dt 6466.15ms; iter 8: train loss 0.07941 temperature: 5.399999999999999\n",
      "mean_logits tensor([-0.8001, -0.7573, -0.8596, -0.9701, -0.8196], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0171, -0.9508, -0.7752, -0.9109, -0.9311])\n",
      "mean: tensor(-0.9170)\n",
      "iter_dt 6463.29ms; iter 9: train loss 0.12591 temperature: 5.449999999999998\n",
      "mean_logits tensor([-0.8460, -0.9534, -0.8038, -0.7795, -0.8683], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.7333, -0.6337, -0.8557, -0.6147, -1.0081])\n",
      "mean: tensor(-0.7691)\n",
      "iter_dt 6474.77ms; iter 10: train loss 0.16351 temperature: 5.499999999999998\n",
      "mean_logits tensor([-0.8664, -0.9916, -0.9776, -0.8743, -0.9387], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.5974, -0.9949, -1.0175, -0.9722, -1.0208])\n",
      "mean: tensor(-0.9205)\n",
      "iter_dt 6567.48ms; iter 11: train loss 0.08700 temperature: 5.549999999999998\n",
      "mean_logits tensor([-0.8682, -0.9181, -0.8739, -0.8377, -0.8049], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9615, -0.9693, -0.9252, -1.0435, -1.0130])\n",
      "mean: tensor(-0.9825)\n",
      "iter_dt 6549.51ms; iter 12: train loss 0.12682 temperature: 5.599999999999998\n",
      "mean_logits tensor([-0.8698, -0.7740, -0.9396, -0.9113, -0.8417], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9664, -1.0021, -1.0262, -0.9480, -0.9234])\n",
      "mean: tensor(-0.9732)\n",
      "iter_dt 6500.49ms; iter 13: train loss 0.09369 temperature: 5.649999999999998\n",
      "mean_logits tensor([-1.0106, -0.9602, -1.1118, -1.1671, -1.0263], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9490, -1.0463, -0.8522, -1.0238, -1.0415])\n",
      "mean: tensor(-0.9826)\n",
      "iter_dt 6506.84ms; iter 14: train loss 0.15019 temperature: 5.6999999999999975\n",
      "mean_logits tensor([-0.9941, -0.9770, -0.8866, -0.9488, -1.2075], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0180, -0.9918, -1.0026, -0.9766, -1.0061])\n",
      "mean: tensor(-0.9990)\n",
      "iter_dt 6482.77ms; iter 15: train loss 0.09450 temperature: 5.749999999999997\n",
      "mean_logits tensor([-0.8957, -1.0466, -1.0182, -1.0792, -1.0362], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.6583, -0.9870, -1.0293, -0.8656, -0.9604])\n",
      "mean: tensor(-0.9001)\n",
      "iter_dt 6473.18ms; iter 16: train loss 0.13168 temperature: 5.799999999999997\n",
      "mean_logits tensor([-1.0978, -0.9078, -1.0299, -1.0055, -1.0098], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9865, -1.0514, -0.8398, -1.0219, -1.0460])\n",
      "mean: tensor(-0.9891)\n",
      "iter_dt 6482.20ms; iter 17: train loss 0.09875 temperature: 5.849999999999997\n",
      "mean_logits tensor([-1.0080, -1.0157, -0.8753, -0.9201, -0.9706], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9266, -0.9744, -0.9965, -0.9480, -0.9555])\n",
      "mean: tensor(-0.9602)\n",
      "iter_dt 6470.24ms; iter 18: train loss 0.03211 temperature: 5.899999999999997\n",
      "mean_logits tensor([-0.9500, -0.9797, -0.9734, -1.0525, -1.0118], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0361, -0.9519, -0.7543, -0.9095, -0.9893])\n",
      "mean: tensor(-0.9282)\n",
      "iter_dt 6519.17ms; iter 19: train loss 0.09608 temperature: 5.949999999999997\n",
      "mean_logits tensor([-1.1293, -1.0442, -0.9922, -1.0352, -0.9688], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0384, -0.9612, -1.0047, -0.9546, -1.0417])\n",
      "mean: tensor(-1.0001)\n",
      "iter_dt 6513.83ms; iter 20: train loss 0.04238 temperature: 5.9999999999999964\n",
      "mean_logits tensor([-1.0803, -0.9269, -1.0865, -1.0558, -0.9764], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0240, -1.0104, -0.7787, -0.8814, -1.0155])\n",
      "mean: tensor(-0.9420)\n",
      "iter_dt 6483.06ms; iter 21: train loss 0.18278 temperature: 6.049999999999996\n",
      "mean_logits tensor([-0.8729, -0.9157, -0.9689, -0.9076, -1.0062], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0276, -1.0281, -0.9078, -0.9929, -1.0143])\n",
      "mean: tensor(-0.9941)\n",
      "iter_dt 6462.24ms; iter 22: train loss 0.06449 temperature: 6.099999999999996\n",
      "mean_logits tensor([-0.9860, -0.9525, -1.1040, -1.0331, -0.8578], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0456, -1.0103, -1.0011, -1.0055, -0.9447])\n",
      "mean: tensor(-1.0014)\n",
      "iter_dt 6483.24ms; iter 23: train loss 0.03789 temperature: 6.149999999999996\n",
      "mean_logits tensor([-0.9909, -0.9676, -1.0312, -0.9573, -0.9692], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9187, -0.9693, -0.7289, -0.9647, -1.0249])\n",
      "mean: tensor(-0.9213)\n",
      "iter_dt 6485.99ms; iter 24: train loss 0.11876 temperature: 6.199999999999996\n",
      "mean_logits tensor([-1.1137, -0.8720, -1.0865, -0.9172, -0.9993], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0435, -0.9519, -0.8192, -0.7824, -0.9347])\n",
      "mean: tensor(-0.9063)\n",
      "iter_dt 6452.66ms; iter 25: train loss 0.13876 temperature: 6.249999999999996\n",
      "mean_logits tensor([-1.1119, -0.9123, -1.1113, -0.9575, -0.9503], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0313, -0.9488, -0.7600, -0.9903, -0.9822])\n",
      "mean: tensor(-0.9425)\n",
      "iter_dt 6483.78ms; iter 26: train loss 0.17772 temperature: 6.299999999999995\n",
      "mean_logits tensor([-0.9970, -0.9828, -1.0015, -0.8661, -1.0504], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0507, -1.0407, -1.0215, -1.0295, -1.0160])\n",
      "mean: tensor(-1.0317)\n",
      "iter_dt 6476.72ms; iter 27: train loss 0.04767 temperature: 6.349999999999995\n",
      "mean_logits tensor([-0.9413, -1.1217, -0.9790, -0.8749, -1.1058], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0454, -1.0392, -1.0245, -1.0003, -0.9957])\n",
      "mean: tensor(-1.0210)\n",
      "iter_dt 6482.60ms; iter 28: train loss 0.07109 temperature: 6.399999999999995\n",
      "mean_logits tensor([-0.9051, -0.8719, -0.8470, -0.8503, -0.8194], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9764, -1.0288, -0.9817, -0.8501, -1.0360])\n",
      "mean: tensor(-0.9746)\n",
      "iter_dt 6530.84ms; iter 29: train loss 0.12254 temperature: 6.449999999999995\n",
      "mean_logits tensor([-0.9737, -0.9970, -1.1401, -0.9748, -0.9955], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0047, -0.9531, -0.9800, -1.0180, -1.0127])\n",
      "mean: tensor(-0.9937)\n",
      "iter_dt 6480.53ms; iter 30: train loss 0.05009 temperature: 6.499999999999995\n",
      "mean_logits tensor([-1.0033, -0.8355, -0.9874, -0.9059, -0.8322], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8573, -1.0440, -0.9752, -0.9584, -1.0071])\n",
      "mean: tensor(-0.9684)\n",
      "iter_dt 6486.35ms; iter 31: train loss 0.12697 temperature: 6.5499999999999945\n",
      "mean_logits tensor([-0.9034, -0.9841, -1.0607, -0.9515, -0.9069], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0027, -0.9802, -0.9584, -1.0151, -1.0262])\n",
      "mean: tensor(-0.9965)\n",
      "iter_dt 6494.47ms; iter 32: train loss 0.05455 temperature: 6.599999999999994\n",
      "mean_logits tensor([-0.9175, -1.0631, -0.6927, -1.0548, -0.9104], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0474, -1.0064, -1.0202, -0.8871, -1.0037])\n",
      "mean: tensor(-0.9929)\n",
      "iter_dt 6494.98ms; iter 33: train loss 0.20028 temperature: 6.649999999999994\n",
      "mean_logits tensor([-0.9400, -0.9305, -0.9705, -1.0294, -0.9811], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9868, -1.0454, -1.0341, -1.0192, -1.0223])\n",
      "mean: tensor(-1.0215)\n",
      "iter_dt 6497.53ms; iter 34: train loss 0.03073 temperature: 6.699999999999994\n",
      "mean_logits tensor([-1.0000, -0.8448, -1.0606, -0.9109, -1.0051], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0177, -1.0309, -1.0322, -1.0377, -0.9892])\n",
      "mean: tensor(-1.0215)\n",
      "iter_dt 6476.33ms; iter 35: train loss 0.07010 temperature: 6.749999999999994\n",
      "mean_logits tensor([-0.8530, -1.0175, -1.0955, -1.0388, -1.1693], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0027, -1.0190, -1.0177, -0.9521, -0.9879])\n",
      "mean: tensor(-0.9959)\n",
      "iter_dt 6477.05ms; iter 36: train loss 0.10685 temperature: 6.799999999999994\n",
      "mean_logits tensor([-1.1680, -0.9513, -0.9112, -1.1111, -0.9298], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9808, -1.0160, -0.9882, -1.0321, -0.9976])\n",
      "mean: tensor(-1.0029)\n",
      "iter_dt 6463.90ms; iter 37: train loss 0.09117 temperature: 6.849999999999993\n",
      "mean_logits tensor([-0.9958, -1.1400, -1.1439, -1.1006, -1.0666], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0431, -1.0458, -0.9913, -0.9837, -1.0138])\n",
      "mean: tensor(-1.0156)\n",
      "iter_dt 6429.61ms; iter 38: train loss 0.08514 temperature: 6.899999999999993\n",
      "mean_logits tensor([-0.9762, -0.9364, -1.0032, -1.2010, -1.0187], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0418, -0.9889, -1.0002, -1.0348, -1.0123])\n",
      "mean: tensor(-1.0156)\n",
      "iter_dt 6448.16ms; iter 39: train loss 0.06214 temperature: 6.949999999999993\n",
      "mean_logits tensor([-1.0508, -1.0679, -1.0676, -0.9898, -1.0923], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0121, -1.0267, -1.0432, -1.0062, -1.0392])\n",
      "mean: tensor(-1.0255)\n",
      "iter_dt 6445.94ms; iter 40: train loss 0.01126 temperature: 6.999999999999993\n",
      "mean_logits tensor([-0.8602, -1.0422, -0.8515, -1.0700, -1.0195], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0312, -0.9753, -0.8044, -1.0106, -0.9815])\n",
      "mean: tensor(-0.9606)\n",
      "iter_dt 6462.25ms; iter 41: train loss 0.05571 temperature: 7.049999999999993\n",
      "mean_logits tensor([-1.0354, -1.0666, -0.9892, -1.0133, -1.0469], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9830, -0.9827, -1.0311, -0.9373, -1.0287])\n",
      "mean: tensor(-0.9926)\n",
      "iter_dt 6445.15ms; iter 42: train loss 0.02636 temperature: 7.0999999999999925\n",
      "mean_logits tensor([-1.0624, -1.0119, -0.9334, -0.9108, -1.0835], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0425, -0.9906, -1.0076, -1.0012, -0.8944])\n",
      "mean: tensor(-0.9873)\n",
      "iter_dt 6456.22ms; iter 43: train loss 0.07196 temperature: 7.149999999999992\n",
      "mean_logits tensor([-0.9454, -1.0110, -0.8616, -1.0008, -0.9923], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0245, -1.0175, -1.0405, -1.0444, -1.0233])\n",
      "mean: tensor(-1.0300)\n",
      "iter_dt 6472.18ms; iter 44: train loss 0.05643 temperature: 7.199999999999992\n",
      "mean_logits tensor([-0.9806, -1.0146, -1.0814, -0.8456, -0.9651], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0193, -1.0418, -1.0402, -0.8901, -1.0431])\n",
      "mean: tensor(-1.0069)\n",
      "iter_dt 6441.84ms; iter 45: train loss 0.01754 temperature: 7.249999999999992\n",
      "mean_logits tensor([-0.9481, -0.9902, -0.9585, -0.9654, -1.0077], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9723, -0.9844, -0.9974, -1.0111, -1.0154])\n",
      "mean: tensor(-0.9961)\n",
      "iter_dt 6441.85ms; iter 46: train loss 0.00611 temperature: 7.299999999999992\n",
      "mean_logits tensor([-1.0375, -1.0022, -0.9863, -0.9945, -0.9419], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0418, -1.0319, -0.9570, -0.9966, -0.9569])\n",
      "mean: tensor(-0.9968)\n",
      "iter_dt 6435.40ms; iter 47: train loss 0.00288 temperature: 7.349999999999992\n",
      "mean_logits tensor([-0.9348, -0.8762, -1.0192, -1.1045, -1.0217], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9533, -1.0460, -0.9891, -1.0253, -1.0021])\n",
      "mean: tensor(-1.0032)\n",
      "iter_dt 6429.61ms; iter 48: train loss 0.05245 temperature: 7.3999999999999915\n",
      "mean_logits tensor([-1.0958, -1.0523, -1.2226, -1.0074, -1.0961], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9906, -1.0352, -0.8774, -1.0402, -1.0315])\n",
      "mean: tensor(-0.9950)\n",
      "iter_dt 6430.42ms; iter 49: train loss 0.22363 temperature: 7.449999999999991\n",
      "mean_logits tensor([-1.0140, -1.0212, -1.1388, -1.0454, -1.0554], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9431, -0.8920, -1.0409, -1.0483, -0.9947])\n",
      "mean: tensor(-0.9838)\n",
      "iter_dt 6428.15ms; iter 50: train loss 0.05251 temperature: 7.499999999999991\n",
      "mean_logits tensor([-1.0818, -1.1075, -1.1238, -1.0757, -1.0559], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0428, -0.9855, -1.0338, -1.0158, -1.0120])\n",
      "mean: tensor(-1.0180)\n",
      "iter_dt 6434.58ms; iter 51: train loss 0.04957 temperature: 7.549999999999991\n",
      "mean_logits tensor([-0.9473, -0.9918, -0.9334, -1.0674, -1.0255], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0293, -1.0274, -1.0338, -1.0413, -0.9582])\n",
      "mean: tensor(-1.0180)\n",
      "iter_dt 6442.31ms; iter 52: train loss 0.03377 temperature: 7.599999999999991\n",
      "mean_logits tensor([-1.0217, -0.9733, -1.0659, -1.0645, -1.0908], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0180, -1.0313, -1.0344, -1.0410, -1.0270])\n",
      "mean: tensor(-1.0303)\n",
      "iter_dt 6423.80ms; iter 53: train loss 0.01430 temperature: 7.649999999999991\n",
      "mean_logits tensor([-1.0798, -1.1003, -1.0047, -1.0064, -1.0096], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0336, -1.0060, -0.8353, -1.0255, -0.9995])\n",
      "mean: tensor(-0.9800)\n",
      "iter_dt 6432.95ms; iter 54: train loss 0.05511 temperature: 7.69999999999999\n",
      "mean_logits tensor([-1.0140, -1.0769, -0.9255, -1.0398, -1.0125], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0213, -0.9320, -1.0299, -0.7678, -0.9840])\n",
      "mean: tensor(-0.9470)\n",
      "iter_dt 6426.01ms; iter 55: train loss 0.13883 temperature: 7.74999999999999\n",
      "mean_logits tensor([-1.0458, -0.9741, -1.0974, -1.0322, -0.8884], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0232, -1.0067, -1.0282, -0.9603, -0.9926])\n",
      "mean: tensor(-1.0022)\n",
      "iter_dt 6420.11ms; iter 56: train loss 0.03221 temperature: 7.79999999999999\n",
      "mean_logits tensor([-1.0503, -0.9594, -1.0370, -0.9570, -1.1377], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9513, -1.0321, -1.0531, -1.0008, -1.0338])\n",
      "mean: tensor(-1.0142)\n",
      "iter_dt 6423.92ms; iter 57: train loss 0.04433 temperature: 7.84999999999999\n",
      "mean_logits tensor([-1.0145, -0.9745, -1.2916, -1.1439, -0.9579], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0371, -1.0244, -0.9725, -1.0336, -1.0486])\n",
      "mean: tensor(-1.0233)\n",
      "iter_dt 6419.88ms; iter 58: train loss 0.23577 temperature: 7.89999999999999\n",
      "mean_logits tensor([-1.0439, -0.9385, -0.8694, -0.9861, -0.8985], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0437, -1.0180, -1.0142, -0.8766, -1.0164])\n",
      "mean: tensor(-0.9938)\n",
      "iter_dt 6435.69ms; iter 59: train loss 0.07097 temperature: 7.9499999999999895\n",
      "mean_logits tensor([-1.0124, -0.9857, -1.0752, -0.9445, -0.9390], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9902, -1.0099, -1.0118, -1.0061, -0.9758])\n",
      "mean: tensor(-0.9988)\n",
      "iter_dt 6402.55ms; iter 60: train loss 0.01526 temperature: 7.999999999999989\n",
      "mean_logits tensor([-0.9292, -0.9509, -0.9723, -1.0446, -0.9883], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0473, -1.0325, -1.0016, -1.0407, -1.0380])\n",
      "mean: tensor(-1.0320)\n",
      "iter_dt 6417.27ms; iter 61: train loss 0.03488 temperature: 8.04999999999999\n",
      "mean_logits tensor([-0.9646, -0.9960, -0.9774, -0.9468, -1.0074], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9876, -0.9725, -0.9729, -1.0284, -1.0251])\n",
      "mean: tensor(-0.9973)\n",
      "iter_dt 6394.30ms; iter 62: train loss 0.01164 temperature: 8.09999999999999\n",
      "mean_logits tensor([-1.0042, -0.9692, -0.8941, -1.0033, -0.9942], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9966, -1.0136, -1.0365, -0.9995, -1.0319])\n",
      "mean: tensor(-1.0156)\n",
      "iter_dt 6445.71ms; iter 63: train loss 0.03317 temperature: 8.149999999999991\n",
      "mean_logits tensor([-0.9156, -1.1249, -1.1442, -0.9558, -0.9343], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9977, -1.0233, -1.0337, -0.9270, -1.0381])\n",
      "mean: tensor(-1.0040)\n",
      "iter_dt 6412.55ms; iter 64: train loss 0.06502 temperature: 8.199999999999992\n",
      "mean_logits tensor([-1.0759, -1.1160, -1.2720, -1.0375, -0.9399], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9978, -1.0347, -0.9546, -0.9655, -1.0377])\n",
      "mean: tensor(-0.9980)\n",
      "iter_dt 6421.25ms; iter 65: train loss 0.23090 temperature: 8.249999999999993\n",
      "mean_logits tensor([-0.9720, -0.9342, -1.0612, -0.8686, -1.0351], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0297, -0.9821, -0.9685, -1.0029, -1.0529])\n",
      "mean: tensor(-1.0072)\n",
      "iter_dt 6424.53ms; iter 66: train loss 0.04515 temperature: 8.299999999999994\n",
      "mean_logits tensor([-1.0739, -1.0844, -0.9857, -1.0376, -1.0914], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9661, -1.0507, -1.0362, -0.7594, -1.0495])\n",
      "mean: tensor(-0.9724)\n",
      "iter_dt 6455.09ms; iter 67: train loss 0.12068 temperature: 8.349999999999994\n",
      "mean_logits tensor([-1.0442, -1.0566, -1.0052, -0.8985, -1.0531], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0085, -1.0260, -1.0347, -1.0007, -1.0352])\n",
      "mean: tensor(-1.0210)\n",
      "iter_dt 6413.80ms; iter 68: train loss 0.01934 temperature: 8.399999999999995\n",
      "mean_logits tensor([-1.0433, -1.0112, -1.1713, -1.1072, -0.9867], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0312, -1.0283, -0.9213, -0.9981, -1.0320])\n",
      "mean: tensor(-1.0022)\n",
      "iter_dt 6450.49ms; iter 69: train loss 0.12519 temperature: 8.449999999999996\n",
      "mean_logits tensor([-1.0208, -1.0147, -1.1325, -1.0526, -1.1165], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0357, -1.0065, -0.8064, -0.9463, -1.0210])\n",
      "mean: tensor(-0.9632)\n",
      "iter_dt 6427.51ms; iter 70: train loss 0.18175 temperature: 8.499999999999996\n",
      "mean_logits tensor([-1.0392, -0.9910, -1.1013, -1.0155, -0.9585], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0318, -1.0450, -1.0372, -0.9956, -1.0470])\n",
      "mean: tensor(-1.0313)\n",
      "iter_dt 6424.21ms; iter 71: train loss 0.02378 temperature: 8.549999999999997\n",
      "mean_logits tensor([-0.8999, -1.0001, -0.9957, -1.0176, -1.1545], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9857, -1.0364, -1.0114, -1.0447, -0.9503])\n",
      "mean: tensor(-1.0057)\n",
      "iter_dt 6408.69ms; iter 72: train loss 0.08198 temperature: 8.599999999999998\n",
      "mean_logits tensor([-0.8981, -0.8964, -0.8588, -0.9870, -1.0211], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0102, -0.9654, -0.9787, -0.9955, -1.0296])\n",
      "mean: tensor(-0.9959)\n",
      "iter_dt 6413.05ms; iter 73: train loss 0.04139 temperature: 8.649999999999999\n",
      "mean_logits tensor([-1.0324, -1.0782, -1.0482, -0.9476, -0.9365], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0262, -1.0401, -0.3292, -0.9765, -0.9534])\n",
      "mean: tensor(-0.8651)\n",
      "iter_dt 6414.12ms; iter 74: train loss 0.43178 temperature: 8.7\n",
      "mean_logits tensor([-1.0703, -0.9978, -1.0188, -0.9868, -0.9826], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0053, -1.0282, -1.0375, -1.0095, -1.0172])\n",
      "mean: tensor(-1.0196)\n",
      "iter_dt 6395.70ms; iter 75: train loss 0.01120 temperature: 8.75\n",
      "mean_logits tensor([-1.0643, -1.0103, -1.0197, -0.8925, -1.0607], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0125, -1.0372, -1.0069, -1.0371, -0.9647])\n",
      "mean: tensor(-1.0117)\n",
      "iter_dt 6416.90ms; iter 76: train loss 0.04848 temperature: 8.8\n",
      "mean_logits tensor([-0.9878, -1.0102, -1.0837, -1.0495, -0.9782], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0409, -1.0483, -0.8961, -1.0340, -0.9968])\n",
      "mean: tensor(-1.0032)\n",
      "iter_dt 6422.36ms; iter 77: train loss 0.05862 temperature: 8.850000000000001\n",
      "mean_logits tensor([-1.0822, -1.0426, -1.0126, -1.1017, -1.0816], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9447, -0.9399, -0.9597, -0.9239, -1.0527])\n",
      "mean: tensor(-0.9642)\n",
      "iter_dt 6396.14ms; iter 78: train loss 0.09759 temperature: 8.900000000000002\n",
      "mean_logits tensor([-0.9542, -1.0382, -1.0554, -1.0397, -0.9661], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9121, -0.8002, -0.8438, -0.9604, -1.0339])\n",
      "mean: tensor(-0.9101)\n",
      "iter_dt 6438.02ms; iter 79: train loss 0.14998 temperature: 8.950000000000003\n",
      "mean_logits tensor([-1.1040, -1.0506, -1.0552, -1.0521, -0.9352], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.7815, -1.0098, -1.0463, -1.0214, -1.0061])\n",
      "mean: tensor(-0.9730)\n",
      "iter_dt 6427.55ms; iter 80: train loss 0.14946 temperature: 9.000000000000004\n",
      "mean_logits tensor([-1.0751, -0.9990, -0.9088, -1.0151, -1.1006], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8611, -0.9443, -1.0208, -0.9373, -0.6055])\n",
      "mean: tensor(-0.8738)\n",
      "iter_dt 6428.83ms; iter 81: train loss 0.36924 temperature: 9.050000000000004\n",
      "mean_logits tensor([-1.0053, -1.1282, -0.9915, -0.9581, -0.8856], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0173, -1.0445, -0.8451, -1.0115, -1.0149])\n",
      "mean: tensor(-0.9867)\n",
      "iter_dt 6411.25ms; iter 82: train loss 0.06597 temperature: 9.100000000000005\n",
      "mean_logits tensor([-0.8881, -0.9524, -0.9695, -0.9644, -0.9667], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9303, -1.0401, -0.9914, -1.0317, -1.0300])\n",
      "mean: tensor(-1.0047)\n",
      "iter_dt 6444.86ms; iter 83: train loss 0.02676 temperature: 9.150000000000006\n",
      "mean_logits tensor([-0.9899, -1.0209, -1.0037, -1.0595, -0.9727], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0372, -1.0068, -0.9656, -1.0496, -1.0399])\n",
      "mean: tensor(-1.0198)\n",
      "iter_dt 6409.55ms; iter 84: train loss 0.01270 temperature: 9.200000000000006\n",
      "mean_logits tensor([-0.9306, -0.9577, -0.9265, -0.8705, -0.8815], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0146, -0.9683, -0.9772, -1.0290, -0.9580])\n",
      "mean: tensor(-0.9894)\n",
      "iter_dt 6508.55ms; iter 85: train loss 0.05449 temperature: 9.250000000000007\n",
      "mean_logits tensor([-1.0826, -0.9887, -0.9881, -1.0218, -1.0562], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9213, -1.0055, -1.0138, -0.9939, -0.9867])\n",
      "mean: tensor(-0.9843)\n",
      "iter_dt 6446.52ms; iter 86: train loss 0.04866 temperature: 9.300000000000008\n",
      "mean_logits tensor([-1.0531, -0.9658, -1.0194, -0.9038, -1.0078], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9883, -0.9944, -1.0344, -1.0000, -0.9857])\n",
      "mean: tensor(-1.0006)\n",
      "iter_dt 6401.64ms; iter 87: train loss 0.02113 temperature: 9.350000000000009\n",
      "mean_logits tensor([-0.9751, -1.0440, -0.9975, -1.0676, -0.9402], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0250, -0.9942, -1.0217, -1.0542, -1.0281])\n",
      "mean: tensor(-1.0247)\n",
      "iter_dt 6404.44ms; iter 88: train loss 0.01973 temperature: 9.40000000000001\n",
      "mean_logits tensor([-1.0462, -1.0316, -1.0692, -0.9419, -0.9945], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0413, -1.0096, -0.9783, -0.9227, -0.9526])\n",
      "mean: tensor(-0.9809)\n",
      "iter_dt 6406.63ms; iter 89: train loss 0.01655 temperature: 9.45000000000001\n",
      "mean_logits tensor([-0.9388, -1.0309, -0.9017, -1.0506, -1.1004], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0203, -1.0212, -1.0185, -0.8747, -1.0371])\n",
      "mean: tensor(-0.9944)\n",
      "iter_dt 6410.33ms; iter 90: train loss 0.07751 temperature: 9.50000000000001\n",
      "mean_logits tensor([-1.0214, -1.0521, -0.9069, -1.0181, -1.0433], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0499, -1.0418, -1.0453, -0.9950, -1.0103])\n",
      "mean: tensor(-1.0285)\n",
      "iter_dt 6418.65ms; iter 91: train loss 0.03099 temperature: 9.550000000000011\n",
      "mean_logits tensor([-1.0483, -1.0096, -0.9753, -0.8679, -0.9560], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0442, -1.0372, -1.0513, -1.0057, -1.0040])\n",
      "mean: tensor(-1.0285)\n",
      "iter_dt 6398.21ms; iter 92: train loss 0.03800 temperature: 9.600000000000012\n",
      "mean_logits tensor([-1.0086, -1.0901, -0.9248, -1.1190, -1.0551], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9913, -1.0321, -1.0226, -1.0511, -1.0322])\n",
      "mean: tensor(-1.0259)\n",
      "iter_dt 6414.50ms; iter 93: train loss 0.02841 temperature: 9.650000000000013\n",
      "mean_logits tensor([-0.9341, -0.9955, -1.0395, -1.0581, -1.1149], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0440, -1.0414, -1.0346, -1.0450, -0.9750])\n",
      "mean: tensor(-1.0280)\n",
      "iter_dt 6332.96ms; iter 94: train loss 0.05278 temperature: 9.700000000000014\n",
      "mean_logits tensor([-1.0134, -0.9663, -1.0729, -1.0215, -1.0930], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0421, -0.9887, -1.0437, -0.9533, -1.0326])\n",
      "mean: tensor(-1.0121)\n",
      "iter_dt 6415.17ms; iter 95: train loss 0.01622 temperature: 9.750000000000014\n",
      "mean_logits tensor([-1.0048, -0.9207, -1.0563, -1.0481, -0.9877], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9825, -1.0000, -0.9957, -0.9360, -1.0398])\n",
      "mean: tensor(-0.9908)\n",
      "iter_dt 6410.84ms; iter 96: train loss 0.03745 temperature: 9.800000000000015\n",
      "mean_logits tensor([-1.0299, -1.0097, -1.0129, -1.0303, -1.0551], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9727, -1.0501, -1.0539, -1.0349, -0.9782])\n",
      "mean: tensor(-1.0180)\n",
      "iter_dt 6408.26ms; iter 97: train loss 0.01916 temperature: 9.850000000000016\n",
      "mean_logits tensor([-1.0136, -1.0243, -0.9686, -1.0869, -1.0467], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0345, -1.0175, -1.0227, -1.0376, -1.0306])\n",
      "mean: tensor(-1.0286)\n",
      "iter_dt 6430.58ms; iter 98: train loss 0.00950 temperature: 9.900000000000016\n",
      "mean_logits tensor([-1.0393, -1.1417, -1.0709, -1.0377, -1.0744], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0198, -0.9677, -1.0294, -0.9993, -1.0337])\n",
      "mean: tensor(-1.0100)\n",
      "iter_dt 6411.80ms; iter 99: train loss 0.05844 temperature: 9.950000000000017\n",
      "converged SCF energy = -1.1011282422677\n",
      "Starting to parse FermionOperator using 4 qubits...\n",
      "\n",
      "Operator t:  0.13236616802789528 [] +\n",
      "-0.04343266093641473 [X0 X1 Y2 Y3] +\n",
      "0.04343266093641473 [X0 Y1 Y2 X3] +\n",
      "0.04343266093641473 [Y0 X1 X2 Y3] +\n",
      "-0.04343266093641473 [Y0 Y1 X2 X3] +\n",
      "0.1948086773502512 [Z0] +\n",
      "0.17533443228762685 [Z0 Z1] +\n",
      "0.12876561341512974 [Z0 Z2] +\n",
      "0.17219827435154444 [Z0 Z3] +\n",
      "0.19480867735025129 [Z1] +\n",
      "0.17219827435154444 [Z1 Z2] +\n",
      "0.12876561341512974 [Z1 Z3] +\n",
      "-0.299205109236114 [Z2] +\n",
      "0.18112650612285508 [Z2 Z3] +\n",
      "-0.299205109236114 [Z3]\n",
      "Term, coeff:  () 0.13236616802789528\n",
      "Term, coeff:  ((0, 'Z'),) 0.1948086773502512\n",
      "Index, p_char:  0 Z\n",
      "Term, coeff:  ((1, 'Z'),) 0.19480867735025129\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((2, 'Z'),) -0.299205109236114\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((3, 'Z'),) -0.299205109236114\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((0, 'Z'), (1, 'Z')) 0.17533443228762685\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((0, 'Y'), (1, 'X'), (2, 'X'), (3, 'Y')) 0.04343266093641473\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'Y'), (1, 'Y'), (2, 'X'), (3, 'X')) -0.04343266093641473\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'X'), (1, 'X'), (2, 'Y'), (3, 'Y')) -0.04343266093641473\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'X'), (1, 'Y'), (2, 'Y'), (3, 'X')) 0.04343266093641473\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'Z'), (2, 'Z')) 0.12876561341512974\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((0, 'Z'), (3, 'Z')) 0.17219827435154444\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((1, 'Z'), (2, 'Z')) 0.17219827435154444\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((1, 'Z'), (3, 'Z')) 0.12876561341512974\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((2, 'Z'), (3, 'Z')) 0.18112650612285508\n",
      "Index, p_char:  2 Z\n",
      "Index, p_char:  3 Z\n",
      "ground state: -1.1162860068695402\n",
      "hf state: -1.1011282422677016\n",
      "number of parameters: 85.29M\n",
      "running on device cpu\n",
      "mean_logits tensor([-1.1676, -0.9894, -1.0338, -1.0321, -1.2728], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1021, -1.0924, -1.0446, -1.0621, -0.9224])\n",
      "mean: tensor(-1.0447)\n",
      "iter_dt 0.00ms; iter 0: train loss 0.24976 temperature: 5\n",
      "mean_logits tensor([-0.9255, -1.4398, -0.9669, -0.9280, -0.9414], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0019, -1.1157, -1.1023, -1.0404, -1.0769])\n",
      "mean: tensor(-1.0675)\n",
      "iter_dt 1694968764633.83ms; iter 1: train loss 0.35575 temperature: 5.05\n",
      "mean_logits tensor([-0.9954, -1.0432, -1.1477, -1.2323, -1.0114], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9461, -1.1031, -1.0806, -0.2644, -1.0212])\n",
      "mean: tensor(-0.8831)\n",
      "iter_dt 6522.59ms; iter 2: train loss 0.92238 temperature: 5.1\n",
      "mean_logits tensor([-1.0050, -0.9317, -1.0435, -1.0856, -1.0331], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8792, -1.0678, -1.0032, -1.1108, -1.0191])\n",
      "mean: tensor(-1.0160)\n",
      "iter_dt 6588.70ms; iter 3: train loss 0.05224 temperature: 5.1499999999999995\n",
      "mean_logits tensor([-0.8609, -1.0741, -1.0545, -0.9809, -1.1091], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0832, -1.1006, -0.9455, -1.0629, -0.6969])\n",
      "mean: tensor(-0.9778)\n",
      "iter_dt 6476.62ms; iter 4: train loss 0.30829 temperature: 5.199999999999999\n",
      "mean_logits tensor([-1.0412, -0.9056, -0.9922, -0.9396, -0.8495], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0289, -0.7822, -1.1041, -1.0489, -0.9525])\n",
      "mean: tensor(-0.9833)\n",
      "iter_dt 6450.71ms; iter 5: train loss 0.06746 temperature: 5.249999999999999\n",
      "mean_logits tensor([-0.9968, -1.0523, -0.9706, -0.8672, -0.8854], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0351, -1.1150, -1.0104, -0.9854, -1.0354])\n",
      "mean: tensor(-1.0363)\n",
      "iter_dt 6438.22ms; iter 6: train loss 0.06001 temperature: 5.299999999999999\n",
      "mean_logits tensor([-0.9865, -1.0425, -0.9996, -1.1070, -0.9948], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8912, -0.7361, -1.0486, -1.0095, -0.8303])\n",
      "mean: tensor(-0.9031)\n",
      "iter_dt 6412.12ms; iter 7: train loss 0.17715 temperature: 5.349999999999999\n",
      "mean_logits tensor([-1.4174, -1.1694, -1.0258, -1.0725, -1.2192], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1075, -1.1144, -1.0106, -1.0897, -1.0404])\n",
      "mean: tensor(-1.0725)\n",
      "iter_dt 6496.66ms; iter 8: train loss 0.31003 temperature: 5.399999999999999\n",
      "mean_logits tensor([-0.9663, -0.8118, -1.0015, -1.0141, -1.0842], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0199, -1.0135, -0.8764, -1.0387, -1.1099])\n",
      "mean: tensor(-1.0117)\n",
      "iter_dt 6426.62ms; iter 9: train loss 0.07745 temperature: 5.449999999999998\n",
      "mean_logits tensor([-0.9913, -1.3027, -0.9971, -0.9933, -0.9609], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8973, -1.0288, -1.0877, -0.9455, -1.0627])\n",
      "mean: tensor(-1.0044)\n",
      "iter_dt 6372.55ms; iter 10: train loss 0.19908 temperature: 5.499999999999998\n",
      "mean_logits tensor([-1.0974, -1.1476, -1.1886, -0.9548, -1.0357], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0647, -1.0564, -1.0680, -0.9822, -1.0907])\n",
      "mean: tensor(-1.0524)\n",
      "iter_dt 6425.89ms; iter 11: train loss 0.05087 temperature: 5.549999999999998\n",
      "mean_logits tensor([-1.0762, -0.8727, -1.0265, -1.2202, -1.0392], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0394, -0.9346, -1.0841, -1.0771, -1.0871])\n",
      "mean: tensor(-1.0445)\n",
      "iter_dt 6433.65ms; iter 12: train loss 0.05707 temperature: 5.599999999999998\n",
      "mean_logits tensor([-1.0199, -0.8790, -0.9979, -0.9923, -1.0982], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0627, -1.0934, -1.0675, -1.0819, -1.0815])\n",
      "mean: tensor(-1.0774)\n",
      "iter_dt 6406.85ms; iter 13: train loss 0.09019 temperature: 5.649999999999998\n",
      "mean_logits tensor([-1.0273, -1.0167, -0.9599, -0.8601, -0.9580], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9851, -1.0891, -1.0962, -1.0373, -1.0256])\n",
      "mean: tensor(-1.0467)\n",
      "iter_dt 6511.19ms; iter 14: train loss 0.08898 temperature: 5.6999999999999975\n",
      "mean_logits tensor([-1.1603, -1.1220, -0.9937, -0.8949, -1.0441], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0742, -1.0162, -1.1113, -1.0719, -1.0408])\n",
      "mean: tensor(-1.0629)\n",
      "iter_dt 6586.83ms; iter 15: train loss 0.10050 temperature: 5.749999999999997\n",
      "mean_logits tensor([-1.0092, -1.0143, -1.0496, -0.9969, -0.9328], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0039, -1.0909, -0.9889, -1.0289, -1.0274])\n",
      "mean: tensor(-1.0280)\n",
      "iter_dt 6495.36ms; iter 16: train loss 0.02962 temperature: 5.799999999999997\n",
      "mean_logits tensor([-1.0275, -1.0603, -0.9651, -1.0574, -1.1532], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9955, -1.0848, -1.0662, -1.0251, -1.1127])\n",
      "mean: tensor(-1.0568)\n",
      "iter_dt 6648.37ms; iter 17: train loss 0.02300 temperature: 5.849999999999997\n",
      "mean_logits tensor([-1.2148, -1.2324, -1.0283, -1.1100, -1.1710], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0405, -0.8676, -1.0070, -1.0753, -1.1002])\n",
      "mean: tensor(-1.0181)\n",
      "iter_dt 6481.04ms; iter 18: train loss 0.29043 temperature: 5.899999999999997\n",
      "mean_logits tensor([-1.0563, -1.0380, -1.0420, -1.2883, -1.0665], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0273, -1.0842, -1.0965, -1.0955, -1.0292])\n",
      "mean: tensor(-1.0665)\n",
      "iter_dt 6451.79ms; iter 19: train loss 0.09306 temperature: 5.949999999999997\n",
      "mean_logits tensor([-1.2253, -1.0776, -1.0123, -1.0812, -1.1384], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0035, -1.0395, -1.1080, -1.0638, -0.9444])\n",
      "mean: tensor(-1.0318)\n",
      "iter_dt 6442.82ms; iter 20: train loss 0.17065 temperature: 5.9999999999999964\n",
      "mean_logits tensor([-1.0588, -0.9793, -0.9746, -1.0331, -1.0880], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0701, -1.0163, -1.0680, -1.0739, -0.9380])\n",
      "mean: tensor(-1.0333)\n",
      "iter_dt 6412.27ms; iter 21: train loss 0.05260 temperature: 6.049999999999996\n",
      "mean_logits tensor([-1.1836, -1.1910, -1.0790, -1.0902, -1.1054], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0665, -1.0524, -1.1066, -1.0677, -1.0969])\n",
      "mean: tensor(-1.0780)\n",
      "iter_dt 6452.90ms; iter 22: train loss 0.06473 temperature: 6.099999999999996\n",
      "mean_logits tensor([-1.0757, -1.0929, -1.1362, -1.0445, -1.0617], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0444, -0.8619, -1.0980, -0.8620, -0.7790])\n",
      "mean: tensor(-0.9290)\n",
      "iter_dt 6610.92ms; iter 23: train loss 0.22645 temperature: 6.149999999999996\n",
      "mean_logits tensor([-0.9222, -1.0556, -0.9423, -1.0314, -1.0450], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9034, -0.9103, -1.1112, -1.0582, -1.0147])\n",
      "mean: tensor(-0.9995)\n",
      "iter_dt 6525.73ms; iter 24: train loss 0.07783 temperature: 6.199999999999996\n",
      "mean_logits tensor([-1.0236, -1.0087, -1.1163, -1.0867, -0.9669], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0884, -0.9320, -0.9194, -1.0238, -1.0715])\n",
      "mean: tensor(-1.0070)\n",
      "iter_dt 6460.49ms; iter 25: train loss 0.09807 temperature: 6.249999999999996\n",
      "mean_logits tensor([-0.9520, -0.9510, -1.0454, -1.1555, -1.0816], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1033, -1.0418, -1.0106, -1.0999, -1.0384])\n",
      "mean: tensor(-1.0588)\n",
      "iter_dt 6394.09ms; iter 26: train loss 0.05886 temperature: 6.299999999999995\n",
      "mean_logits tensor([-0.9280, -0.9142, -1.0922, -0.9684, -1.1855], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0731, -1.0404, -1.0953, -1.0976, -1.0362])\n",
      "mean: tensor(-1.0685)\n",
      "iter_dt 6559.63ms; iter 27: train loss 0.12134 temperature: 6.349999999999995\n",
      "mean_logits tensor([-0.9704, -0.9615, -1.2155, -0.9195, -1.1248], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1017, -1.0766, -1.0701, -1.1113, -1.0797])\n",
      "mean: tensor(-1.0879)\n",
      "iter_dt 6515.44ms; iter 28: train loss 0.14933 temperature: 6.399999999999995\n",
      "mean_logits tensor([-1.0258, -0.9580, -1.0904, -0.9590, -0.9306], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0318, -1.0239, -1.0773, -1.0905, -1.0787])\n",
      "mean: tensor(-1.0604)\n",
      "iter_dt 6491.44ms; iter 29: train loss 0.06638 temperature: 6.449999999999995\n",
      "mean_logits tensor([-1.0669, -1.0300, -0.9205, -1.0415, -1.0384], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0875, -0.9845, -1.0600, -1.0872, -1.0898])\n",
      "mean: tensor(-1.0618)\n",
      "iter_dt 6435.66ms; iter 30: train loss 0.04005 temperature: 6.499999999999995\n",
      "mean_logits tensor([-0.9713, -1.0152, -1.1384, -1.0357, -1.0224], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0586, -0.9913, -1.0940, -1.0683, -1.0844])\n",
      "mean: tensor(-1.0593)\n",
      "iter_dt 6446.06ms; iter 31: train loss 0.02420 temperature: 6.5499999999999945\n",
      "mean_logits tensor([-1.0861, -1.0014, -1.0475, -1.0640, -1.0433], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0736, -1.1023, -1.0768, -1.0581, -0.8616])\n",
      "mean: tensor(-1.0345)\n",
      "iter_dt 6440.36ms; iter 32: train loss 0.06297 temperature: 6.599999999999994\n",
      "mean_logits tensor([-1.0677, -1.0292, -1.0856, -0.9566, -1.1401], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0486, -0.9708, -1.0439, -1.0599, -1.0742])\n",
      "mean: tensor(-1.0395)\n",
      "iter_dt 6453.90ms; iter 33: train loss 0.03257 temperature: 6.649999999999994\n",
      "mean_logits tensor([-1.1344, -1.0262, -1.0935, -1.0353, -1.1214], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1010, -1.0950, -1.0410, -1.0206, -1.0650])\n",
      "mean: tensor(-1.0645)\n",
      "iter_dt 6488.00ms; iter 34: train loss 0.02063 temperature: 6.699999999999994\n",
      "mean_logits tensor([-0.9866, -1.1428, -1.0694, -0.9568, -1.2367], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0924, -1.0864, -1.0440, -1.0814, -1.1147])\n",
      "mean: tensor(-1.0838)\n",
      "iter_dt 6474.41ms; iter 35: train loss 0.08003 temperature: 6.749999999999994\n",
      "mean_logits tensor([-1.0389, -1.0835, -0.9411, -1.0554, -0.9886], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0381, -1.0579, -1.0968, -1.0734, -1.0647])\n",
      "mean: tensor(-1.0662)\n",
      "iter_dt 6434.21ms; iter 36: train loss 0.04800 temperature: 6.799999999999994\n",
      "mean_logits tensor([-0.9987, -1.1407, -1.1116, -1.2048, -1.0999], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0426, -1.0815, -1.0844, -1.0678, -1.0802])\n",
      "mean: tensor(-1.0713)\n",
      "iter_dt 6463.02ms; iter 37: train loss 0.04795 temperature: 6.849999999999993\n",
      "mean_logits tensor([-1.0795, -1.0987, -1.0853, -1.0849, -1.1740], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0533, -1.0489, -1.0633, -1.0406, -0.9944])\n",
      "mean: tensor(-1.0401)\n",
      "iter_dt 6424.28ms; iter 38: train loss 0.06610 temperature: 6.899999999999993\n",
      "mean_logits tensor([-0.9881, -1.0078, -1.0062, -1.0396, -1.1611], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0965, -1.0863, -1.0613, -1.0645, -1.0493])\n",
      "mean: tensor(-1.0716)\n",
      "iter_dt 6411.40ms; iter 39: train loss 0.05754 temperature: 6.949999999999993\n",
      "mean_logits tensor([-1.1624, -1.1296, -1.0886, -0.9561, -1.1049], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0952, -1.0567, -0.9635, -0.9617, -1.1039])\n",
      "mean: tensor(-1.0362)\n",
      "iter_dt 6416.89ms; iter 40: train loss 0.04253 temperature: 6.999999999999993\n",
      "mean_logits tensor([-0.8677, -1.2154, -0.9684, -1.0287, -1.2283], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0778, -0.9951, -1.1038, -1.0948, -1.0671])\n",
      "mean: tensor(-1.0677)\n",
      "iter_dt 6469.73ms; iter 41: train loss 0.23912 temperature: 7.049999999999993\n",
      "mean_logits tensor([-1.0192, -0.9586, -1.0235, -1.1327, -1.1251], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0579, -1.0502, -1.0552, -1.1058, -1.0736])\n",
      "mean: tensor(-1.0686)\n",
      "iter_dt 6423.09ms; iter 42: train loss 0.02264 temperature: 7.0999999999999925\n",
      "mean_logits tensor([-1.0654, -1.1827, -1.1192, -0.9153, -1.0632], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0423, -1.0708, -1.0722, -1.0145, -1.0909])\n",
      "mean: tensor(-1.0581)\n",
      "iter_dt 6409.32ms; iter 43: train loss 0.04361 temperature: 7.149999999999992\n",
      "mean_logits tensor([-1.0813, -1.0456, -0.9369, -1.1457, -1.0396], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0742, -1.0677, -1.1084, -0.9567, -0.7846])\n",
      "mean: tensor(-0.9983)\n",
      "iter_dt 6411.66ms; iter 44: train loss 0.18625 temperature: 7.199999999999992\n",
      "mean_logits tensor([-1.0577, -1.0000, -1.0955, -1.0718, -1.0620], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0445, -1.0863, -1.1027, -1.0913, -1.0337])\n",
      "mean: tensor(-1.0717)\n",
      "iter_dt 6450.03ms; iter 45: train loss 0.01435 temperature: 7.249999999999992\n",
      "mean_logits tensor([-1.0612, -1.0278, -0.9710, -1.0561, -1.1160], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0017, -1.1008, -1.0744, -1.0774, -1.0668])\n",
      "mean: tensor(-1.0642)\n",
      "iter_dt 6400.59ms; iter 46: train loss 0.03616 temperature: 7.299999999999992\n",
      "mean_logits tensor([-1.0602, -1.1557, -1.0479, -1.0251, -1.0397], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0725, -1.0945, -1.0939, -1.0499, -1.1034])\n",
      "mean: tensor(-1.0828)\n",
      "iter_dt 6434.47ms; iter 47: train loss 0.01889 temperature: 7.349999999999992\n",
      "mean_logits tensor([-1.0163, -0.9434, -1.1640, -1.0208, -1.0725], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0750, -1.1064, -1.1099, -1.1034, -1.0771])\n",
      "mean: tensor(-1.0943)\n",
      "iter_dt 6418.93ms; iter 48: train loss 0.06404 temperature: 7.3999999999999915\n",
      "mean_logits tensor([-1.1207, -1.0069, -1.1153, -1.1332, -0.9670], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0578, -1.0925, -1.0228, -1.0928, -1.0075])\n",
      "mean: tensor(-1.0547)\n",
      "iter_dt 6429.66ms; iter 49: train loss 0.03886 temperature: 7.449999999999991\n",
      "mean_logits tensor([-1.0653, -1.1678, -1.1321, -0.9798, -1.0074], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0650, -1.0911, -1.0123, -1.1082, -1.0748])\n",
      "mean: tensor(-1.0703)\n",
      "iter_dt 6444.99ms; iter 50: train loss 0.06974 temperature: 7.499999999999991\n",
      "mean_logits tensor([-1.0238, -1.0109, -1.0204, -1.0516, -0.9975], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1057, -1.0700, -1.0949, -1.0658, -1.0643])\n",
      "mean: tensor(-1.0802)\n",
      "iter_dt 6440.86ms; iter 51: train loss 0.03348 temperature: 7.549999999999991\n",
      "mean_logits tensor([-1.0717, -0.9202, -0.9559, -1.0076, -1.0572], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0905, -1.1079, -1.0638, -1.0855, -1.0444])\n",
      "mean: tensor(-1.0784)\n",
      "iter_dt 6402.76ms; iter 52: train loss 0.08201 temperature: 7.599999999999991\n",
      "mean_logits tensor([-1.2128, -1.0927, -1.1147, -0.9617, -1.0804], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0750, -1.0809, -1.0299, -1.0314, -1.0654])\n",
      "mean: tensor(-1.0565)\n",
      "iter_dt 6442.45ms; iter 53: train loss 0.05759 temperature: 7.649999999999991\n",
      "mean_logits tensor([-1.1218, -1.0369, -1.1711, -1.0537, -1.0446], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0994, -1.0059, -1.1140, -1.0474, -0.9323])\n",
      "mean: tensor(-1.0398)\n",
      "iter_dt 6525.64ms; iter 54: train loss 0.02712 temperature: 7.69999999999999\n",
      "mean_logits tensor([-1.1403, -1.1492, -1.1307, -1.0555, -1.0307], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0462, -1.0133, -0.9954, -1.0743, -1.1141])\n",
      "mean: tensor(-1.0487)\n",
      "iter_dt 6447.00ms; iter 55: train loss 0.09111 temperature: 7.74999999999999\n",
      "mean_logits tensor([-0.9268, -1.0022, -1.1682, -1.1280, -1.1589], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0929, -1.0520, -1.0754, -1.1023, -1.0757])\n",
      "mean: tensor(-1.0797)\n",
      "iter_dt 6384.66ms; iter 56: train loss 0.07599 temperature: 7.79999999999999\n",
      "mean_logits tensor([-1.1401, -0.9004, -1.0919, -1.1151, -1.0016], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0560, -1.1070, -1.0833, -1.0961, -1.0953])\n",
      "mean: tensor(-1.0875)\n",
      "iter_dt 6425.38ms; iter 57: train loss 0.09159 temperature: 7.84999999999999\n",
      "mean_logits tensor([-1.0205, -1.1657, -1.0975, -1.1438, -0.9500], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0994, -0.9697, -1.1090, -1.1055, -1.0387])\n",
      "mean: tensor(-1.0645)\n",
      "iter_dt 6423.48ms; iter 58: train loss 0.09012 temperature: 7.89999999999999\n",
      "mean_logits tensor([-1.0192, -1.0443, -1.0915, -1.0252, -1.1118], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0864, -1.0527, -1.0921, -1.0950, -1.1014])\n",
      "mean: tensor(-1.0855)\n",
      "iter_dt 6496.22ms; iter 59: train loss 0.01586 temperature: 7.9499999999999895\n",
      "mean_logits tensor([-1.0697, -1.0678, -1.0664, -1.0840, -1.0510], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0798, -0.9997, -1.0804, -1.0965, -1.1062])\n",
      "mean: tensor(-1.0725)\n",
      "iter_dt 6528.21ms; iter 60: train loss 0.01339 temperature: 7.999999999999989\n",
      "mean_logits tensor([-1.0570, -0.9623, -1.0418, -1.0629, -1.0783], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0548, -1.0177, -1.1103, -1.1094, -1.0929])\n",
      "mean: tensor(-1.0770)\n",
      "iter_dt 6409.70ms; iter 61: train loss 0.01669 temperature: 8.04999999999999\n",
      "mean_logits tensor([-1.0829, -1.1936, -1.1825, -1.1056, -1.1401], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0753, -1.0899, -1.0881, -1.0739, -1.0982])\n",
      "mean: tensor(-1.0851)\n",
      "iter_dt 6491.87ms; iter 62: train loss 0.04355 temperature: 8.09999999999999\n",
      "mean_logits tensor([-1.0212, -0.9383, -1.0418, -1.1330, -1.0388], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0191, -1.0907, -1.0742, -1.0689, -1.0682])\n",
      "mean: tensor(-1.0642)\n",
      "iter_dt 6693.69ms; iter 63: train loss 0.04597 temperature: 8.149999999999991\n",
      "mean_logits tensor([-1.0099, -1.1483, -1.0465, -1.1737, -1.0911], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0519, -1.0240, -1.0204, -1.0381, -1.0962])\n",
      "mean: tensor(-1.0461)\n",
      "iter_dt 6616.35ms; iter 64: train loss 0.06469 temperature: 8.199999999999992\n",
      "mean_logits tensor([-1.1313, -1.1384, -1.1110, -1.1882, -1.0385], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0791, -1.1011, -1.0715, -1.0392, -1.1095])\n",
      "mean: tensor(-1.0801)\n",
      "iter_dt 6521.65ms; iter 65: train loss 0.06027 temperature: 8.249999999999993\n",
      "mean_logits tensor([-1.1313, -1.1146, -1.0149, -1.0765, -1.1634], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0955, -1.0852, -1.1101, -1.1015, -1.1113])\n",
      "mean: tensor(-1.1007)\n",
      "iter_dt 6476.06ms; iter 66: train loss 0.02550 temperature: 8.299999999999994\n",
      "mean_logits tensor([-1.1399, -1.1333, -1.0956, -1.0026, -1.0611], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0949, -1.0999, -1.0811, -0.8891, -1.0783])\n",
      "mean: tensor(-1.0486)\n",
      "iter_dt 6488.43ms; iter 67: train loss 0.02384 temperature: 8.349999999999994\n",
      "mean_logits tensor([-1.1569, -1.0240, -1.0238, -1.0615, -1.0872], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0099, -1.0409, -1.0911, -1.0907, -1.0407])\n",
      "mean: tensor(-1.0546)\n",
      "iter_dt 6467.19ms; iter 68: train loss 0.05085 temperature: 8.399999999999995\n",
      "mean_logits tensor([-1.1275, -1.1668, -1.1815, -1.0832, -1.1768], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0617, -1.0672, -1.0852, -1.1024, -1.0642])\n",
      "mean: tensor(-1.0761)\n",
      "iter_dt 6519.06ms; iter 69: train loss 0.06874 temperature: 8.449999999999996\n",
      "mean_logits tensor([-1.0527, -1.1476, -0.9626, -1.0675, -1.1210], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9280, -1.0762, -1.0276, -1.0642, -1.1086])\n",
      "mean: tensor(-1.0409)\n",
      "iter_dt 6481.05ms; iter 70: train loss 0.03850 temperature: 8.499999999999996\n",
      "mean_logits tensor([-1.1075, -1.0171, -1.1001, -1.0714, -1.1037], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0980, -1.1033, -1.1128, -1.1051, -1.1077])\n",
      "mean: tensor(-1.1054)\n",
      "iter_dt 6529.83ms; iter 71: train loss 0.01487 temperature: 8.549999999999997\n",
      "mean_logits tensor([-1.1193, -1.0023, -1.0003, -0.9404, -1.0992], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1100, -1.0898, -1.0864, -1.0749, -1.1008])\n",
      "mean: tensor(-1.0924)\n",
      "iter_dt 6450.53ms; iter 72: train loss 0.05174 temperature: 8.599999999999998\n",
      "mean_logits tensor([-1.0101, -1.0848, -0.8692, -1.0575, -1.1678], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9990, -1.0495, -1.1030, -1.1009, -1.1058])\n",
      "mean: tensor(-1.0716)\n",
      "iter_dt 6490.15ms; iter 73: train loss 0.09196 temperature: 8.649999999999999\n",
      "mean_logits tensor([-1.0197, -1.0812, -1.0831, -0.8866, -1.0715], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8564, -1.1142, -1.0840, -1.1076, -1.0703])\n",
      "mean: tensor(-1.0465)\n",
      "iter_dt 6476.64ms; iter 74: train loss 0.10891 temperature: 8.7\n",
      "mean_logits tensor([-0.9877, -1.0458, -1.0978, -0.9503, -1.1285], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1019, -1.0797, -1.0203, -1.0143, -1.0244])\n",
      "mean: tensor(-1.0481)\n",
      "iter_dt 6457.19ms; iter 75: train loss 0.05754 temperature: 8.75\n",
      "mean_logits tensor([-1.0933, -1.0088, -0.9422, -1.0253, -1.0278], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0950, -1.1077, -1.0207, -1.0194, -1.0000])\n",
      "mean: tensor(-1.0486)\n",
      "iter_dt 6462.27ms; iter 76: train loss 0.02625 temperature: 8.8\n",
      "mean_logits tensor([-1.0566, -1.0971, -1.0788, -0.9687, -0.9706], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0843, -1.0708, -0.8541, -1.0854, -1.0907])\n",
      "mean: tensor(-1.0370)\n",
      "iter_dt 6415.84ms; iter 77: train loss 0.11648 temperature: 8.850000000000001\n",
      "mean_logits tensor([-1.1159, -1.1694, -1.0770, -1.0457, -1.1349], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0625, -1.0816, -1.1118, -1.1057, -1.0378])\n",
      "mean: tensor(-1.0799)\n",
      "iter_dt 6355.66ms; iter 78: train loss 0.04461 temperature: 8.900000000000002\n",
      "mean_logits tensor([-0.9658, -1.0291, -1.0663, -0.9973, -1.0367], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0681, -1.0248, -1.0443, -1.1074, -1.1126])\n",
      "mean: tensor(-1.0715)\n",
      "iter_dt 6363.28ms; iter 79: train loss 0.04663 temperature: 8.950000000000003\n",
      "mean_logits tensor([-1.1708, -1.1427, -0.9450, -1.0332, -1.1517], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9994, -1.0998, -1.0973, -1.0879, -1.1137])\n",
      "mean: tensor(-1.0796)\n",
      "iter_dt 6426.01ms; iter 80: train loss 0.09862 temperature: 9.000000000000004\n",
      "mean_logits tensor([-1.0561, -1.1354, -1.1790, -0.9252, -1.0880], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0902, -1.0858, -1.0604, -1.0816, -1.1130])\n",
      "mean: tensor(-1.0862)\n",
      "iter_dt 6387.95ms; iter 81: train loss 0.07056 temperature: 9.050000000000004\n",
      "mean_logits tensor([-1.0413, -1.0628, -1.1398, -1.1553, -1.0964], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1008, -1.0886, -1.1045, -1.0380, -0.9899])\n",
      "mean: tensor(-1.0644)\n",
      "iter_dt 6414.97ms; iter 82: train loss 0.05252 temperature: 9.100000000000005\n",
      "mean_logits tensor([-1.0707, -1.0410, -1.0549, -1.1041, -1.1354], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1092, -0.9795, -1.0586, -1.0555, -1.1035])\n",
      "mean: tensor(-1.0613)\n",
      "iter_dt 6436.37ms; iter 83: train loss 0.01435 temperature: 9.150000000000006\n",
      "mean_logits tensor([-1.1744, -1.2318, -1.0996, -1.2231, -1.1253], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0948, -1.0995, -1.1118, -1.1156, -1.0936])\n",
      "mean: tensor(-1.1031)\n",
      "iter_dt 6444.07ms; iter 84: train loss 0.07445 temperature: 9.200000000000006\n",
      "mean_logits tensor([-1.0761, -1.0444, -1.0381, -1.0717, -1.0575], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0239, -1.0390, -1.0727, -1.0373, -1.0606])\n",
      "mean: tensor(-1.0467)\n",
      "iter_dt 6477.64ms; iter 85: train loss 0.00844 temperature: 9.250000000000007\n",
      "mean_logits tensor([-1.1174, -1.0175, -0.9920, -1.0853, -1.0840], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1008, -1.0866, -0.9928, -1.0551, -1.0139])\n",
      "mean: tensor(-1.0498)\n",
      "iter_dt 6434.94ms; iter 86: train loss 0.01791 temperature: 9.300000000000008\n",
      "mean_logits tensor([-1.1395, -1.0429, -1.0751, -1.0659, -1.0507], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0841, -1.1078, -1.0949, -1.0927, -1.0786])\n",
      "mean: tensor(-1.0916)\n",
      "iter_dt 6538.29ms; iter 87: train loss 0.01615 temperature: 9.350000000000009\n",
      "mean_logits tensor([-1.0406, -1.0584, -1.1269, -1.0828, -1.0852], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0358, -1.0730, -1.0852, -1.0673, -1.0758])\n",
      "mean: tensor(-1.0674)\n",
      "iter_dt 6421.90ms; iter 88: train loss 0.00414 temperature: 9.40000000000001\n",
      "mean_logits tensor([-1.0088, -1.0743, -1.1399, -1.0830, -1.0575], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1045, -1.0639, -1.1030, -1.0870, -1.1136])\n",
      "mean: tensor(-1.0944)\n",
      "iter_dt 6420.30ms; iter 89: train loss 0.02349 temperature: 9.45000000000001\n",
      "mean_logits tensor([-1.0085, -1.0418, -1.1183, -1.0109, -1.0237], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0493, -1.0777, -1.0934, -0.9647, -1.1053])\n",
      "mean: tensor(-1.0581)\n",
      "iter_dt 6461.26ms; iter 90: train loss 0.02018 temperature: 9.50000000000001\n",
      "mean_logits tensor([-1.0748, -1.1746, -1.1248, -1.0363, -1.0663], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0898, -1.0983, -1.0698, -1.0936, -1.0834])\n",
      "mean: tensor(-1.0870)\n",
      "iter_dt 6465.83ms; iter 91: train loss 0.02319 temperature: 9.550000000000011\n",
      "mean_logits tensor([-0.9995, -1.0675, -1.0258, -1.1275, -0.9814], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0291, -0.9517, -1.1010, -1.0844, -1.1011])\n",
      "mean: tensor(-1.0535)\n",
      "iter_dt 6459.46ms; iter 92: train loss 0.05745 temperature: 9.600000000000012\n",
      "mean_logits tensor([-0.9662, -1.0938, -1.0789, -0.9929, -1.0287], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0953, -1.0935, -1.1036, -0.9876, -1.0694])\n",
      "mean: tensor(-1.0699)\n",
      "iter_dt 6440.73ms; iter 93: train loss 0.03008 temperature: 9.650000000000013\n",
      "mean_logits tensor([-1.0085, -1.0642, -1.0341, -0.9953, -0.9787], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0887, -1.0307, -1.0900, -1.1058, -1.0650])\n",
      "mean: tensor(-1.0760)\n",
      "iter_dt 6444.06ms; iter 94: train loss 0.04905 temperature: 9.700000000000014\n",
      "mean_logits tensor([-1.0473, -1.1105, -1.0059, -1.1178, -1.0766], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0471, -1.0890, -1.0517, -1.0844, -1.1136])\n",
      "mean: tensor(-1.0772)\n",
      "iter_dt 6437.31ms; iter 95: train loss 0.00858 temperature: 9.750000000000014\n",
      "mean_logits tensor([-1.0593, -1.0953, -1.1347, -1.1941, -1.0589], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0821, -1.0789, -1.1091, -1.0466, -1.1092])\n",
      "mean: tensor(-1.0852)\n",
      "iter_dt 6542.86ms; iter 96: train loss 0.04801 temperature: 9.800000000000015\n",
      "mean_logits tensor([-1.0965, -1.1086, -1.0768, -1.0306, -1.0772], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0721, -1.0746, -1.0683, -1.0887, -1.0417])\n",
      "mean: tensor(-1.0691)\n",
      "iter_dt 6482.56ms; iter 97: train loss 0.01095 temperature: 9.850000000000016\n",
      "mean_logits tensor([-1.1044, -1.1304, -0.9994, -1.0904, -1.1193], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1001, -1.0887, -0.9955, -1.0776, -1.0420])\n",
      "mean: tensor(-1.0608)\n",
      "iter_dt 6405.92ms; iter 98: train loss 0.01395 temperature: 9.900000000000016\n",
      "mean_logits tensor([-1.1400, -1.1147, -1.0479, -1.0370, -0.9246], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1112, -1.0892, -1.1110, -1.1026, -1.0969])\n",
      "mean: tensor(-1.1022)\n",
      "iter_dt 6475.89ms; iter 99: train loss 0.06189 temperature: 9.950000000000017\n",
      "converged SCF energy = -1.11734903499028\n",
      "Starting to parse FermionOperator using 4 qubits...\n",
      "\n",
      "Operator t:  -0.042078976477822244 [] +\n",
      "-0.04475014401535168 [X0 X1 Y2 Y3] +\n",
      "0.04475014401535168 [X0 Y1 Y2 X3] +\n",
      "0.04475014401535168 [Y0 X1 X2 Y3] +\n",
      "-0.04475014401535168 [Y0 Y1 X2 X3] +\n",
      "0.1777128746513991 [Z0] +\n",
      "0.1705973832880105 [Z0 Z1] +\n",
      "0.12293305056183806 [Z0 Z2] +\n",
      "0.16768319457718975 [Z0 Z3] +\n",
      "0.1777128746513991 [Z1] +\n",
      "0.16768319457718975 [Z1 Z2] +\n",
      "0.12293305056183806 [Z1 Z3] +\n",
      "-0.24274280513140478 [Z2] +\n",
      "0.1762764080431961 [Z2 Z3] +\n",
      "-0.24274280513140478 [Z3]\n",
      "Term, coeff:  () -0.042078976477822244\n",
      "Term, coeff:  ((0, 'Z'),) 0.1777128746513991\n",
      "Index, p_char:  0 Z\n",
      "Term, coeff:  ((1, 'Z'),) 0.1777128746513991\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((2, 'Z'),) -0.24274280513140478\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((3, 'Z'),) -0.24274280513140478\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((0, 'Z'), (1, 'Z')) 0.1705973832880105\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((0, 'Y'), (1, 'X'), (2, 'X'), (3, 'Y')) 0.04475014401535168\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'Y'), (1, 'Y'), (2, 'X'), (3, 'X')) -0.04475014401535168\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'X'), (1, 'X'), (2, 'Y'), (3, 'Y')) -0.04475014401535168\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'X'), (1, 'Y'), (2, 'Y'), (3, 'X')) 0.04475014401535168\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'Z'), (2, 'Z')) 0.12293305056183806\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((0, 'Z'), (3, 'Z')) 0.16768319457718975\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((1, 'Z'), (2, 'Z')) 0.16768319457718975\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((1, 'Z'), (3, 'Z')) 0.12293305056183806\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((2, 'Z'), (3, 'Z')) 0.1762764080431961\n",
      "Index, p_char:  2 Z\n",
      "Index, p_char:  3 Z\n",
      "ground state: -1.136189454065922\n",
      "hf state: -1.1173490349902788\n",
      "number of parameters: 85.29M\n",
      "running on device cpu\n",
      "mean_logits tensor([-1.1676, -0.9894, -1.0338, -1.0321, -1.2728], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1282, -1.1111, -1.0832, -1.0989, -0.9519])\n",
      "mean: tensor(-1.0747)\n",
      "iter_dt 0.00ms; iter 0: train loss 0.23107 temperature: 5\n",
      "mean_logits tensor([-0.9138, -0.9494, -0.9739, -0.9740, -0.8944], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9229, -1.1013, -1.1203, -1.0256, -1.1062])\n",
      "mean: tensor(-1.0553)\n",
      "iter_dt 1694969423201.21ms; iter 1: train loss 0.14140 temperature: 5.05\n",
      "mean_logits tensor([-1.0646, -1.3029, -1.3588, -1.2981, -1.0717], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1005, -1.1075, -1.0508, -0.4751, -1.1189])\n",
      "mean: tensor(-0.9706)\n",
      "iter_dt 6603.43ms; iter 2: train loss 1.14838 temperature: 5.1\n",
      "mean_logits tensor([-1.1322, -1.2457, -1.1758, -1.1562, -1.1454], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0459, -1.0796, -1.0777, -1.1293, -0.9951])\n",
      "mean: tensor(-1.0655)\n",
      "iter_dt 6480.66ms; iter 3: train loss 0.12808 temperature: 5.1499999999999995\n",
      "mean_logits tensor([-1.1613, -1.2809, -1.2204, -1.2471, -1.1945], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0886, -1.1156, -1.0660, -0.9172, -0.8939])\n",
      "mean: tensor(-1.0163)\n",
      "iter_dt 6493.40ms; iter 4: train loss 0.45538 temperature: 5.199999999999999\n",
      "mean_logits tensor([-1.2582, -1.0335, -1.2475, -0.8396, -1.0900], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1201, -1.0407, -1.1266, -1.1172, -1.0772])\n",
      "mean: tensor(-1.0964)\n",
      "iter_dt 6439.31ms; iter 5: train loss 0.18278 temperature: 5.249999999999999\n",
      "mean_logits tensor([-1.1069, -1.0329, -1.1527, -1.0596, -1.0817], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1133, -0.8687, -1.0971, -1.0946, -1.1033])\n",
      "mean: tensor(-1.0554)\n",
      "iter_dt 6405.25ms; iter 6: train loss 0.04510 temperature: 5.299999999999999\n",
      "mean_logits tensor([-1.0975, -1.0219, -1.0076, -1.0733, -0.9519], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0622, -0.9523, -0.9379, -1.0504, -0.9671])\n",
      "mean: tensor(-0.9940)\n",
      "iter_dt 6421.56ms; iter 7: train loss 0.01714 temperature: 5.349999999999999\n",
      "mean_logits tensor([-1.1432, -0.8690, -0.8807, -1.0088, -1.0603], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0457, -1.0929, -1.0964, -0.9585, -1.0651])\n",
      "mean: tensor(-1.0517)\n",
      "iter_dt 6448.85ms; iter 8: train loss 0.15971 temperature: 5.399999999999999\n",
      "mean_logits tensor([-0.8964, -0.8822, -0.7528, -0.9520, -0.9894], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0734, -0.8262, -1.1187, -1.1316, -1.0843])\n",
      "mean: tensor(-1.0468)\n",
      "iter_dt 6432.54ms; iter 9: train loss 0.29080 temperature: 5.449999999999998\n",
      "mean_logits tensor([-0.9239, -0.9155, -0.9552, -0.9734, -1.0525], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0499, -1.1271, -1.0687, -1.0787, -1.0781])\n",
      "mean: tensor(-1.0805)\n",
      "iter_dt 6464.58ms; iter 10: train loss 0.13013 temperature: 5.499999999999998\n",
      "mean_logits tensor([-0.8737, -1.0145, -0.9113, -0.8659, -0.9343], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1212, -1.0391, -0.9899, -1.0079, -1.1215])\n",
      "mean: tensor(-1.0559)\n",
      "iter_dt 6433.50ms; iter 11: train loss 0.18097 temperature: 5.549999999999998\n",
      "mean_logits tensor([-1.0142, -0.7979, -1.0673, -0.9286, -0.9277], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0952, -1.1286, -1.0982, -1.0498, -1.0075])\n",
      "mean: tensor(-1.0759)\n",
      "iter_dt 6454.51ms; iter 12: train loss 0.19416 temperature: 5.599999999999998\n",
      "mean_logits tensor([-0.9044, -0.8230, -1.0059, -0.9820, -0.9453], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0733, -1.0602, -1.1251, -1.0751, -1.1267])\n",
      "mean: tensor(-1.0921)\n",
      "iter_dt 6486.57ms; iter 13: train loss 0.20567 temperature: 5.649999999999998\n",
      "mean_logits tensor([-0.8838, -1.0593, -0.9544, -0.8539, -1.0281], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1246, -1.0627, -1.1277, -1.1222, -1.0955])\n",
      "mean: tensor(-1.1065)\n",
      "iter_dt 6430.11ms; iter 14: train loss 0.24725 temperature: 5.6999999999999975\n",
      "mean_logits tensor([-1.1365, -1.0890, -0.9504, -1.0626, -1.1192], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1309, -1.0934, -1.1134, -1.0929, -1.0933])\n",
      "mean: tensor(-1.1048)\n",
      "iter_dt 6566.63ms; iter 15: train loss 0.04488 temperature: 5.749999999999997\n",
      "mean_logits tensor([-1.1964, -1.0323, -1.1275, -1.0441, -0.9907], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0281, -1.1060, -1.0719, -1.1283, -1.1086])\n",
      "mean: tensor(-1.0886)\n",
      "iter_dt 6445.38ms; iter 16: train loss 0.10250 temperature: 5.799999999999997\n",
      "mean_logits tensor([-1.1064, -0.9816, -1.0192, -1.2565, -1.0382], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1082, -1.0859, -1.1330, -1.0510, -1.0922])\n",
      "mean: tensor(-1.0940)\n",
      "iter_dt 6515.11ms; iter 17: train loss 0.12966 temperature: 5.849999999999997\n",
      "mean_logits tensor([-1.1050, -1.1750, -1.1479, -1.1975, -1.2431], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0656, -0.9647, -1.1278, -1.0924, -0.9777])\n",
      "mean: tensor(-1.0456)\n",
      "iter_dt 6509.90ms; iter 18: train loss 0.23135 temperature: 5.899999999999997\n",
      "mean_logits tensor([-1.1899, -1.3019, -1.0580, -1.2798, -1.3445], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1036, -1.1065, -1.1079, -1.0493, -1.1316])\n",
      "mean: tensor(-1.0998)\n",
      "iter_dt 6459.24ms; iter 19: train loss 0.32221 temperature: 5.949999999999997\n",
      "mean_logits tensor([-1.2116, -1.2405, -1.2220, -1.1652, -1.1439], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1127, -1.0298, -1.1206, -1.0555, -1.0970])\n",
      "mean: tensor(-1.0831)\n",
      "iter_dt 6423.18ms; iter 20: train loss 0.15411 temperature: 5.9999999999999964\n",
      "mean_logits tensor([-1.1771, -1.1322, -1.3412, -1.1891, -1.2588], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8962, -0.9156, -1.0921, -1.0983, -1.1115])\n",
      "mean: tensor(-1.0227)\n",
      "iter_dt 6447.87ms; iter 21: train loss 0.40425 temperature: 6.049999999999996\n",
      "mean_logits tensor([-1.2575, -1.1807, -1.2605, -1.2874, -1.1620], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1092, -1.1245, -1.1321, -1.0953, -1.1147])\n",
      "mean: tensor(-1.1151)\n",
      "iter_dt 6468.41ms; iter 22: train loss 0.17400 temperature: 6.099999999999996\n",
      "mean_logits tensor([-1.3410, -1.2177, -1.2729, -1.2961, -1.0786], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1035, -1.1042, -1.1327, -1.1100, -0.9852])\n",
      "mean: tensor(-1.0871)\n",
      "iter_dt 6518.40ms; iter 23: train loss 0.29130 temperature: 6.149999999999996\n",
      "mean_logits tensor([-1.0419, -1.0296, -1.0837, -1.0109, -1.1550], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9637, -1.0516, -1.1168, -1.1146, -1.1197])\n",
      "mean: tensor(-1.0733)\n",
      "iter_dt 6468.48ms; iter 24: train loss 0.03231 temperature: 6.199999999999996\n",
      "mean_logits tensor([-1.1065, -1.1387, -1.0569, -1.2011, -1.1375], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1357, -1.0466, -1.1042, -1.1291, -1.0765])\n",
      "mean: tensor(-1.0984)\n",
      "iter_dt 6521.24ms; iter 25: train loss 0.03804 temperature: 6.249999999999996\n",
      "mean_logits tensor([-1.2992, -1.0516, -1.1076, -1.2337, -1.1950], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0631, -1.1120, -1.0766, -1.1175, -1.0837])\n",
      "mean: tensor(-1.0906)\n",
      "iter_dt 6495.02ms; iter 26: train loss 0.17957 temperature: 6.299999999999995\n",
      "mean_logits tensor([-1.2185, -1.2950, -1.0735, -1.0337, -1.1708], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1110, -1.0569, -1.0353, -1.0967, -1.0973])\n",
      "mean: tensor(-1.0794)\n",
      "iter_dt 6451.60ms; iter 27: train loss 0.16297 temperature: 6.349999999999995\n",
      "mean_logits tensor([-1.0257, -1.1674, -0.9968, -1.0665, -1.1897], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0994, -1.1005, -1.1006, -1.1241, -1.0385])\n",
      "mean: tensor(-1.0926)\n",
      "iter_dt 6513.67ms; iter 28: train loss 0.08376 temperature: 6.399999999999995\n",
      "mean_logits tensor([-0.9648, -1.1520, -0.9352, -1.1212, -0.9815], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1250, -1.1322, -1.0330, -1.1269, -1.0059])\n",
      "mean: tensor(-1.0846)\n",
      "iter_dt 6449.43ms; iter 29: train loss 0.05696 temperature: 6.449999999999995\n",
      "mean_logits tensor([-1.0298, -1.0918, -1.0061, -1.1596, -1.1418], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0975, -1.0914, -1.1271, -1.1239, -1.0834])\n",
      "mean: tensor(-1.1047)\n",
      "iter_dt 6510.18ms; iter 30: train loss 0.04130 temperature: 6.499999999999995\n",
      "mean_logits tensor([-1.1417, -1.1552, -1.1723, -1.1234, -1.0967], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1044, -0.9312, -1.0908, -1.1192, -1.0825])\n",
      "mean: tensor(-1.0656)\n",
      "iter_dt 6462.77ms; iter 31: train loss 0.09703 temperature: 6.5499999999999945\n",
      "mean_logits tensor([-1.0858, -1.0519, -1.1334, -1.0416, -1.0253], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0410, -1.0908, -1.1125, -1.0737, -0.8734])\n",
      "mean: tensor(-1.0383)\n",
      "iter_dt 6492.74ms; iter 32: train loss 0.03935 temperature: 6.599999999999994\n",
      "mean_logits tensor([-0.9235, -1.0736, -1.0129, -0.9263, -1.0787], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0553, -1.1298, -1.0837, -1.0903, -1.1302])\n",
      "mean: tensor(-1.0979)\n",
      "iter_dt 6435.76ms; iter 33: train loss 0.08444 temperature: 6.649999999999994\n",
      "mean_logits tensor([-1.1887, -1.0548, -1.1185, -1.0094, -1.1368], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1032, -1.0703, -0.9119, -1.1111, -0.9765])\n",
      "mean: tensor(-1.0346)\n",
      "iter_dt 6461.11ms; iter 34: train loss 0.14007 temperature: 6.699999999999994\n",
      "mean_logits tensor([-1.0560, -0.9274, -1.0996, -1.0463, -1.1852], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1058, -1.0574, -1.1146, -0.9706, -1.0757])\n",
      "mean: tensor(-1.0648)\n",
      "iter_dt 6439.23ms; iter 35: train loss 0.06098 temperature: 6.749999999999994\n",
      "mean_logits tensor([-0.9907, -1.0914, -0.9787, -1.0931, -1.1404], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9673, -1.0809, -1.1179, -1.0899, -1.0180])\n",
      "mean: tensor(-1.0548)\n",
      "iter_dt 6373.27ms; iter 36: train loss 0.05858 temperature: 6.799999999999994\n",
      "mean_logits tensor([-1.1141, -1.0572, -1.0136, -1.1651, -1.2714], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1171, -1.0551, -1.0883, -1.1176, -1.1344])\n",
      "mean: tensor(-1.1025)\n",
      "iter_dt 6441.93ms; iter 37: train loss 0.05531 temperature: 6.849999999999993\n",
      "mean_logits tensor([-1.1450, -1.0297, -1.0160, -1.0512, -1.0716], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0394, -1.0823, -0.8811, -1.0697, -1.0885])\n",
      "mean: tensor(-1.0322)\n",
      "iter_dt 6418.14ms; iter 38: train loss 0.04974 temperature: 6.899999999999993\n",
      "mean_logits tensor([-0.9435, -1.0207, -1.0633, -1.0086, -1.1070], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0472, -0.9945, -0.8455, -1.1202, -1.1037])\n",
      "mean: tensor(-1.0222)\n",
      "iter_dt 6415.81ms; iter 39: train loss 0.10201 temperature: 6.949999999999993\n",
      "mean_logits tensor([-1.0511, -1.0722, -1.1759, -0.9180, -1.0659], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0690, -1.0684, -1.1183, -1.0418, -1.1056])\n",
      "mean: tensor(-1.0806)\n",
      "iter_dt 6391.35ms; iter 40: train loss 0.03168 temperature: 6.999999999999993\n",
      "mean_logits tensor([-1.1307, -1.0924, -1.0679, -0.9980, -1.0850], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0804, -0.8953, -1.0738, -1.0969, -1.0935])\n",
      "mean: tensor(-1.0480)\n",
      "iter_dt 6394.91ms; iter 41: train loss 0.07765 temperature: 7.049999999999993\n",
      "mean_logits tensor([-0.9299, -1.0604, -1.0796, -1.0369, -1.0369], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0944, -1.0813, -1.0944, -1.0842, -1.1153])\n",
      "mean: tensor(-1.0939)\n",
      "iter_dt 6429.97ms; iter 42: train loss 0.05653 temperature: 7.0999999999999925\n",
      "mean_logits tensor([-1.0564, -1.0021, -1.1233, -1.0446, -0.8558], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9670, -0.9578, -1.1006, -1.0552, -1.0712])\n",
      "mean: tensor(-1.0303)\n",
      "iter_dt 6432.54ms; iter 43: train loss 0.07997 temperature: 7.149999999999992\n",
      "mean_logits tensor([-1.0509, -1.0663, -1.1287, -1.1247, -1.1138], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1021, -1.1006, -1.1050, -1.1011, -1.1201])\n",
      "mean: tensor(-1.1058)\n",
      "iter_dt 6407.05ms; iter 44: train loss 0.00874 temperature: 7.199999999999992\n",
      "mean_logits tensor([-1.0990, -1.0756, -0.9768, -1.0086, -1.2032], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1343, -1.1242, -1.1119, -1.0926, -1.1160])\n",
      "mean: tensor(-1.1158)\n",
      "iter_dt 6407.29ms; iter 45: train loss 0.06311 temperature: 7.249999999999992\n",
      "mean_logits tensor([-1.1098, -1.0766, -1.0149, -0.9144, -0.9835], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1083, -1.1025, -1.0854, -1.0972, -1.0848])\n",
      "mean: tensor(-1.0956)\n",
      "iter_dt 6448.89ms; iter 46: train loss 0.07567 temperature: 7.299999999999992\n",
      "mean_logits tensor([-1.0840, -1.1240, -1.0118, -1.0786, -1.0823], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1034, -1.1051, -1.1099, -0.9282, -1.1289])\n",
      "mean: tensor(-1.0751)\n",
      "iter_dt 6435.76ms; iter 47: train loss 0.05507 temperature: 7.349999999999992\n",
      "mean_logits tensor([-1.1335, -1.1472, -1.1272, -1.0410, -1.0614], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0985, -1.0047, -1.0611, -1.0206, -1.1058])\n",
      "mean: tensor(-1.0581)\n",
      "iter_dt 6587.07ms; iter 48: train loss 0.04915 temperature: 7.3999999999999915\n",
      "mean_logits tensor([-1.0046, -1.1112, -1.1304, -0.9557, -1.0944], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0576, -0.9545, -1.0654, -1.1053, -1.1146])\n",
      "mean: tensor(-1.0595)\n",
      "iter_dt 6525.65ms; iter 49: train loss 0.08682 temperature: 7.449999999999991\n",
      "mean_logits tensor([-0.8770, -1.1137, -1.1297, -1.0913, -1.0758], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1210, -1.1122, -1.1000, -1.0647, -1.1062])\n",
      "mean: tensor(-1.1008)\n",
      "iter_dt 6519.67ms; iter 50: train loss 0.09270 temperature: 7.499999999999991\n",
      "mean_logits tensor([-1.1501, -1.2128, -1.2281, -1.1551, -0.9909], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1134, -1.0611, -1.1204, -1.1124, -0.9595])\n",
      "mean: tensor(-1.0734)\n",
      "iter_dt 6542.77ms; iter 51: train loss 0.07665 temperature: 7.549999999999991\n",
      "mean_logits tensor([-1.1045, -1.0829, -1.0341, -1.0391, -1.1268], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9810, -1.1112, -1.0681, -1.1190, -1.1238])\n",
      "mean: tensor(-1.0806)\n",
      "iter_dt 6517.33ms; iter 52: train loss 0.03900 temperature: 7.599999999999991\n",
      "mean_logits tensor([-1.1365, -1.1116, -1.1989, -1.1561, -1.1223], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0369, -1.0774, -1.0361, -1.0791, -1.1302])\n",
      "mean: tensor(-1.0719)\n",
      "iter_dt 6513.42ms; iter 53: train loss 0.08042 temperature: 7.649999999999991\n",
      "mean_logits tensor([-1.0799, -1.0503, -1.1623, -1.0911, -1.1322], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0375, -1.1260, -1.1273, -0.9390, -1.0982])\n",
      "mean: tensor(-1.0656)\n",
      "iter_dt 6496.30ms; iter 54: train loss 0.05298 temperature: 7.69999999999999\n",
      "mean_logits tensor([-1.0446, -1.0528, -1.0505, -1.2183, -0.9490], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0818, -1.0436, -1.1247, -1.1137, -1.1176])\n",
      "mean: tensor(-1.0963)\n",
      "iter_dt 6643.07ms; iter 55: train loss 0.07969 temperature: 7.74999999999999\n",
      "mean_logits tensor([-1.0783, -1.0776, -1.1202, -1.1237, -1.0780], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0928, -1.1094, -1.1185, -1.0334, -1.0612])\n",
      "mean: tensor(-1.0830)\n",
      "iter_dt 6440.51ms; iter 56: train loss 0.01677 temperature: 7.79999999999999\n",
      "mean_logits tensor([-1.1379, -1.1489, -1.1111, -1.0501, -0.9873], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0996, -1.0622, -1.0659, -1.1157, -1.1079])\n",
      "mean: tensor(-1.0903)\n",
      "iter_dt 6471.15ms; iter 57: train loss 0.05125 temperature: 7.84999999999999\n",
      "mean_logits tensor([-1.1067, -1.0883, -1.0788, -1.0950, -0.9398], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1068, -1.1138, -1.0989, -1.0664, -1.1224])\n",
      "mean: tensor(-1.1017)\n",
      "iter_dt 6482.14ms; iter 58: train loss 0.05590 temperature: 7.89999999999999\n",
      "mean_logits tensor([-1.1205, -1.0410, -1.1254, -0.9715, -1.1423], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0710, -1.1170, -1.0879, -1.0146, -1.0940])\n",
      "mean: tensor(-1.0769)\n",
      "iter_dt 6467.91ms; iter 59: train loss 0.02405 temperature: 7.9499999999999895\n",
      "mean_logits tensor([-1.0522, -1.1344, -1.1408, -0.9852, -1.1257], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0547, -1.0667, -1.1051, -0.9952, -1.1245])\n",
      "mean: tensor(-1.0692)\n",
      "iter_dt 6528.55ms; iter 60: train loss 0.01084 temperature: 7.999999999999989\n",
      "mean_logits tensor([-1.0489, -1.1474, -1.0635, -1.1062, -1.1514], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1250, -1.1272, -1.1154, -1.0895, -1.0376])\n",
      "mean: tensor(-1.0989)\n",
      "iter_dt 6461.11ms; iter 61: train loss 0.03938 temperature: 8.04999999999999\n",
      "mean_logits tensor([-1.1443, -0.9743, -0.9471, -1.0790, -1.1144], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0725, -1.1122, -1.0830, -1.0898, -1.1097])\n",
      "mean: tensor(-1.0934)\n",
      "iter_dt 6456.94ms; iter 62: train loss 0.06858 temperature: 8.09999999999999\n",
      "mean_logits tensor([-1.2037, -1.1066, -1.0702, -1.1334, -1.0080], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1313, -1.0396, -1.1034, -1.0275, -1.0925])\n",
      "mean: tensor(-1.0789)\n",
      "iter_dt 6444.95ms; iter 63: train loss 0.05160 temperature: 8.149999999999991\n",
      "mean_logits tensor([-1.0066, -1.0880, -1.1536, -1.0136, -1.2254], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0999, -1.1016, -1.1189, -1.0879, -1.0841])\n",
      "mean: tensor(-1.0985)\n",
      "iter_dt 6460.22ms; iter 64: train loss 0.06626 temperature: 8.199999999999992\n",
      "mean_logits tensor([-1.0678, -1.1213, -0.9770, -1.0331, -0.9708], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0937, -1.1330, -1.1292, -1.0998, -1.1094])\n",
      "mean: tensor(-1.1130)\n",
      "iter_dt 6469.53ms; iter 65: train loss 0.07788 temperature: 8.249999999999993\n",
      "mean_logits tensor([-1.0104, -1.1273, -1.0624, -1.0966, -1.0298], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1181, -1.1172, -1.1059, -1.0862, -0.9938])\n",
      "mean: tensor(-1.0842)\n",
      "iter_dt 6517.71ms; iter 66: train loss 0.02515 temperature: 8.299999999999994\n",
      "mean_logits tensor([-1.1186, -1.0489, -1.0106, -1.0914, -1.0899], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1360, -1.1082, -1.1049, -1.1225, -1.1269])\n",
      "mean: tensor(-1.1197)\n",
      "iter_dt 6480.63ms; iter 67: train loss 0.02570 temperature: 8.349999999999994\n",
      "mean_logits tensor([-1.0453, -1.0415, -1.0077, -0.9954, -1.0611], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1092, -1.1082, -1.0985, -0.9810, -1.0097])\n",
      "mean: tensor(-1.0613)\n",
      "iter_dt 6484.06ms; iter 68: train loss 0.03274 temperature: 8.399999999999995\n",
      "mean_logits tensor([-1.0894, -1.0309, -1.1322, -1.2536, -1.1470], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1253, -1.0986, -1.1214, -1.1261, -1.0981])\n",
      "mean: tensor(-1.1139)\n",
      "iter_dt 6464.45ms; iter 69: train loss 0.05003 temperature: 8.449999999999996\n",
      "mean_logits tensor([-1.1712, -1.1706, -1.1564, -1.1242, -1.1124], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0653, -1.1110, -1.0588, -1.1242, -1.1341])\n",
      "mean: tensor(-1.0987)\n",
      "iter_dt 6450.55ms; iter 70: train loss 0.04629 temperature: 8.499999999999996\n",
      "mean_logits tensor([-1.1673, -1.0568, -1.1146, -1.1343, -1.1156], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0649, -1.1311, -1.0764, -1.0703, -1.1294])\n",
      "mean: tensor(-1.0944)\n",
      "iter_dt 6450.18ms; iter 71: train loss 0.03980 temperature: 8.549999999999997\n",
      "mean_logits tensor([-1.1009, -1.1506, -1.0624, -1.1134, -1.1507], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1019, -1.0607, -0.9630, -1.1146, -1.0862])\n",
      "mean: tensor(-1.0653)\n",
      "iter_dt 6447.33ms; iter 72: train loss 0.03752 temperature: 8.599999999999998\n",
      "mean_logits tensor([-1.0189, -1.1572, -1.0489, -1.1710, -1.1593], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0934, -1.0712, -1.1179, -1.1289, -1.1186])\n",
      "mean: tensor(-1.1060)\n",
      "iter_dt 6497.12ms; iter 73: train loss 0.03801 temperature: 8.649999999999999\n",
      "mean_logits tensor([-1.0891, -1.0853, -1.1712, -1.1055, -1.1145], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1253, -1.0840, -1.1153, -1.1155, -1.1092])\n",
      "mean: tensor(-1.1099)\n",
      "iter_dt 6492.02ms; iter 74: train loss 0.00879 temperature: 8.7\n",
      "mean_logits tensor([-1.0428, -1.1304, -1.0348, -1.0555, -1.1324], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0976, -1.1256, -1.1038, -1.0959, -1.1205])\n",
      "mean: tensor(-1.1087)\n",
      "iter_dt 6468.23ms; iter 75: train loss 0.01633 temperature: 8.75\n",
      "mean_logits tensor([-1.1602, -1.1367, -1.1175, -1.0408, -1.0416], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0815, -1.1284, -1.1290, -1.0462, -1.0959])\n",
      "mean: tensor(-1.0962)\n",
      "iter_dt 6458.11ms; iter 76: train loss 0.01711 temperature: 8.8\n",
      "mean_logits tensor([-1.0978, -1.0922, -1.0575, -1.0741, -1.1124], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0562, -1.1164, -1.0734, -1.0922, -1.1032])\n",
      "mean: tensor(-1.0883)\n",
      "iter_dt 6420.86ms; iter 77: train loss 0.00521 temperature: 8.850000000000001\n",
      "mean_logits tensor([-1.0939, -1.0260, -1.1176, -0.9901, -1.1457], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1215, -1.1152, -1.1230, -1.0066, -1.0773])\n",
      "mean: tensor(-1.0887)\n",
      "iter_dt 6426.05ms; iter 78: train loss 0.02406 temperature: 8.900000000000002\n",
      "mean_logits tensor([-1.0754, -1.1525, -1.0988, -0.9767, -1.1757], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0295, -1.0615, -1.1243, -1.1328, -1.1299])\n",
      "mean: tensor(-1.0956)\n",
      "iter_dt 6420.53ms; iter 79: train loss 0.06433 temperature: 8.950000000000003\n",
      "mean_logits tensor([-1.1557, -1.0673, -1.1256, -1.1108, -1.0490], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0733, -1.0507, -1.1122, -1.1214, -1.0457])\n",
      "mean: tensor(-1.0807)\n",
      "iter_dt 6423.58ms; iter 80: train loss 0.01367 temperature: 9.000000000000004\n",
      "mean_logits tensor([-1.1409, -1.1376, -1.2033, -1.1238, -1.1065], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1015, -1.1310, -1.1234, -1.0922, -1.1025])\n",
      "mean: tensor(-1.1101)\n",
      "iter_dt 6410.55ms; iter 81: train loss 0.01795 temperature: 9.050000000000004\n",
      "mean_logits tensor([-1.0702, -1.0791, -1.1589, -1.0602, -1.1269], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0385, -1.0830, -1.1015, -1.1209, -1.1204])\n",
      "mean: tensor(-1.0929)\n",
      "iter_dt 6339.35ms; iter 82: train loss 0.01461 temperature: 9.100000000000005\n",
      "mean_logits tensor([-1.1215, -1.1761, -1.1088, -1.1467, -1.0341], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0474, -1.1226, -1.0551, -1.0371, -1.1276])\n",
      "mean: tensor(-1.0780)\n",
      "iter_dt 6409.73ms; iter 83: train loss 0.05691 temperature: 9.150000000000006\n",
      "mean_logits tensor([-1.1948, -1.1033, -1.1417, -1.1381, -1.1410], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0986, -1.0913, -1.1280, -1.0811, -1.0552])\n",
      "mean: tensor(-1.0908)\n",
      "iter_dt 6442.48ms; iter 84: train loss 0.03818 temperature: 9.200000000000006\n",
      "mean_logits tensor([-0.9422, -1.1099, -1.0724, -1.1421, -1.1511], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1019, -1.1277, -1.1103, -1.1250, -1.0793])\n",
      "mean: tensor(-1.1088)\n",
      "iter_dt 6417.82ms; iter 85: train loss 0.05276 temperature: 9.250000000000007\n",
      "mean_logits tensor([-1.1664, -0.9935, -1.0936, -1.0488, -1.0775], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1005, -1.1219, -1.1214, -1.0771, -1.1313])\n",
      "mean: tensor(-1.1104)\n",
      "iter_dt 6393.21ms; iter 86: train loss 0.04382 temperature: 9.300000000000008\n",
      "mean_logits tensor([-1.0906, -1.1176, -1.0043, -1.0831, -1.0608], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1299, -1.1265, -1.1145, -1.1264, -1.1131])\n",
      "mean: tensor(-1.1221)\n",
      "iter_dt 6430.45ms; iter 87: train loss 0.03144 temperature: 9.350000000000009\n",
      "mean_logits tensor([-1.0683, -1.1414, -1.1709, -1.0947, -0.9897], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0672, -1.1057, -1.1175, -1.1344, -1.1131])\n",
      "mean: tensor(-1.1076)\n",
      "iter_dt 6627.83ms; iter 88: train loss 0.03591 temperature: 9.40000000000001\n",
      "mean_logits tensor([-1.1523, -1.0884, -1.1664, -1.1626, -1.0707], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1018, -1.1267, -1.1213, -1.1195, -1.1290])\n",
      "mean: tensor(-1.1197)\n",
      "iter_dt 6535.20ms; iter 89: train loss 0.02131 temperature: 9.45000000000001\n",
      "mean_logits tensor([-1.1561, -1.0441, -1.0739, -1.0366, -1.0399], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1219, -1.1327, -1.0700, -1.1174, -1.1258])\n",
      "mean: tensor(-1.1136)\n",
      "iter_dt 6468.14ms; iter 90: train loss 0.04032 temperature: 9.50000000000001\n",
      "mean_logits tensor([-1.0498, -1.0376, -1.1347, -1.1528, -1.0907], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1330, -1.1237, -1.1339, -1.1264, -1.1304])\n",
      "mean: tensor(-1.1295)\n",
      "iter_dt 6437.56ms; iter 91: train loss 0.02942 temperature: 9.550000000000011\n",
      "mean_logits tensor([-1.1151, -1.0817, -1.0769, -1.1079, -1.1743], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1104, -1.1318, -1.1127, -1.0932, -1.1224])\n",
      "mean: tensor(-1.1141)\n",
      "iter_dt 6550.82ms; iter 92: train loss 0.01269 temperature: 9.600000000000012\n",
      "mean_logits tensor([-1.2093, -0.9908, -1.1940, -1.1033, -1.1388], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1023, -1.0241, -1.0827, -1.1343, -1.0043])\n",
      "mean: tensor(-1.0696)\n",
      "iter_dt 6487.12ms; iter 93: train loss 0.08157 temperature: 9.650000000000013\n",
      "mean_logits tensor([-1.1717, -1.0663, -1.0624, -1.1034, -1.1557], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1354, -1.1338, -1.1154, -1.1319, -1.0288])\n",
      "mean: tensor(-1.1091)\n",
      "iter_dt 6468.51ms; iter 94: train loss 0.04602 temperature: 9.700000000000014\n",
      "mean_logits tensor([-1.1034, -1.1393, -1.2188, -1.1743, -1.1793], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0962, -1.0986, -1.1033, -1.1257, -1.1239])\n",
      "mean: tensor(-1.1095)\n",
      "iter_dt 6494.14ms; iter 95: train loss 0.04128 temperature: 9.750000000000014\n",
      "mean_logits tensor([-1.1297, -1.1529, -1.1451, -1.1039, -1.2135], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1277, -1.0744, -0.8677, -1.1233, -1.1302])\n",
      "mean: tensor(-1.0646)\n",
      "iter_dt 6451.47ms; iter 96: train loss 0.14254 temperature: 9.800000000000015\n",
      "mean_logits tensor([-1.1524, -1.2615, -1.1476, -1.1096, -1.0643], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0771, -1.1111, -1.1264, -1.1350, -1.1313])\n",
      "mean: tensor(-1.1162)\n",
      "iter_dt 6429.05ms; iter 97: train loss 0.06932 temperature: 9.850000000000016\n",
      "mean_logits tensor([-1.1086, -1.1213, -1.0808, -1.0486, -1.1298], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1098, -1.0779, -1.0880, -1.1205, -1.1189])\n",
      "mean: tensor(-1.1030)\n",
      "iter_dt 6531.85ms; iter 98: train loss 0.01275 temperature: 9.900000000000016\n",
      "mean_logits tensor([-1.1150, -1.1228, -1.1378, -1.2178, -1.1005], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1288, -1.1336, -1.0861, -1.1188, -1.1177])\n",
      "mean: tensor(-1.1170)\n",
      "iter_dt 6505.30ms; iter 99: train loss 0.02636 temperature: 9.950000000000017\n",
      "converged SCF energy = -1.11668438708534\n",
      "Starting to parse FermionOperator using 4 qubits...\n",
      "\n",
      "Operator t:  -0.09886396933545816 [] +\n",
      "-0.04532220205287399 [X0 X1 Y2 Y3] +\n",
      "0.04532220205287399 [X0 Y1 Y2 X3] +\n",
      "0.04532220205287399 [Y0 X1 X2 Y3] +\n",
      "-0.04532220205287399 [Y0 Y1 X2 X3] +\n",
      "0.17119774903432977 [Z0] +\n",
      "0.1686221915892094 [Z0 Z1] +\n",
      "0.12054482205301804 [Z0 Z2] +\n",
      "0.165867024105892 [Z0 Z3] +\n",
      "0.17119774903432972 [Z1] +\n",
      "0.165867024105892 [Z1 Z2] +\n",
      "0.12054482205301804 [Z1 Z3] +\n",
      "-0.22278593040418473 [Z2] +\n",
      "0.17434844185575682 [Z2 Z3] +\n",
      "-0.2227859304041847 [Z3]\n",
      "Term, coeff:  () -0.09886396933545816\n",
      "Term, coeff:  ((0, 'Z'),) 0.17119774903432977\n",
      "Index, p_char:  0 Z\n",
      "Term, coeff:  ((1, 'Z'),) 0.17119774903432972\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((2, 'Z'),) -0.22278593040418473\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((3, 'Z'),) -0.2227859304041847\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((0, 'Z'), (1, 'Z')) 0.1686221915892094\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((0, 'Y'), (1, 'X'), (2, 'X'), (3, 'Y')) 0.04532220205287399\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'Y'), (1, 'Y'), (2, 'X'), (3, 'X')) -0.04532220205287399\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'X'), (1, 'X'), (2, 'Y'), (3, 'Y')) -0.04532220205287399\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'X'), (1, 'Y'), (2, 'Y'), (3, 'X')) 0.04532220205287399\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'Z'), (2, 'Z')) 0.12054482205301804\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((0, 'Z'), (3, 'Z')) 0.165867024105892\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((1, 'Z'), (2, 'Z')) 0.165867024105892\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((1, 'Z'), (3, 'Z')) 0.12054482205301804\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((2, 'Z'), (3, 'Z')) 0.17434844185575682\n",
      "Index, p_char:  2 Z\n",
      "Index, p_char:  3 Z\n",
      "ground state: -1.1372701746609029\n",
      "hf state: -1.116684387085341\n",
      "number of parameters: 85.29M\n",
      "running on device cpu\n",
      "mean_logits tensor([-1.1676, -0.9894, -1.0338, -1.0321, -1.2728], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1313, -1.1114, -1.0908, -1.1059, -0.9558])\n",
      "mean: tensor(-1.0790)\n",
      "iter_dt 0.00ms; iter 0: train loss 0.22984 temperature: 5\n",
      "mean_logits tensor([-0.9155, -0.9513, -0.9750, -0.9755, -0.8960], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9452, -1.1024, -1.1203, -1.0381, -1.1117])\n",
      "mean: tensor(-1.0635)\n",
      "iter_dt 1694970082239.66ms; iter 1: train loss 0.14659 temperature: 5.05\n",
      "mean_logits tensor([-1.1299, -1.3051, -1.3636, -1.3027, -1.0755], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1067, -1.1133, -1.0616, -0.5278, -1.1197])\n",
      "mean: tensor(-0.9858)\n",
      "iter_dt 6532.57ms; iter 2: train loss 1.08232 temperature: 5.1\n",
      "mean_logits tensor([-1.1659, -1.2483, -1.1785, -1.1580, -1.1481], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0164, -1.0884, -1.0868, -1.1301, -1.0087])\n",
      "mean: tensor(-1.0661)\n",
      "iter_dt 6567.65ms; iter 3: train loss 0.14406 temperature: 5.1499999999999995\n",
      "mean_logits tensor([-1.1877, -1.3006, -1.2308, -1.2289, -1.2000], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0908, -1.0945, -1.1001, -0.8555, -0.9184])\n",
      "mean: tensor(-1.0119)\n",
      "iter_dt 6538.10ms; iter 4: train loss 0.50663 temperature: 5.199999999999999\n",
      "mean_logits tensor([-1.2651, -1.0400, -1.2533, -0.8456, -1.0045], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1246, -1.0516, -1.1282, -1.1213, -1.0864])\n",
      "mean: tensor(-1.1024)\n",
      "iter_dt 6582.72ms; iter 5: train loss 0.19752 temperature: 5.249999999999999\n",
      "mean_logits tensor([-1.1153, -0.9987, -1.1589, -1.0629, -1.0867], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1160, -1.1185, -1.1035, -1.0946, -1.1097])\n",
      "mean: tensor(-1.1084)\n",
      "iter_dt 6508.32ms; iter 6: train loss 0.03246 temperature: 5.299999999999999\n",
      "mean_logits tensor([-1.1173, -1.0307, -1.0326, -1.1066, -0.9477], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0728, -0.9724, -0.9425, -1.0142, -1.0253])\n",
      "mean: tensor(-1.0054)\n",
      "iter_dt 6426.83ms; iter 7: train loss 0.04320 temperature: 5.349999999999999\n",
      "mean_logits tensor([-0.9689, -0.8843, -0.8970, -0.9902, -1.0750], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0939, -1.0975, -1.1099, -1.0397, -1.0859])\n",
      "mean: tensor(-1.0854)\n",
      "iter_dt 6455.70ms; iter 8: train loss 0.16255 temperature: 5.399999999999999\n",
      "mean_logits tensor([-1.0023, -0.9559, -0.8828, -1.0117, -0.9759], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0922, -0.9919, -0.8836, -1.1037, -1.0890])\n",
      "mean: tensor(-1.0321)\n",
      "iter_dt 6544.65ms; iter 9: train loss 0.04919 temperature: 5.449999999999998\n",
      "mean_logits tensor([-0.8777, -1.0498, -0.9944, -0.9778, -1.0506], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0200, -1.1270, -1.1158, -0.9998, -1.1014])\n",
      "mean: tensor(-1.0728)\n",
      "iter_dt 6447.26ms; iter 10: train loss 0.06707 temperature: 5.499999999999998\n",
      "mean_logits tensor([-0.9260, -1.0759, -1.1858, -0.9644, -1.0165], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1222, -1.0943, -1.0785, -1.0623, -1.1346])\n",
      "mean: tensor(-1.0984)\n",
      "iter_dt 6414.87ms; iter 11: train loss 0.12125 temperature: 5.549999999999998\n",
      "mean_logits tensor([-1.0289, -0.9382, -1.0936, -1.0560, -0.9310], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9155, -1.0845, -1.1105, -1.1034, -1.1094])\n",
      "mean: tensor(-1.0647)\n",
      "iter_dt 6540.18ms; iter 12: train loss 0.10391 temperature: 5.599999999999998\n",
      "mean_logits tensor([-1.0871, -0.8751, -0.9707, -0.9257, -0.9727], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1041, -1.1099, -1.1277, -1.1254, -1.1268])\n",
      "mean: tensor(-1.1188)\n",
      "iter_dt 6535.68ms; iter 13: train loss 0.22248 temperature: 5.649999999999998\n",
      "mean_logits tensor([-1.1022, -1.1388, -1.0916, -0.9370, -1.0163], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0972, -1.0085, -1.1106, -1.1029, -1.0997])\n",
      "mean: tensor(-1.0838)\n",
      "iter_dt 6391.62ms; iter 14: train loss 0.08371 temperature: 5.6999999999999975\n",
      "mean_logits tensor([-1.1809, -1.1377, -1.2138, -1.2856, -1.2821], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0761, -1.0624, -1.1205, -1.1250, -1.1296])\n",
      "mean: tensor(-1.1027)\n",
      "iter_dt 6393.01ms; iter 15: train loss 0.15887 temperature: 5.749999999999997\n",
      "mean_logits tensor([-1.0972, -1.0564, -1.2057, -1.0569, -1.1344], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0164, -1.0983, -1.0621, -1.0153, -1.0437])\n",
      "mean: tensor(-1.0472)\n",
      "iter_dt 6442.50ms; iter 16: train loss 0.07101 temperature: 5.799999999999997\n",
      "mean_logits tensor([-1.2219, -1.1015, -1.0666, -1.2428, -1.1179], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1122, -1.1120, -1.0821, -1.0740, -1.1211])\n",
      "mean: tensor(-1.1003)\n",
      "iter_dt 6494.47ms; iter 17: train loss 0.08349 temperature: 5.849999999999997\n",
      "mean_logits tensor([-1.2236, -1.1702, -1.0770, -1.1662, -1.4555], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0661, -1.1071, -0.9965, -1.0606, -1.1046])\n",
      "mean: tensor(-1.0670)\n",
      "iter_dt 6396.78ms; iter 18: train loss 0.40970 temperature: 5.899999999999997\n",
      "mean_logits tensor([-1.0738, -1.2153, -1.1306, -1.2313, -1.1961], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1335, -1.0572, -1.1069, -1.1137, -1.1046])\n",
      "mean: tensor(-1.1032)\n",
      "iter_dt 6401.96ms; iter 19: train loss 0.10179 temperature: 5.949999999999997\n",
      "mean_logits tensor([-1.3130, -1.1710, -1.1262, -1.2061, -1.1351], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0874, -1.0569, -1.1274, -1.1097, -1.0497])\n",
      "mean: tensor(-1.0862)\n",
      "iter_dt 6383.89ms; iter 20: train loss 0.16875 temperature: 5.9999999999999964\n",
      "mean_logits tensor([-1.1831, -1.2118, -1.0808, -1.0248, -1.1429], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9027, -1.0750, -1.1254, -1.1223, -1.1130])\n",
      "mean: tensor(-1.0677)\n",
      "iter_dt 6535.55ms; iter 21: train loss 0.18592 temperature: 6.049999999999996\n",
      "mean_logits tensor([-1.0532, -1.2207, -1.1652, -1.2423, -1.1740], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0889, -1.0272, -1.1340, -1.1228, -1.0971])\n",
      "mean: tensor(-1.0940)\n",
      "iter_dt 6500.20ms; iter 22: train loss 0.11717 temperature: 6.099999999999996\n",
      "mean_logits tensor([-1.1639, -1.0665, -1.1264, -1.0623, -1.0696], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1254, -1.0947, -1.0558, -1.0594, -0.9808])\n",
      "mean: tensor(-1.0632)\n",
      "iter_dt 6575.09ms; iter 23: train loss 0.02542 temperature: 6.149999999999996\n",
      "mean_logits tensor([-1.0728, -0.8817, -1.2526, -1.0393, -1.1264], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1091, -0.9585, -1.0366, -1.0995, -1.0716])\n",
      "mean: tensor(-1.0550)\n",
      "iter_dt 6514.47ms; iter 24: train loss 0.11375 temperature: 6.199999999999996\n",
      "mean_logits tensor([-1.0378, -0.9670, -1.0217, -0.9938, -1.0694], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1264, -1.1351, -1.1257, -1.1129, -1.0397])\n",
      "mean: tensor(-1.1080)\n",
      "iter_dt 6584.60ms; iter 25: train loss 0.10340 temperature: 6.249999999999996\n",
      "mean_logits tensor([-1.1151, -1.0179, -1.0889, -1.1180, -0.9733], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0391, -1.1184, -1.1131, -0.8810, -1.1345])\n",
      "mean: tensor(-1.0572)\n",
      "iter_dt 6437.52ms; iter 26: train loss 0.15429 temperature: 6.299999999999995\n",
      "mean_logits tensor([-1.2197, -1.0229, -1.2387, -1.0063, -1.1021], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1195, -1.0809, -1.1343, -1.1319, -1.1311])\n",
      "mean: tensor(-1.1195)\n",
      "iter_dt 6372.42ms; iter 27: train loss 0.07814 temperature: 6.349999999999995\n",
      "mean_logits tensor([-1.1120, -0.9204, -0.9948, -1.0446, -1.0526], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1286, -1.0341, -1.1261, -1.1331, -1.1232])\n",
      "mean: tensor(-1.1090)\n",
      "iter_dt 6364.11ms; iter 28: train loss 0.07017 temperature: 6.399999999999995\n",
      "mean_logits tensor([-1.1093, -1.0098, -0.9819, -1.1235, -0.9292], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0507, -1.1366, -1.1267, -1.1083, -1.1099])\n",
      "mean: tensor(-1.1064)\n",
      "iter_dt 6499.95ms; iter 29: train loss 0.11883 temperature: 6.449999999999995\n",
      "mean_logits tensor([-1.1237, -1.0538, -0.9827, -1.1069, -1.1112], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1157, -1.0663, -1.0938, -1.1279, -1.1232])\n",
      "mean: tensor(-1.1054)\n",
      "iter_dt 6460.70ms; iter 30: train loss 0.02119 temperature: 6.499999999999995\n",
      "mean_logits tensor([-1.1637, -1.1369, -1.0724, -1.0693, -1.0095], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1190, -1.0648, -1.1240, -1.0766, -1.1295])\n",
      "mean: tensor(-1.1028)\n",
      "iter_dt 6422.30ms; iter 31: train loss 0.04269 temperature: 6.5499999999999945\n",
      "mean_logits tensor([-1.1563, -1.1064, -1.2053, -1.0876, -0.9859], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0646, -1.0808, -1.1123, -1.0559, -1.0202])\n",
      "mean: tensor(-1.0668)\n",
      "iter_dt 6531.14ms; iter 32: train loss 0.03772 temperature: 6.599999999999994\n",
      "mean_logits tensor([-1.0565, -1.1351, -1.1106, -1.1199, -1.0639], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0848, -1.1173, -1.1070, -1.0185, -1.0244])\n",
      "mean: tensor(-1.0704)\n",
      "iter_dt 6547.67ms; iter 33: train loss 0.02198 temperature: 6.649999999999994\n",
      "mean_logits tensor([-1.2014, -1.1422, -1.0318, -1.2446, -1.0918], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0758, -1.1029, -1.1004, -1.0306, -1.0367])\n",
      "mean: tensor(-1.0693)\n",
      "iter_dt 6594.61ms; iter 34: train loss 0.13616 temperature: 6.699999999999994\n",
      "mean_logits tensor([-1.1602, -1.1511, -1.0544, -1.1935, -1.1531], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1341, -1.0649, -0.8858, -1.1131, -1.0660])\n",
      "mean: tensor(-1.0528)\n",
      "iter_dt 6582.50ms; iter 35: train loss 0.08159 temperature: 6.749999999999994\n",
      "mean_logits tensor([-0.9769, -1.0919, -1.2334, -1.1826, -1.0979], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0813, -1.1052, -1.0795, -1.0264, -1.1113])\n",
      "mean: tensor(-1.0807)\n",
      "iter_dt 6608.32ms; iter 36: train loss 0.11020 temperature: 6.799999999999994\n",
      "mean_logits tensor([-1.0511, -1.0801, -1.0274, -1.1175, -1.0688], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1019, -1.0768, -1.1080, -1.0784, -0.9156])\n",
      "mean: tensor(-1.0561)\n",
      "iter_dt 6461.00ms; iter 37: train loss 0.05244 temperature: 6.849999999999993\n",
      "mean_logits tensor([-0.9483, -1.1860, -0.9960, -1.0571, -1.1469], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1268, -1.1216, -1.0887, -1.0781, -0.9603])\n",
      "mean: tensor(-1.0751)\n",
      "iter_dt 6477.19ms; iter 38: train loss 0.13130 temperature: 6.899999999999993\n",
      "mean_logits tensor([-1.0547, -1.0599, -1.0235, -1.2298, -1.0848], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0302, -1.0755, -1.1159, -1.1192, -1.0930])\n",
      "mean: tensor(-1.0868)\n",
      "iter_dt 6519.47ms; iter 39: train loss 0.04164 temperature: 6.949999999999993\n",
      "mean_logits tensor([-1.1286, -0.9638, -1.1606, -0.9116, -1.0502], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0919, -1.0440, -1.1204, -0.9804, -1.1320])\n",
      "mean: tensor(-1.0738)\n",
      "iter_dt 6427.11ms; iter 40: train loss 0.03338 temperature: 6.999999999999993\n",
      "mean_logits tensor([-1.0009, -1.1191, -1.0541, -1.0357, -1.0621], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0941, -1.0998, -1.0414, -1.1301, -1.1336])\n",
      "mean: tensor(-1.0998)\n",
      "iter_dt 6344.21ms; iter 41: train loss 0.03984 temperature: 7.049999999999993\n",
      "mean_logits tensor([-1.0711, -1.0935, -0.9991, -1.0454, -1.0878], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0935, -1.0485, -1.0143, -0.9894, -0.9588])\n",
      "mean: tensor(-1.0209)\n",
      "iter_dt 6314.26ms; iter 42: train loss 0.03528 temperature: 7.0999999999999925\n",
      "mean_logits tensor([-1.0866, -1.0855, -1.1795, -1.0629, -1.0593], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1124, -1.0942, -1.1140, -1.0216, -1.1008])\n",
      "mean: tensor(-1.0886)\n",
      "iter_dt 6341.81ms; iter 43: train loss 0.01554 temperature: 7.149999999999992\n",
      "mean_logits tensor([-1.1261, -1.0693, -1.1264, -1.1292, -1.0894], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0907, -1.0418, -1.1001, -1.1179, -1.0710])\n",
      "mean: tensor(-1.0843)\n",
      "iter_dt 6374.32ms; iter 44: train loss 0.00566 temperature: 7.199999999999992\n",
      "mean_logits tensor([-1.2114, -0.9091, -1.0473, -1.1049, -1.0884], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0230, -1.0703, -1.0733, -1.0782, -1.0604])\n",
      "mean: tensor(-1.0610)\n",
      "iter_dt 6316.78ms; iter 45: train loss 0.10798 temperature: 7.249999999999992\n",
      "mean_logits tensor([-1.0555, -1.1228, -0.9988, -1.1120, -1.0512], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1157, -1.1248, -1.1015, -1.1012, -1.1198])\n",
      "mean: tensor(-1.1126)\n",
      "iter_dt 6320.22ms; iter 46: train loss 0.03205 temperature: 7.299999999999992\n",
      "mean_logits tensor([-0.9537, -1.0515, -1.1275, -0.9576, -1.1132], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1313, -1.1077, -1.0891, -1.0498, -1.0989])\n",
      "mean: tensor(-1.0954)\n",
      "iter_dt 6313.43ms; iter 47: train loss 0.07208 temperature: 7.349999999999992\n",
      "mean_logits tensor([-0.9550, -0.8878, -1.0401, -1.0869, -0.9940], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0974, -1.1347, -1.1212, -0.9797, -1.0413])\n",
      "mean: tensor(-1.0749)\n",
      "iter_dt 6335.33ms; iter 48: train loss 0.15729 temperature: 7.3999999999999915\n",
      "mean_logits tensor([-1.1126, -1.0090, -1.0893, -0.9907, -1.1056], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1057, -1.0921, -0.9662, -1.1067, -1.1148])\n",
      "mean: tensor(-1.0771)\n",
      "iter_dt 6312.09ms; iter 49: train loss 0.05716 temperature: 7.449999999999991\n",
      "mean_logits tensor([-1.0114, -1.0613, -1.0293, -1.0757, -1.2018], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0563, -1.0180, -1.1204, -1.1272, -1.0977])\n",
      "mean: tensor(-1.0839)\n",
      "iter_dt 6317.36ms; iter 50: train loss 0.04686 temperature: 7.499999999999991\n",
      "mean_logits tensor([-1.0424, -1.0643, -1.0806, -1.0506, -1.1063], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1291, -1.1100, -1.0541, -1.0954, -1.0763])\n",
      "mean: tensor(-1.0930)\n",
      "iter_dt 6306.20ms; iter 51: train loss 0.02311 temperature: 7.549999999999991\n",
      "mean_logits tensor([-1.1705, -1.0171, -1.1179, -1.0590, -1.1516], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0864, -1.1114, -1.0525, -1.1247, -1.0123])\n",
      "mean: tensor(-1.0775)\n",
      "iter_dt 6313.88ms; iter 52: train loss 0.07748 temperature: 7.599999999999991\n",
      "mean_logits tensor([-1.1368, -1.0237, -1.0622, -1.2096, -1.0905], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9743, -1.0481, -1.0748, -1.0754, -1.1051])\n",
      "mean: tensor(-1.0556)\n",
      "iter_dt 6336.14ms; iter 53: train loss 0.08079 temperature: 7.649999999999991\n",
      "mean_logits tensor([-1.1064, -1.2038, -1.1750, -1.1671, -1.0824], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1301, -1.1153, -1.1168, -1.0483, -1.1197])\n",
      "mean: tensor(-1.1060)\n",
      "iter_dt 6367.48ms; iter 54: train loss 0.05214 temperature: 7.69999999999999\n",
      "mean_logits tensor([-1.1571, -1.1244, -1.1074, -1.0898, -1.0835], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1248, -1.0738, -1.0890, -1.0437, -1.1324])\n",
      "mean: tensor(-1.0928)\n",
      "iter_dt 6328.71ms; iter 55: train loss 0.01524 temperature: 7.74999999999999\n",
      "mean_logits tensor([-1.1478, -1.1753, -1.0737, -1.0646, -1.0503], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1224, -1.0695, -1.0422, -1.0986, -1.1030])\n",
      "mean: tensor(-1.0871)\n",
      "iter_dt 6328.13ms; iter 56: train loss 0.03084 temperature: 7.79999999999999\n",
      "mean_logits tensor([-1.0853, -1.0131, -1.1475, -1.1588, -1.0891], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0235, -1.0169, -1.1038, -1.0751, -1.1134])\n",
      "mean: tensor(-1.0665)\n",
      "iter_dt 6346.60ms; iter 57: train loss 0.02412 temperature: 7.84999999999999\n",
      "mean_logits tensor([-1.0424, -1.1107, -0.9770, -1.0539, -1.1039], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1269, -1.0829, -1.0611, -1.0780, -1.1370])\n",
      "mean: tensor(-1.0972)\n",
      "iter_dt 6318.57ms; iter 58: train loss 0.02780 temperature: 7.89999999999999\n",
      "mean_logits tensor([-1.0319, -1.0373, -1.0844, -1.0881, -0.9853], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0674, -1.0776, -1.1212, -1.0989, -1.1323])\n",
      "mean: tensor(-1.0995)\n",
      "iter_dt 6339.05ms; iter 59: train loss 0.04342 temperature: 7.9499999999999895\n",
      "mean_logits tensor([-1.0890, -1.0886, -1.0215, -1.0530, -1.0278], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1036, -1.1229, -1.1061, -1.1270, -1.0842])\n",
      "mean: tensor(-1.1088)\n",
      "iter_dt 6325.56ms; iter 60: train loss 0.02951 temperature: 7.999999999999989\n",
      "mean_logits tensor([-1.0746, -1.0276, -1.0058, -1.1194, -1.1188], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0795, -1.0864, -1.1320, -1.0058, -1.0745])\n",
      "mean: tensor(-1.0756)\n",
      "iter_dt 6302.46ms; iter 61: train loss 0.05796 temperature: 8.04999999999999\n",
      "mean_logits tensor([-1.0993, -1.0537, -1.0470, -1.1769, -1.1179], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0779, -1.1290, -1.1274, -1.1127, -1.1047])\n",
      "mean: tensor(-1.1103)\n",
      "iter_dt 6312.60ms; iter 62: train loss 0.03067 temperature: 8.09999999999999\n",
      "mean_logits tensor([-1.1102, -1.1935, -1.0894, -1.1701, -1.1936], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1189, -1.0762, -1.1227, -1.1190, -1.1302])\n",
      "mean: tensor(-1.1134)\n",
      "iter_dt 6321.22ms; iter 63: train loss 0.04221 temperature: 8.149999999999991\n",
      "mean_logits tensor([-1.1807, -1.1735, -1.1750, -0.9631, -1.0564], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1313, -1.0957, -1.1192, -1.1122, -1.1247])\n",
      "mean: tensor(-1.1166)\n",
      "iter_dt 6326.73ms; iter 64: train loss 0.06655 temperature: 8.199999999999992\n",
      "mean_logits tensor([-1.2099, -1.1027, -1.1119, -1.1500, -0.9146], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1325, -1.0325, -1.1243, -1.0273, -1.1207])\n",
      "mean: tensor(-1.0875)\n",
      "iter_dt 6301.58ms; iter 65: train loss 0.11304 temperature: 8.249999999999993\n",
      "mean_logits tensor([-1.1161, -1.2329, -1.0210, -1.1216, -1.1867], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1347, -1.0998, -1.1101, -1.0812, -1.0633])\n",
      "mean: tensor(-1.0978)\n",
      "iter_dt 6314.13ms; iter 66: train loss 0.08252 temperature: 8.299999999999994\n",
      "mean_logits tensor([-1.1202, -1.2399, -1.1419, -1.1726, -0.9851], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1167, -1.0864, -1.0856, -1.0433, -1.0703])\n",
      "mean: tensor(-1.0805)\n",
      "iter_dt 6289.85ms; iter 67: train loss 0.09629 temperature: 8.349999999999994\n",
      "mean_logits tensor([-1.0534, -1.0698, -1.1571, -1.0731, -1.1548], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1361, -1.1246, -1.1298, -1.0698, -1.1226])\n",
      "mean: tensor(-1.1166)\n",
      "iter_dt 6310.18ms; iter 68: train loss 0.02112 temperature: 8.399999999999995\n",
      "mean_logits tensor([-1.0521, -1.1195, -1.0895, -1.1053, -1.1384], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9636, -1.1211, -1.1125, -1.0483, -1.0982])\n",
      "mean: tensor(-1.0687)\n",
      "iter_dt 6303.96ms; iter 69: train loss 0.02136 temperature: 8.449999999999996\n",
      "mean_logits tensor([-1.1025, -1.0718, -1.0272, -1.0773, -1.0800], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1238, -1.0822, -1.0106, -1.1248, -1.1130])\n",
      "mean: tensor(-1.0909)\n",
      "iter_dt 6309.57ms; iter 70: train loss 0.00748 temperature: 8.499999999999996\n",
      "mean_logits tensor([-1.0758, -1.1416, -1.0477, -1.0514, -1.0087], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0849, -1.1092, -1.1328, -0.8922, -1.1319])\n",
      "mean: tensor(-1.0702)\n",
      "iter_dt 6398.85ms; iter 71: train loss 0.07624 temperature: 8.549999999999997\n",
      "mean_logits tensor([-1.1266, -1.0459, -0.9807, -1.0723, -0.9946], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1209, -0.9882, -1.0713, -1.1155, -1.1330])\n",
      "mean: tensor(-1.0858)\n",
      "iter_dt 6324.51ms; iter 72: train loss 0.05346 temperature: 8.599999999999998\n",
      "mean_logits tensor([-1.0742, -0.9927, -1.0678, -1.0619, -1.0643], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1059, -1.0318, -1.1272, -1.0735, -1.1316])\n",
      "mean: tensor(-1.0940)\n",
      "iter_dt 6309.98ms; iter 73: train loss 0.01879 temperature: 8.649999999999999\n",
      "mean_logits tensor([-1.0922, -1.0526, -0.9787, -1.1270, -1.0871], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1344, -1.0036, -1.0693, -1.1278, -1.1326])\n",
      "mean: tensor(-1.0935)\n",
      "iter_dt 6306.68ms; iter 74: train loss 0.02358 temperature: 8.7\n",
      "mean_logits tensor([-1.0652, -1.0445, -0.9979, -1.0196, -1.1026], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1285, -1.1212, -1.0873, -1.1166, -1.0448])\n",
      "mean: tensor(-1.0997)\n",
      "iter_dt 6303.63ms; iter 75: train loss 0.05198 temperature: 8.75\n",
      "mean_logits tensor([-0.9923, -1.0408, -1.1526, -1.0807, -1.0745], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1259, -1.1321, -1.1079, -1.1354, -1.1179])\n",
      "mean: tensor(-1.1238)\n",
      "iter_dt 6321.34ms; iter 76: train loss 0.05706 temperature: 8.8\n",
      "mean_logits tensor([-1.1705, -1.1322, -1.1058, -1.1369, -1.0580], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0849, -1.0839, -1.1320, -1.0961, -1.1153])\n",
      "mean: tensor(-1.1024)\n",
      "iter_dt 6302.22ms; iter 77: train loss 0.02842 temperature: 8.850000000000001\n",
      "mean_logits tensor([-1.1822, -1.1382, -1.1251, -1.0867, -1.1372], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1181, -1.1283, -1.0971, -1.1047, -1.0944])\n",
      "mean: tensor(-1.1085)\n",
      "iter_dt 6302.84ms; iter 78: train loss 0.01381 temperature: 8.900000000000002\n",
      "mean_logits tensor([-1.1245, -1.2021, -1.2112, -1.2030, -1.1930], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1298, -1.1187, -1.0598, -1.1046, -1.1324])\n",
      "mean: tensor(-1.1090)\n",
      "iter_dt 6320.19ms; iter 79: train loss 0.08578 temperature: 8.950000000000003\n",
      "mean_logits tensor([-1.1406, -1.1652, -1.0990, -1.2506, -1.1294], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1177, -1.1066, -1.0799, -1.0632, -1.1301])\n",
      "mean: tensor(-1.0995)\n",
      "iter_dt 6310.24ms; iter 80: train loss 0.07959 temperature: 9.000000000000004\n",
      "mean_logits tensor([-1.2131, -1.1229, -1.0647, -1.1305, -1.1248], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0936, -1.1344, -1.1346, -1.1268, -1.1076])\n",
      "mean: tensor(-1.1194)\n",
      "iter_dt 6308.39ms; iter 81: train loss 0.03832 temperature: 9.050000000000004\n",
      "mean_logits tensor([-1.1620, -1.2166, -1.1959, -1.1888, -1.1156], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1263, -1.1221, -1.1351, -1.0770, -1.0624])\n",
      "mean: tensor(-1.1046)\n",
      "iter_dt 6327.73ms; iter 82: train loss 0.05779 temperature: 9.100000000000005\n",
      "mean_logits tensor([-1.2099, -1.1850, -1.1864, -1.0804, -1.1481], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0924, -1.0997, -1.0898, -1.1165, -1.0736])\n",
      "mean: tensor(-1.0944)\n",
      "iter_dt 6352.10ms; iter 83: train loss 0.07275 temperature: 9.150000000000006\n",
      "mean_logits tensor([-1.1688, -1.1621, -1.1306, -1.1927, -1.1761], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1293, -1.1026, -1.1214, -1.1198, -1.1054])\n",
      "mean: tensor(-1.1157)\n",
      "iter_dt 6310.12ms; iter 84: train loss 0.03058 temperature: 9.200000000000006\n",
      "mean_logits tensor([-1.1227, -1.0939, -1.0988, -1.1637, -1.0277], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1195, -1.1219, -1.1310, -1.1242, -1.0790])\n",
      "mean: tensor(-1.1151)\n",
      "iter_dt 6338.29ms; iter 85: train loss 0.01080 temperature: 9.250000000000007\n",
      "mean_logits tensor([-1.1094, -1.0905, -1.1477, -1.0593, -1.0993], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1193, -1.0959, -1.1258, -1.1236, -1.1336])\n",
      "mean: tensor(-1.1196)\n",
      "iter_dt 6303.95ms; iter 86: train loss 0.01069 temperature: 9.300000000000008\n",
      "mean_logits tensor([-1.0585, -1.0755, -1.0591, -1.1080, -1.0778], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1350, -1.1288, -1.1364, -1.0965, -1.1231])\n",
      "mean: tensor(-1.1240)\n",
      "iter_dt 6340.60ms; iter 87: train loss 0.03038 temperature: 9.350000000000009\n",
      "mean_logits tensor([-1.1382, -1.1151, -1.1194, -1.0748, -1.1583], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1182, -1.0219, -1.1257, -1.1127, -1.1245])\n",
      "mean: tensor(-1.1006)\n",
      "iter_dt 6320.53ms; iter 88: train loss 0.02037 temperature: 9.40000000000001\n",
      "mean_logits tensor([-1.1493, -1.1070, -1.1243, -1.0901, -1.1671], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1189, -0.9571, -1.1258, -1.0916, -1.0889])\n",
      "mean: tensor(-1.0764)\n",
      "iter_dt 6330.39ms; iter 89: train loss 0.04893 temperature: 9.45000000000001\n",
      "mean_logits tensor([-1.1618, -1.0679, -1.0848, -1.0754, -1.1273], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1104, -1.0897, -1.0649, -1.1073, -1.0943])\n",
      "mean: tensor(-1.0933)\n",
      "iter_dt 6345.76ms; iter 90: train loss 0.01044 temperature: 9.50000000000001\n",
      "mean_logits tensor([-1.0899, -1.0706, -1.0534, -1.1673, -1.1023], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1183, -1.1064, -1.1018, -1.1165, -1.1265])\n",
      "mean: tensor(-1.1139)\n",
      "iter_dt 6366.80ms; iter 91: train loss 0.01393 temperature: 9.550000000000011\n",
      "mean_logits tensor([-1.1002, -1.1097, -1.0702, -1.0395, -1.1374], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1183, -1.1238, -1.1110, -1.0962, -1.1322])\n",
      "mean: tensor(-1.1163)\n",
      "iter_dt 6365.23ms; iter 92: train loss 0.00941 temperature: 9.600000000000012\n",
      "mean_logits tensor([-1.0436, -1.1684, -1.1021, -1.1248, -1.0941], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1066, -1.1364, -1.1268, -1.1077, -1.1169])\n",
      "mean: tensor(-1.1189)\n",
      "iter_dt 6374.93ms; iter 93: train loss 0.01148 temperature: 9.650000000000013\n",
      "mean_logits tensor([-1.1294, -1.0871, -1.1604, -1.1193, -1.0621], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0997, -1.1341, -1.1116, -1.1142, -1.1121])\n",
      "mean: tensor(-1.1143)\n",
      "iter_dt 6346.83ms; iter 94: train loss 0.01478 temperature: 9.700000000000014\n",
      "mean_logits tensor([-1.1117, -1.1814, -1.1629, -1.1518, -1.1401], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1367, -1.1273, -1.1325, -1.1235, -1.1228])\n",
      "mean: tensor(-1.1286)\n",
      "iter_dt 6376.16ms; iter 95: train loss 0.01107 temperature: 9.750000000000014\n",
      "mean_logits tensor([-1.1703, -1.1109, -1.0665, -1.0944, -1.0594], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1082, -1.0955, -1.0653, -1.0853, -1.1206])\n",
      "mean: tensor(-1.0950)\n",
      "iter_dt 6352.15ms; iter 96: train loss 0.01471 temperature: 9.800000000000015\n",
      "mean_logits tensor([-1.1151, -1.1687, -1.1364, -1.1264, -1.1652], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1327, -1.0652, -1.1231, -1.1275, -1.1223])\n",
      "mean: tensor(-1.1142)\n",
      "iter_dt 6362.94ms; iter 97: train loss 0.02456 temperature: 9.850000000000016\n",
      "mean_logits tensor([-1.1345, -1.0798, -1.0318, -1.0728, -1.0942], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1032, -1.1248, -1.1223, -1.1004, -1.1133])\n",
      "mean: tensor(-1.1128)\n",
      "iter_dt 6336.83ms; iter 98: train loss 0.02164 temperature: 9.900000000000016\n",
      "mean_logits tensor([-1.1181, -1.1431, -1.0049, -1.1336, -1.1799], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1066, -1.0840, -1.0992, -1.1248, -1.1212])\n",
      "mean: tensor(-1.1072)\n",
      "iter_dt 6377.74ms; iter 99: train loss 0.02836 temperature: 9.950000000000017\n",
      "converged SCF energy = -1.11085039747659\n",
      "Starting to parse FermionOperator using 4 qubits...\n",
      "\n",
      "Operator t:  -0.16733398905695293 [] +\n",
      "-0.046156695889015276 [X0 X1 Y2 Y3] +\n",
      "0.046156695889015276 [X0 Y1 Y2 X3] +\n",
      "0.046156695889015276 [Y0 X1 X2 Y3] +\n",
      "-0.046156695889015276 [Y0 Y1 X2 X3] +\n",
      "0.16251648748871664 [Z0] +\n",
      "0.16583253721590405 [Z0 Z1] +\n",
      "0.11720364720195843 [Z0 Z2] +\n",
      "0.1633603430909737 [Z0 Z3] +\n",
      "0.1625164874887167 [Z1] +\n",
      "0.1633603430909737 [Z1 Z2] +\n",
      "0.11720364720195843 [Z1 Z3] +\n",
      "-0.19744293699755783 [Z2] +\n",
      "0.17169788392286706 [Z2 Z3] +\n",
      "-0.19744293699755783 [Z3]\n",
      "Term, coeff:  () -0.16733398905695293\n",
      "Term, coeff:  ((0, 'Z'),) 0.16251648748871664\n",
      "Index, p_char:  0 Z\n",
      "Term, coeff:  ((1, 'Z'),) 0.1625164874887167\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((2, 'Z'),) -0.19744293699755783\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((3, 'Z'),) -0.19744293699755783\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((0, 'Z'), (1, 'Z')) 0.16583253721590405\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((0, 'Y'), (1, 'X'), (2, 'X'), (3, 'Y')) 0.046156695889015276\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'Y'), (1, 'Y'), (2, 'X'), (3, 'X')) -0.046156695889015276\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'X'), (1, 'X'), (2, 'Y'), (3, 'Y')) -0.046156695889015276\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'X'), (1, 'Y'), (2, 'Y'), (3, 'X')) 0.046156695889015276\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'Z'), (2, 'Z')) 0.11720364720195843\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((0, 'Z'), (3, 'Z')) 0.1633603430909737\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((1, 'Z'), (2, 'Z')) 0.1633603430909737\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((1, 'Z'), (3, 'Z')) 0.11720364720195843\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((2, 'Z'), (3, 'Z')) 0.17169788392286706\n",
      "Index, p_char:  2 Z\n",
      "Index, p_char:  3 Z\n",
      "ground state: -1.1341476666770958\n",
      "hf state: -1.110850397476595\n",
      "number of parameters: 85.29M\n",
      "running on device cpu\n",
      "mean_logits tensor([-1.1676, -0.9894, -1.0338, -1.0321, -1.2728], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1303, -1.1067, -1.0957, -1.1100, -0.9557])\n",
      "mean: tensor(-1.0797)\n",
      "iter_dt 0.00ms; iter 0: train loss 0.23027 temperature: 5\n",
      "mean_logits tensor([-0.9162, -0.9515, -0.9750, -0.9761, -0.8963], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9692, -1.0988, -1.1153, -1.0494, -1.1140])\n",
      "mean: tensor(-1.0693)\n",
      "iter_dt 1694970733482.34ms; iter 1: train loss 0.14860 temperature: 5.05\n",
      "mean_logits tensor([-1.1317, -1.3054, -1.3667, -1.3046, -1.0760], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1051, -1.1159, -1.0708, -0.5909, -1.1158])\n",
      "mean: tensor(-0.9997)\n",
      "iter_dt 6380.98ms; iter 2: train loss 0.99442 temperature: 5.1\n",
      "mean_logits tensor([-1.1523, -1.2480, -1.1779, -1.1267, -1.1484], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9467, -1.0950, -1.0938, -1.1185, -1.0214])\n",
      "mean: tensor(-1.0551)\n",
      "iter_dt 6420.37ms; iter 3: train loss 0.16019 temperature: 5.1499999999999995\n",
      "mean_logits tensor([-0.8548, -1.3030, -1.2347, -1.2320, -1.2478], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0704, -1.0998, -1.1048, -0.8885, -0.9448])\n",
      "mean: tensor(-1.0217)\n",
      "iter_dt 6399.76ms; iter 4: train loss 0.55513 temperature: 5.199999999999999\n",
      "mean_logits tensor([-1.3639, -1.0454, -1.2578, -0.8518, -1.0098], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1258, -1.0608, -1.1254, -1.1218, -1.0930])\n",
      "mean: tensor(-1.1053)\n",
      "iter_dt 6408.69ms; iter 5: train loss 0.29273 temperature: 5.249999999999999\n",
      "mean_logits tensor([-1.1812, -1.0026, -1.1885, -1.0630, -1.0874], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0269, -1.1130, -1.0471, -1.0895, -1.1130])\n",
      "mean: tensor(-1.0779)\n",
      "iter_dt 6388.50ms; iter 6: train loss 0.10350 temperature: 5.299999999999999\n",
      "mean_logits tensor([-1.1056, -1.0255, -1.0254, -1.0801, -0.9362], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0817, -0.9936, -0.9432, -1.0708, -1.0267])\n",
      "mean: tensor(-1.0232)\n",
      "iter_dt 6391.14ms; iter 7: train loss 0.02405 temperature: 5.349999999999999\n",
      "mean_logits tensor([-1.1557, -0.8766, -0.8829, -0.9759, -1.0495], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0688, -1.0987, -1.1127, -1.0515, -1.0928])\n",
      "mean: tensor(-1.0849)\n",
      "iter_dt 6400.06ms; iter 8: train loss 0.17529 temperature: 5.399999999999999\n",
      "mean_logits tensor([-0.9863, -0.8600, -0.7978, -0.9864, -0.9880], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0924, -1.0305, -1.0526, -1.1179, -1.0602])\n",
      "mean: tensor(-1.0707)\n",
      "iter_dt 6405.14ms; iter 9: train loss 0.17614 temperature: 5.449999999999998\n",
      "mean_logits tensor([-0.8902, -1.0382, -0.8977, -0.9379, -1.0482], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1058, -1.1174, -1.1107, -1.1244, -1.0984])\n",
      "mean: tensor(-1.1113)\n",
      "iter_dt 6420.68ms; iter 10: train loss 0.20647 temperature: 5.499999999999998\n",
      "mean_logits tensor([-0.9100, -1.1214, -1.1565, -0.9291, -0.9587], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1187, -1.1122, -1.0844, -1.0959, -1.1308])\n",
      "mean: tensor(-1.1084)\n",
      "iter_dt 6405.61ms; iter 11: train loss 0.16666 temperature: 5.549999999999998\n",
      "mean_logits tensor([-1.0200, -0.9098, -1.0724, -0.9993, -0.9746], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0882, -1.0892, -1.1064, -1.1313, -1.0944])\n",
      "mean: tensor(-1.1019)\n",
      "iter_dt 6387.27ms; iter 12: train loss 0.10947 temperature: 5.599999999999998\n",
      "mean_logits tensor([-1.0304, -1.2163, -1.0070, -0.8977, -0.9586], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1292, -1.0896, -1.1278, -1.1236, -1.1221])\n",
      "mean: tensor(-1.1185)\n",
      "iter_dt 6397.93ms; iter 13: train loss 0.19414 temperature: 5.649999999999998\n",
      "mean_logits tensor([-1.0598, -1.0695, -1.0338, -0.9000, -1.1247], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0962, -1.0148, -1.1002, -1.1223, -1.0974])\n",
      "mean: tensor(-1.0862)\n",
      "iter_dt 6400.19ms; iter 14: train loss 0.09085 temperature: 5.6999999999999975\n",
      "mean_logits tensor([-1.1047, -1.0818, -1.0251, -1.2935, -1.2185], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0505, -1.1094, -1.0523, -1.1117, -1.1152])\n",
      "mean: tensor(-1.0878)\n",
      "iter_dt 6396.07ms; iter 15: train loss 0.10313 temperature: 5.749999999999997\n",
      "mean_logits tensor([-1.1588, -1.0870, -1.0492, -1.1248, -1.0888], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0824, -1.1002, -1.1188, -1.0694, -1.0803])\n",
      "mean: tensor(-1.0902)\n",
      "iter_dt 6387.64ms; iter 16: train loss 0.02540 temperature: 5.799999999999997\n",
      "mean_logits tensor([-1.1369, -1.0841, -1.1098, -1.2237, -1.0872], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1224, -1.1304, -1.1045, -1.0687, -1.0962])\n",
      "mean: tensor(-1.1045)\n",
      "iter_dt 6413.39ms; iter 17: train loss 0.05214 temperature: 5.849999999999997\n",
      "mean_logits tensor([-1.2459, -1.1283, -1.1473, -1.0923, -1.1188], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1137, -1.1107, -1.1079, -1.1159, -1.0357])\n",
      "mean: tensor(-1.0968)\n",
      "iter_dt 6402.45ms; iter 18: train loss 0.05352 temperature: 5.899999999999997\n",
      "mean_logits tensor([-1.1058, -1.2289, -1.0861, -1.2774, -1.2912], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1188, -1.1033, -1.1168, -1.1190, -1.0362])\n",
      "mean: tensor(-1.0988)\n",
      "iter_dt 6396.91ms; iter 19: train loss 0.22400 temperature: 5.949999999999997\n",
      "mean_logits tensor([-1.1796, -1.2109, -1.3227, -1.0534, -1.2558], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1277, -0.9411, -1.0806, -1.1123, -1.1293])\n",
      "mean: tensor(-1.0782)\n",
      "iter_dt 6405.50ms; iter 20: train loss 0.30244 temperature: 5.9999999999999964\n",
      "mean_logits tensor([-1.0844, -1.0597, -1.1175, -1.1208, -1.1509], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0245, -1.0298, -1.1231, -1.0976, -1.0946])\n",
      "mean: tensor(-1.0739)\n",
      "iter_dt 6391.16ms; iter 21: train loss 0.01440 temperature: 6.049999999999996\n",
      "mean_logits tensor([-1.0569, -1.1410, -1.0532, -1.2806, -1.2212], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1054, -1.0186, -1.1245, -1.1273, -1.1255])\n",
      "mean: tensor(-1.1003)\n",
      "iter_dt 6401.76ms; iter 22: train loss 0.11053 temperature: 6.099999999999996\n",
      "mean_logits tensor([-1.2135, -1.0931, -1.0775, -1.1603, -1.0235], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1138, -1.0769, -1.1194, -1.0434, -1.0567])\n",
      "mean: tensor(-1.0820)\n",
      "iter_dt 6403.02ms; iter 23: train loss 0.05052 temperature: 6.149999999999996\n",
      "mean_logits tensor([-1.0179, -1.0235, -1.1411, -1.0454, -1.1680], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1141, -1.0131, -1.0833, -1.0736, -1.1239])\n",
      "mean: tensor(-1.0816)\n",
      "iter_dt 6390.80ms; iter 24: train loss 0.02715 temperature: 6.199999999999996\n",
      "mean_logits tensor([-1.1697, -1.0916, -1.0075, -1.1682, -1.2183], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1067, -1.0956, -1.0887, -1.1068, -1.1268])\n",
      "mean: tensor(-1.1049)\n",
      "iter_dt 6400.00ms; iter 25: train loss 0.04332 temperature: 6.249999999999996\n",
      "mean_logits tensor([-1.0972, -0.9731, -1.0432, -1.1350, -1.1129], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0901, -1.0892, -1.0918, -1.1219, -1.1048])\n",
      "mean: tensor(-1.0995)\n",
      "iter_dt 6417.23ms; iter 26: train loss 0.02574 temperature: 6.299999999999995\n",
      "mean_logits tensor([-1.1004, -1.0275, -1.0807, -1.0685, -1.1917], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1130, -1.1000, -1.0923, -1.1040, -1.0711])\n",
      "mean: tensor(-1.0961)\n",
      "iter_dt 6404.96ms; iter 27: train loss 0.03955 temperature: 6.349999999999995\n",
      "mean_logits tensor([-1.0234, -1.0474, -1.1060, -0.9188, -1.0637], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1291, -0.9272, -1.1009, -1.0704, -1.0642])\n",
      "mean: tensor(-1.0584)\n",
      "iter_dt 6429.69ms; iter 28: train loss 0.07378 temperature: 6.399999999999995\n",
      "mean_logits tensor([-0.9985, -1.1044, -0.8415, -1.1310, -0.8925], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1094, -1.0162, -1.0951, -1.1224, -1.0844])\n",
      "mean: tensor(-1.0855)\n",
      "iter_dt 6400.99ms; iter 29: train loss 0.17646 temperature: 6.449999999999995\n",
      "mean_logits tensor([-1.0768, -1.1885, -1.1581, -1.0545, -1.0219], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1310, -0.9287, -1.0450, -1.1205, -1.1160])\n",
      "mean: tensor(-1.0682)\n",
      "iter_dt 6402.57ms; iter 30: train loss 0.16402 temperature: 6.499999999999995\n",
      "mean_logits tensor([-1.0070, -1.1381, -1.0116, -1.2264, -1.0436], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1046, -1.0223, -1.1017, -1.0492, -1.0904])\n",
      "mean: tensor(-1.0736)\n",
      "iter_dt 6404.88ms; iter 31: train loss 0.11748 temperature: 6.5499999999999945\n",
      "mean_logits tensor([-1.0275, -1.1207, -1.1015, -1.1751, -1.0769], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1055, -0.9750, -1.1140, -1.0984, -1.0833])\n",
      "mean: tensor(-1.0753)\n",
      "iter_dt 6404.65ms; iter 32: train loss 0.05662 temperature: 6.599999999999994\n",
      "mean_logits tensor([-1.0024, -1.0749, -1.0650, -1.2282, -1.1632], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1064, -1.1152, -1.1098, -1.0100, -1.1253])\n",
      "mean: tensor(-1.0933)\n",
      "iter_dt 6403.70ms; iter 33: train loss 0.11674 temperature: 6.649999999999994\n",
      "mean_logits tensor([-1.0644, -1.0965, -1.1102, -0.9764, -1.0772], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1010, -0.9348, -1.0992, -1.1222, -1.0445])\n",
      "mean: tensor(-1.0603)\n",
      "iter_dt 6396.98ms; iter 34: train loss 0.07901 temperature: 6.699999999999994\n",
      "mean_logits tensor([-1.0322, -0.9394, -1.1312, -0.9423, -1.1937], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1223, -0.9082, -1.1104, -1.1245, -1.1182])\n",
      "mean: tensor(-1.0767)\n",
      "iter_dt 6400.54ms; iter 35: train loss 0.08012 temperature: 6.749999999999994\n",
      "mean_logits tensor([-1.1026, -1.1694, -1.0934, -1.0098, -1.0780], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1057, -1.0622, -1.0839, -1.0796, -1.1135])\n",
      "mean: tensor(-1.0890)\n",
      "iter_dt 6410.60ms; iter 36: train loss 0.03174 temperature: 6.799999999999994\n",
      "mean_logits tensor([-0.9497, -1.0745, -0.9515, -1.0595, -1.1220], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0862, -1.0809, -1.0862, -1.0933, -1.0779])\n",
      "mean: tensor(-1.0849)\n",
      "iter_dt 6410.72ms; iter 37: train loss 0.06200 temperature: 6.849999999999993\n",
      "mean_logits tensor([-1.1479, -0.9631, -1.1891, -1.0631, -0.9633], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0786, -1.1027, -1.0220, -1.0269, -1.1229])\n",
      "mean: tensor(-1.0706)\n",
      "iter_dt 6401.71ms; iter 38: train loss 0.13394 temperature: 6.899999999999993\n",
      "mean_logits tensor([-1.1432, -0.9821, -0.9980, -0.9993, -1.1868], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0576, -1.0804, -1.1173, -1.0874, -1.0842])\n",
      "mean: tensor(-1.0854)\n",
      "iter_dt 6398.18ms; iter 39: train loss 0.08502 temperature: 6.949999999999993\n",
      "mean_logits tensor([-0.9595, -1.0621, -1.1420, -0.9863, -1.0687], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0991, -1.0884, -1.1321, -1.1291, -1.0906])\n",
      "mean: tensor(-1.1079)\n",
      "iter_dt 6403.05ms; iter 40: train loss 0.06668 temperature: 6.999999999999993\n",
      "mean_logits tensor([-1.0724, -1.0453, -1.1540, -1.0185, -1.0593], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1101, -1.0979, -1.0718, -1.1080, -1.0417])\n",
      "mean: tensor(-1.0859)\n",
      "iter_dt 6411.44ms; iter 41: train loss 0.03369 temperature: 7.049999999999993\n",
      "mean_logits tensor([-1.1322, -1.1901, -1.1281, -1.1037, -1.0485], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0892, -1.0279, -1.0814, -1.0283, -1.1128])\n",
      "mean: tensor(-1.0679)\n",
      "iter_dt 6400.04ms; iter 42: train loss 0.07259 temperature: 7.0999999999999925\n",
      "mean_logits tensor([-1.1067, -1.0701, -1.1389, -1.1114, -0.9585], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1311, -1.0193, -1.0156, -1.0330, -1.0941])\n",
      "mean: tensor(-1.0586)\n",
      "iter_dt 6399.89ms; iter 43: train loss 0.07071 temperature: 7.149999999999992\n",
      "mean_logits tensor([-1.1358, -1.0705, -0.9692, -1.0678, -1.1387], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1193, -1.0952, -1.1115, -1.0237, -1.0927])\n",
      "mean: tensor(-1.0885)\n",
      "iter_dt 6398.82ms; iter 44: train loss 0.04114 temperature: 7.199999999999992\n",
      "mean_logits tensor([-1.2344, -1.0177, -1.1181, -1.0656, -1.1211], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1181, -1.0772, -1.1108, -0.9529, -1.0808])\n",
      "mean: tensor(-1.0680)\n",
      "iter_dt 6409.30ms; iter 45: train loss 0.05639 temperature: 7.249999999999992\n",
      "mean_logits tensor([-0.9979, -1.1812, -1.2041, -1.1934, -1.1045], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1174, -1.0548, -1.1042, -1.0396, -1.1106])\n",
      "mean: tensor(-1.0853)\n",
      "iter_dt 6404.65ms; iter 46: train loss 0.11802 temperature: 7.299999999999992\n",
      "mean_logits tensor([-1.1874, -1.0601, -1.0902, -1.0808, -1.0954], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0973, -1.1096, -1.0675, -1.0961, -1.1243])\n",
      "mean: tensor(-1.0990)\n",
      "iter_dt 6411.26ms; iter 47: train loss 0.02311 temperature: 7.349999999999992\n",
      "mean_logits tensor([-1.0912, -1.0298, -1.1144, -1.2698, -1.0739], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1176, -1.0491, -1.1058, -1.1131, -1.0959])\n",
      "mean: tensor(-1.0963)\n",
      "iter_dt 6402.52ms; iter 48: train loss 0.05616 temperature: 7.3999999999999915\n",
      "mean_logits tensor([-1.0868, -1.0790, -1.1201, -1.1624, -1.0905], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0762, -1.0931, -1.0895, -1.0846, -1.1139])\n",
      "mean: tensor(-1.0914)\n",
      "iter_dt 6400.51ms; iter 49: train loss 0.01471 temperature: 7.449999999999991\n",
      "mean_logits tensor([-0.9506, -1.0939, -1.1817, -0.9962, -1.0803], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1056, -1.1058, -1.1221, -1.1035, -1.0390])\n",
      "mean: tensor(-1.0952)\n",
      "iter_dt 6421.98ms; iter 50: train loss 0.06664 temperature: 7.499999999999991\n",
      "mean_logits tensor([-0.9969, -1.1192, -1.0138, -1.1380, -1.1486], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0842, -1.1004, -1.1175, -1.0774, -1.0665])\n",
      "mean: tensor(-1.0892)\n",
      "iter_dt 6406.68ms; iter 51: train loss 0.05008 temperature: 7.549999999999991\n",
      "mean_logits tensor([-1.0866, -1.0978, -1.0122, -1.0688, -1.1948], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0925, -1.1154, -1.0378, -1.1259, -1.1229])\n",
      "mean: tensor(-1.0989)\n",
      "iter_dt 6405.38ms; iter 52: train loss 0.01800 temperature: 7.599999999999991\n",
      "mean_logits tensor([-1.1220, -1.0767, -1.0616, -1.0208, -1.1050], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1292, -1.1001, -1.0945, -1.0563, -1.0985])\n",
      "mean: tensor(-1.0957)\n",
      "iter_dt 6399.05ms; iter 53: train loss 0.00503 temperature: 7.649999999999991\n",
      "mean_logits tensor([-1.2063, -1.1125, -1.1281, -1.0666, -1.0253], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0787, -1.1174, -1.1086, -1.0301, -1.0483])\n",
      "mean: tensor(-1.0766)\n",
      "iter_dt 6405.65ms; iter 54: train loss 0.03580 temperature: 7.69999999999999\n",
      "mean_logits tensor([-1.0588, -1.0772, -1.1081, -1.1290, -1.0052], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9553, -1.0246, -1.0461, -1.0816, -1.1314])\n",
      "mean: tensor(-1.0478)\n",
      "iter_dt 6417.54ms; iter 55: train loss 0.05829 temperature: 7.74999999999999\n",
      "mean_logits tensor([-1.0046, -1.0734, -1.0239, -1.1227, -0.9494], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0845, -0.9935, -1.0410, -1.1233, -1.0912])\n",
      "mean: tensor(-1.0667)\n",
      "iter_dt 6402.68ms; iter 56: train loss 0.05187 temperature: 7.79999999999999\n",
      "mean_logits tensor([-1.0847, -1.0539, -1.0138, -1.0806, -1.0143], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1182, -1.0844, -1.0696, -1.0294, -1.0476])\n",
      "mean: tensor(-1.0698)\n",
      "iter_dt 6403.49ms; iter 57: train loss 0.01466 temperature: 7.84999999999999\n",
      "mean_logits tensor([-1.0713, -1.0371, -1.0189, -1.1368, -1.2585], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0837, -1.0886, -1.1213, -1.0032, -1.1256])\n",
      "mean: tensor(-1.0845)\n",
      "iter_dt 6400.87ms; iter 58: train loss 0.09134 temperature: 7.89999999999999\n",
      "mean_logits tensor([-1.0901, -1.1242, -1.1022, -1.0748, -1.0302], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9064, -1.0983, -1.0749, -1.0806, -1.1320])\n",
      "mean: tensor(-1.0584)\n",
      "iter_dt 6402.27ms; iter 59: train loss 0.07049 temperature: 7.9499999999999895\n",
      "mean_logits tensor([-1.1776, -1.0889, -1.0855, -1.0622, -1.1563], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0309, -1.0501, -0.9873, -1.1222, -1.0947])\n",
      "mean: tensor(-1.0570)\n",
      "iter_dt 6401.59ms; iter 60: train loss 0.07076 temperature: 7.999999999999989\n",
      "mean_logits tensor([-0.9845, -1.0946, -1.2099, -1.0843, -1.0609], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1338, -1.0511, -0.8442, -1.1072, -1.0047])\n",
      "mean: tensor(-1.0282)\n",
      "iter_dt 6400.37ms; iter 61: train loss 0.25722 temperature: 8.04999999999999\n",
      "mean_logits tensor([-1.1598, -1.1065, -1.0253, -1.0219, -1.0748], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1052, -1.0714, -0.9872, -1.1174, -0.9448])\n",
      "mean: tensor(-1.0452)\n",
      "iter_dt 6398.93ms; iter 62: train loss 0.05112 temperature: 8.09999999999999\n",
      "mean_logits tensor([-1.2696, -1.0884, -1.0938, -1.0736, -1.0865], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1299, -1.0742, -1.1193, -1.1060, -1.0875])\n",
      "mean: tensor(-1.1034)\n",
      "iter_dt 6389.66ms; iter 63: train loss 0.04644 temperature: 8.149999999999991\n",
      "mean_logits tensor([-0.8827, -1.1248, -1.0196, -0.9941, -1.1713], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0539, -0.9373, -1.0733, -1.0999, -1.1340])\n",
      "mean: tensor(-1.0597)\n",
      "iter_dt 6407.45ms; iter 64: train loss 0.12179 temperature: 8.199999999999992\n",
      "mean_logits tensor([-1.0720, -1.0707, -1.0446, -1.1027, -1.0508], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0740, -0.9705, -1.0931, -1.1021, -1.1151])\n",
      "mean: tensor(-1.0710)\n",
      "iter_dt 6443.17ms; iter 65: train loss 0.02670 temperature: 8.249999999999993\n",
      "mean_logits tensor([-1.0220, -1.0199, -1.1095, -0.9838, -1.0834], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0745, -1.1041, -1.0341, -1.0997, -0.8658])\n",
      "mean: tensor(-1.0356)\n",
      "iter_dt 6456.43ms; iter 66: train loss 0.11443 temperature: 8.299999999999994\n",
      "mean_logits tensor([-1.0960, -1.0276, -0.9356, -1.0799, -0.9949], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1175, -1.0757, -1.1068, -1.1322, -1.0961])\n",
      "mean: tensor(-1.1057)\n",
      "iter_dt 6387.97ms; iter 67: train loss 0.07153 temperature: 8.349999999999994\n",
      "mean_logits tensor([-1.0587, -1.0234, -1.0966, -1.1639, -1.0688], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1259, -1.0445, -1.1036, -1.0676, -1.1262])\n",
      "mean: tensor(-1.0936)\n",
      "iter_dt 6391.02ms; iter 68: train loss 0.03204 temperature: 8.399999999999995\n",
      "mean_logits tensor([-1.0806, -1.1411, -1.0647, -1.1224, -1.1170], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0893, -1.0146, -1.0979, -1.1233, -1.1070])\n",
      "mean: tensor(-1.0864)\n",
      "iter_dt 6392.55ms; iter 69: train loss 0.02989 temperature: 8.449999999999996\n",
      "mean_logits tensor([-1.0528, -0.9256, -0.9745, -1.0395, -1.1161], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1009, -1.1050, -1.0868, -1.0936, -1.1195])\n",
      "mean: tensor(-1.1011)\n",
      "iter_dt 6391.22ms; iter 70: train loss 0.07793 temperature: 8.499999999999996\n",
      "mean_logits tensor([-1.0784, -0.9880, -1.1428, -1.0878, -1.2170], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0908, -1.1280, -1.1241, -1.0857, -1.1331])\n",
      "mean: tensor(-1.1123)\n",
      "iter_dt 6392.15ms; iter 71: train loss 0.04827 temperature: 8.549999999999997\n",
      "mean_logits tensor([-0.9483, -1.0181, -1.0520, -0.9785, -1.0120], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9405, -1.0849, -1.1280, -1.1231, -1.1086])\n",
      "mean: tensor(-1.0770)\n",
      "iter_dt 6390.16ms; iter 72: train loss 0.06744 temperature: 8.599999999999998\n",
      "mean_logits tensor([-1.1424, -1.0756, -1.1115, -1.1147, -1.1422], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1029, -1.0936, -1.0980, -1.1219, -0.9712])\n",
      "mean: tensor(-1.0775)\n",
      "iter_dt 6396.02ms; iter 73: train loss 0.05246 temperature: 8.649999999999999\n",
      "mean_logits tensor([-1.1251, -1.1007, -1.0939, -1.0907, -1.1866], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1156, -1.1190, -1.0432, -1.1022, -1.1227])\n",
      "mean: tensor(-1.1005)\n",
      "iter_dt 6397.71ms; iter 74: train loss 0.01360 temperature: 8.7\n",
      "mean_logits tensor([-1.0229, -1.2040, -0.9879, -1.0981, -1.0525], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1232, -1.1183, -1.1128, -1.1160, -1.1267])\n",
      "mean: tensor(-1.1194)\n",
      "iter_dt 6395.47ms; iter 75: train loss 0.06806 temperature: 8.75\n",
      "mean_logits tensor([-1.0461, -1.0544, -1.0372, -1.0537, -0.9338], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1257, -1.0740, -1.1173, -1.1248, -1.0839])\n",
      "mean: tensor(-1.1051)\n",
      "iter_dt 6390.46ms; iter 76: train loss 0.06572 temperature: 8.8\n",
      "mean_logits tensor([-1.0838, -1.1304, -1.1217, -1.0589, -1.0846], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0846, -1.1313, -1.1207, -1.1246, -1.1180])\n",
      "mean: tensor(-1.1158)\n",
      "iter_dt 6392.60ms; iter 77: train loss 0.00970 temperature: 8.850000000000001\n",
      "mean_logits tensor([-1.0550, -1.0671, -1.0890, -1.1166, -1.1859], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1040, -1.1278, -1.1235, -1.1065, -1.1287])\n",
      "mean: tensor(-1.1181)\n",
      "iter_dt 6393.74ms; iter 78: train loss 0.01977 temperature: 8.900000000000002\n",
      "mean_logits tensor([-1.0401, -1.1703, -1.0589, -1.1069, -1.0904], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0694, -1.0556, -1.1278, -1.0909, -1.1141])\n",
      "mean: tensor(-1.0916)\n",
      "iter_dt 6428.96ms; iter 79: train loss 0.03575 temperature: 8.950000000000003\n",
      "mean_logits tensor([-1.1677, -1.0701, -1.0832, -1.2295, -1.1824], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0796, -1.1305, -1.1203, -1.1225, -1.1172])\n",
      "mean: tensor(-1.1140)\n",
      "iter_dt 6398.27ms; iter 80: train loss 0.05633 temperature: 9.000000000000004\n",
      "mean_logits tensor([-1.1616, -1.1239, -1.1471, -1.0964, -1.1287], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1167, -1.1316, -1.1125, -1.1195, -0.9011])\n",
      "mean: tensor(-1.0763)\n",
      "iter_dt 6388.84ms; iter 81: train loss 0.08650 temperature: 9.050000000000004\n",
      "mean_logits tensor([-1.0725, -1.1819, -1.2120, -1.1031, -1.1708], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0751, -1.1027, -1.1048, -1.1283, -1.1276])\n",
      "mean: tensor(-1.1077)\n",
      "iter_dt 6390.90ms; iter 82: train loss 0.04056 temperature: 9.100000000000005\n",
      "mean_logits tensor([-1.1857, -1.0342, -1.1788, -1.1568, -1.0640], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1027, -1.1009, -1.1166, -1.0825, -1.0578])\n",
      "mean: tensor(-1.0921)\n",
      "iter_dt 6383.11ms; iter 83: train loss 0.03929 temperature: 9.150000000000006\n",
      "mean_logits tensor([-1.2263, -1.1681, -1.1828, -1.1188, -1.0830], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1210, -1.0962, -1.0915, -1.1214, -1.1216])\n",
      "mean: tensor(-1.1103)\n",
      "iter_dt 6393.40ms; iter 84: train loss 0.05208 temperature: 9.200000000000006\n",
      "mean_logits tensor([-1.1705, -1.2022, -1.1123, -1.1667, -1.1854], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0848, -1.0493, -1.1189, -1.1324, -0.9038])\n",
      "mean: tensor(-1.0578)\n",
      "iter_dt 6395.64ms; iter 85: train loss 0.18986 temperature: 9.250000000000007\n",
      "mean_logits tensor([-1.0893, -1.1447, -1.1819, -1.1079, -0.9920], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1040, -1.1263, -1.1226, -1.1258, -1.1046])\n",
      "mean: tensor(-1.1167)\n",
      "iter_dt 6400.34ms; iter 86: train loss 0.02936 temperature: 9.300000000000008\n",
      "mean_logits tensor([-1.1030, -1.0450, -1.1440, -1.1962, -1.1757], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1221, -1.0921, -1.1314, -1.0683, -1.0872])\n",
      "mean: tensor(-1.1002)\n",
      "iter_dt 6386.46ms; iter 87: train loss 0.05138 temperature: 9.350000000000009\n",
      "mean_logits tensor([-1.1925, -1.1146, -1.1990, -1.1946, -1.1096], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0971, -1.0980, -1.1030, -1.0809, -1.1217])\n",
      "mean: tensor(-1.1002)\n",
      "iter_dt 6392.28ms; iter 88: train loss 0.06238 temperature: 9.40000000000001\n",
      "mean_logits tensor([-1.1688, -1.0760, -1.2270, -1.0824, -1.1500], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0844, -1.1307, -1.1231, -1.1128, -1.1261])\n",
      "mean: tensor(-1.1154)\n",
      "iter_dt 6389.00ms; iter 89: train loss 0.04443 temperature: 9.45000000000001\n",
      "mean_logits tensor([-1.0871, -1.0644, -1.0201, -1.1115, -1.0613], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0977, -1.1196, -1.1324, -1.1064, -1.0924])\n",
      "mean: tensor(-1.1097)\n",
      "iter_dt 6391.68ms; iter 90: train loss 0.02904 temperature: 9.50000000000001\n",
      "mean_logits tensor([-1.1488, -1.0272, -1.0777, -1.0726, -1.1153], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1299, -1.0783, -1.1121, -1.1227, -1.1331])\n",
      "mean: tensor(-1.1152)\n",
      "iter_dt 6397.71ms; iter 91: train loss 0.01222 temperature: 9.550000000000011\n",
      "mean_logits tensor([-1.1119, -1.0468, -1.0536, -1.1386, -1.1541], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0854, -1.1228, -1.0806, -1.1323, -1.0912])\n",
      "mean: tensor(-1.1024)\n",
      "iter_dt 6393.58ms; iter 92: train loss 0.02016 temperature: 9.600000000000012\n",
      "mean_logits tensor([-1.0507, -1.0485, -1.1163, -1.0374, -0.9913], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1100, -1.1296, -1.0916, -1.1094, -1.1320])\n",
      "mean: tensor(-1.1145)\n",
      "iter_dt 6398.32ms; iter 93: train loss 0.06087 temperature: 9.650000000000013\n",
      "mean_logits tensor([-1.0197, -0.9962, -1.1517, -1.1397, -1.0430], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1302, -1.1307, -1.1203, -1.1179, -1.1131])\n",
      "mean: tensor(-1.1224)\n",
      "iter_dt 6388.33ms; iter 94: train loss 0.06266 temperature: 9.700000000000014\n",
      "mean_logits tensor([-1.1032, -1.0370, -1.1325, -1.1575, -1.1383], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0963, -1.1048, -1.0913, -1.1277, -1.1219])\n",
      "mean: tensor(-1.1084)\n",
      "iter_dt 6399.27ms; iter 95: train loss 0.01333 temperature: 9.750000000000014\n",
      "mean_logits tensor([-1.1140, -1.1704, -1.0405, -1.0471, -1.0979], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1046, -1.1291, -1.1253, -1.1100, -1.1190])\n",
      "mean: tensor(-1.1176)\n",
      "iter_dt 6390.32ms; iter 96: train loss 0.02379 temperature: 9.800000000000015\n",
      "mean_logits tensor([-1.1983, -1.1446, -1.1462, -1.1844, -1.1353], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9220, -1.1325, -1.1119, -1.0171, -1.0721])\n",
      "mean: tensor(-1.0511)\n",
      "iter_dt 6398.45ms; iter 97: train loss 0.18863 temperature: 9.850000000000016\n",
      "mean_logits tensor([-1.1924, -1.2107, -1.1368, -1.1871, -1.1220], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1320, -1.1201, -1.1306, -1.1328, -1.1313])\n",
      "mean: tensor(-1.1294)\n",
      "iter_dt 6390.21ms; iter 98: train loss 0.03059 temperature: 9.900000000000016\n",
      "mean_logits tensor([-1.1663, -1.2256, -1.0149, -1.1531, -1.1083], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1230, -1.1225, -1.0825, -1.1059, -1.1079])\n",
      "mean: tensor(-1.1083)\n",
      "iter_dt 6409.87ms; iter 99: train loss 0.03772 temperature: 9.950000000000017\n",
      "converged SCF energy = -1.09191404102006\n",
      "Starting to parse FermionOperator using 4 qubits...\n",
      "\n",
      "Operator t:  -0.2590541222133725 [] +\n",
      "-0.04764292343981624 [X0 X1 Y2 Y3] +\n",
      "0.04764292343981624 [X0 Y1 Y2 X3] +\n",
      "0.04764292343981624 [Y0 X1 X2 Y3] +\n",
      "-0.04764292343981624 [Y0 Y1 X2 X3] +\n",
      "0.14907478844731514 [Z0] +\n",
      "0.16113816378164877 [Z0 Z1] +\n",
      "0.11162723403394152 [Z0 Z2] +\n",
      "0.15927015747375778 [Z0 Z3] +\n",
      "0.1490747884473151 [Z1] +\n",
      "0.15927015747375778 [Z1 Z2] +\n",
      "0.11162723403394152 [Z1 Z3] +\n",
      "-0.16071249108067326 [Z2] +\n",
      "0.1673712594830412 [Z2 Z3] +\n",
      "-0.16071249108067326 [Z3]\n",
      "Term, coeff:  () -0.2590541222133725\n",
      "Term, coeff:  ((0, 'Z'),) 0.14907478844731514\n",
      "Index, p_char:  0 Z\n",
      "Term, coeff:  ((1, 'Z'),) 0.1490747884473151\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((2, 'Z'),) -0.16071249108067326\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((3, 'Z'),) -0.16071249108067326\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((0, 'Z'), (1, 'Z')) 0.16113816378164877\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((0, 'Y'), (1, 'X'), (2, 'X'), (3, 'Y')) 0.04764292343981624\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'Y'), (1, 'Y'), (2, 'X'), (3, 'X')) -0.04764292343981624\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'X'), (1, 'X'), (2, 'Y'), (3, 'Y')) -0.04764292343981624\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'X'), (1, 'Y'), (2, 'Y'), (3, 'X')) 0.04764292343981624\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'Z'), (2, 'Z')) 0.11162723403394152\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((0, 'Z'), (3, 'Z')) 0.15927015747375778\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((1, 'Z'), (2, 'Z')) 0.15927015747375778\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((1, 'Z'), (3, 'Z')) 0.11162723403394152\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((2, 'Z'), (3, 'Z')) 0.1673712594830412\n",
      "Index, p_char:  2 Z\n",
      "Index, p_char:  3 Z\n",
      "ground state: -1.1205602812999886\n",
      "hf state: -1.0919140410200578\n",
      "number of parameters: 85.29M\n",
      "running on device cpu\n",
      "mean_logits tensor([-1.1676, -0.9894, -1.0338, -1.0321, -1.2728], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1189, -1.0895, -1.0931, -1.1062, -0.9448])\n",
      "mean: tensor(-1.0705)\n",
      "iter_dt 0.00ms; iter 0: train loss 0.23540 temperature: 5\n",
      "mean_logits tensor([-0.9143, -0.9475, -0.9721, -0.9740, -0.8930], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9950, -1.0834, -1.0977, -1.0564, -1.1075])\n",
      "mean: tensor(-1.0680)\n",
      "iter_dt 1694971385772.19ms; iter 1: train loss 0.14062 temperature: 5.05\n",
      "mean_logits tensor([-1.0657, -1.3011, -1.3655, -1.2998, -1.0282], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0953, -1.1099, -1.0745, -0.6746, -1.1023])\n",
      "mean: tensor(-1.0113)\n",
      "iter_dt 6418.44ms; iter 2: train loss 0.86977 temperature: 5.1\n",
      "mean_logits tensor([-1.1466, -1.1590, -1.1697, -1.1215, -1.1427], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9767, -1.0898, -1.0943, -1.1030, -1.0303])\n",
      "mean: tensor(-1.0588)\n",
      "iter_dt 6455.81ms; iter 3: train loss 0.09128 temperature: 5.1499999999999995\n",
      "mean_logits tensor([-0.8528, -1.2905, -1.2269, -1.2241, -1.2487], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0743, -1.1030, -1.0863, -0.9279, -0.9749])\n",
      "mean: tensor(-1.0333)\n",
      "iter_dt 6386.56ms; iter 4: train loss 0.47636 temperature: 5.199999999999999\n",
      "mean_logits tensor([-1.3639, -0.9136, -1.2571, -0.8531, -1.0072], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1171, -1.0707, -1.1112, -1.1125, -1.0930])\n",
      "mean: tensor(-1.1009)\n",
      "iter_dt 6395.26ms; iter 5: train loss 0.33653 temperature: 5.249999999999999\n",
      "mean_logits tensor([-1.1737, -1.0042, -1.0976, -1.0678, -0.9273], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0530, -1.0946, -1.0973, -1.0913, -1.0824])\n",
      "mean: tensor(-1.0837)\n",
      "iter_dt 6411.97ms; iter 6: train loss 0.07729 temperature: 5.299999999999999\n",
      "mean_logits tensor([-1.1136, -1.0339, -1.0204, -1.1039, -0.9489], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0851, -1.0152, -0.9334, -1.0452, -1.0183])\n",
      "mean: tensor(-1.0195)\n",
      "iter_dt 6409.42ms; iter 7: train loss 0.02551 temperature: 5.349999999999999\n",
      "mean_logits tensor([-1.0227, -0.8890, -0.8942, -1.0294, -1.0563], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0819, -1.0903, -1.1069, -1.0198, -1.0931])\n",
      "mean: tensor(-1.0784)\n",
      "iter_dt 6438.12ms; iter 8: train loss 0.13423 temperature: 5.399999999999999\n",
      "mean_logits tensor([-0.9948, -0.9998, -0.8609, -0.9776, -0.9683], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0815, -1.0101, -1.0450, -1.0939, -1.0550])\n",
      "mean: tensor(-1.0571)\n",
      "iter_dt 6404.23ms; iter 9: train loss 0.09076 temperature: 5.449999999999998\n",
      "mean_logits tensor([-0.9405, -0.9982, -0.9870, -1.0118, -1.0271], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9536, -1.1179, -1.1117, -1.1138, -1.0418])\n",
      "mean: tensor(-1.0678)\n",
      "iter_dt 6431.44ms; iter 10: train loss 0.06727 temperature: 5.499999999999998\n",
      "mean_logits tensor([-0.8867, -1.1159, -1.1716, -0.9881, -0.9618], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1100, -1.0962, -1.0831, -1.1037, -1.1174])\n",
      "mean: tensor(-1.1021)\n",
      "iter_dt 6392.63ms; iter 11: train loss 0.14987 temperature: 5.549999999999998\n",
      "mean_logits tensor([-0.9753, -0.8105, -1.0601, -0.9823, -0.9622], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9680, -1.0932, -1.0988, -1.0921, -1.1034])\n",
      "mean: tensor(-1.0711)\n",
      "iter_dt 6391.74ms; iter 12: train loss 0.16137 temperature: 5.599999999999998\n",
      "mean_logits tensor([-1.1241, -0.8133, -1.0098, -1.0463, -1.0585], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0886, -1.0540, -1.1134, -1.0855, -1.0596])\n",
      "mean: tensor(-1.0802)\n",
      "iter_dt 6391.71ms; iter 13: train loss 0.09818 temperature: 5.649999999999998\n",
      "mean_logits tensor([-0.9891, -1.1041, -1.0684, -0.9826, -1.1234], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0973, -1.0890, -1.1118, -1.0876, -1.0379])\n",
      "mean: tensor(-1.0847)\n",
      "iter_dt 6394.17ms; iter 14: train loss 0.05282 temperature: 5.6999999999999975\n",
      "mean_logits tensor([-1.1536, -1.1044, -1.1884, -1.3001, -1.1450], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0392, -1.0640, -1.0877, -1.0953, -1.0813])\n",
      "mean: tensor(-1.0735)\n",
      "iter_dt 6433.94ms; iter 15: train loss 0.14606 temperature: 5.749999999999997\n",
      "mean_logits tensor([-1.0951, -1.1050, -1.2362, -1.0410, -1.1280], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0575, -1.0961, -1.0266, -0.9038, -1.1076])\n",
      "mean: tensor(-1.0383)\n",
      "iter_dt 6390.19ms; iter 16: train loss 0.11445 temperature: 5.799999999999997\n",
      "mean_logits tensor([-1.2024, -1.0938, -1.1893, -1.1651, -0.9926], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0952, -1.1126, -1.0809, -1.0244, -1.1021])\n",
      "mean: tensor(-1.0830)\n",
      "iter_dt 6390.45ms; iter 17: train loss 0.10126 temperature: 5.849999999999997\n",
      "mean_logits tensor([-1.2251, -1.2383, -1.1041, -1.2542, -1.2959], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8058, -1.0699, -1.1045, -1.1133, -1.0630])\n",
      "mean: tensor(-1.0313)\n",
      "iter_dt 6420.68ms; iter 18: train loss 0.48677 temperature: 5.899999999999997\n",
      "mean_logits tensor([-1.1790, -1.3229, -1.1143, -1.2690, -1.3112], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0314, -1.0957, -1.1032, -1.1079, -1.0280])\n",
      "mean: tensor(-1.0732)\n",
      "iter_dt 6403.39ms; iter 19: train loss 0.38009 temperature: 5.949999999999997\n",
      "mean_logits tensor([-1.3087, -1.2495, -1.2621, -1.1734, -1.3228], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0461, -1.0465, -1.1189, -1.1022, -1.1066])\n",
      "mean: tensor(-1.0841)\n",
      "iter_dt 6421.36ms; iter 20: train loss 0.38917 temperature: 5.9999999999999964\n",
      "mean_logits tensor([-1.1365, -1.1568, -1.1163, -1.1347, -1.2751], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0592, -1.0344, -1.0568, -1.0633, -1.0718])\n",
      "mean: tensor(-1.0571)\n",
      "iter_dt 6389.61ms; iter 21: train loss 0.13965 temperature: 6.049999999999996\n",
      "mean_logits tensor([-1.0869, -1.2186, -1.1488, -1.3382, -1.2585], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0817, -1.0552, -1.1166, -1.0698, -1.0786])\n",
      "mean: tensor(-1.0804)\n",
      "iter_dt 6408.17ms; iter 22: train loss 0.28227 temperature: 6.099999999999996\n",
      "mean_logits tensor([-1.1624, -1.2020, -1.0508, -1.1261, -1.0101], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1122, -1.0957, -1.0392, -0.9720, -1.0116])\n",
      "mean: tensor(-1.0461)\n",
      "iter_dt 6412.31ms; iter 23: train loss 0.06645 temperature: 6.149999999999996\n",
      "mean_logits tensor([-1.0420, -0.9715, -1.0739, -1.0415, -1.1668], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0025, -1.0827, -1.1021, -1.0861, -1.0983])\n",
      "mean: tensor(-1.0744)\n",
      "iter_dt 6413.66ms; iter 24: train loss 0.03551 temperature: 6.199999999999996\n",
      "mean_logits tensor([-1.1758, -1.0170, -1.0536, -1.0617, -1.1315], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0676, -1.0525, -1.1178, -1.1046, -1.0923])\n",
      "mean: tensor(-1.0870)\n",
      "iter_dt 6415.48ms; iter 25: train loss 0.03739 temperature: 6.249999999999996\n",
      "mean_logits tensor([-1.1549, -0.8395, -1.0931, -1.0657, -1.0533], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0709, -1.0896, -1.1140, -1.0625, -1.0051])\n",
      "mean: tensor(-1.0684)\n",
      "iter_dt 6392.16ms; iter 26: train loss 0.10411 temperature: 6.299999999999995\n",
      "mean_logits tensor([-1.1504, -0.9701, -1.2369, -1.1521, -1.1202], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0591, -1.1100, -1.0963, -1.0192, -1.0629])\n",
      "mean: tensor(-1.0695)\n",
      "iter_dt 6477.13ms; iter 27: train loss 0.12433 temperature: 6.349999999999995\n",
      "mean_logits tensor([-1.0025, -1.0939, -1.0013, -1.0323, -1.0741], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0858, -1.0800, -1.1149, -1.1116, -1.0923])\n",
      "mean: tensor(-1.0969)\n",
      "iter_dt 6424.87ms; iter 28: train loss 0.04435 temperature: 6.399999999999995\n",
      "mean_logits tensor([-1.0917, -0.9736, -0.9989, -1.0793, -0.9643], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9492, -1.0940, -1.1081, -1.1062, -1.0527])\n",
      "mean: tensor(-1.0621)\n",
      "iter_dt 6397.43ms; iter 29: train loss 0.08698 temperature: 6.449999999999995\n",
      "mean_logits tensor([-1.1365, -1.0568, -0.8804, -1.0681, -1.1036], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1040, -1.0983, -1.0563, -1.1017, -1.0449])\n",
      "mean: tensor(-1.0810)\n",
      "iter_dt 6444.18ms; iter 30: train loss 0.05590 temperature: 6.499999999999995\n",
      "mean_logits tensor([-1.0068, -0.9781, -1.0390, -1.0583, -1.0572], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9309, -0.8061, -1.0893, -1.1062, -1.1080])\n",
      "mean: tensor(-1.0081)\n",
      "iter_dt 6430.51ms; iter 31: train loss 0.05608 temperature: 6.5499999999999945\n",
      "mean_logits tensor([-1.0268, -0.9782, -1.0852, -1.0242, -1.1583], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0914, -1.1103, -1.0979, -1.0961, -1.0655])\n",
      "mean: tensor(-1.0922)\n",
      "iter_dt 6567.05ms; iter 32: train loss 0.06000 temperature: 6.599999999999994\n",
      "mean_logits tensor([-0.9881, -1.1357, -1.1017, -1.0106, -1.0247], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0732, -1.1183, -1.1043, -1.1170, -1.0661])\n",
      "mean: tensor(-1.0958)\n",
      "iter_dt 6482.38ms; iter 33: train loss 0.03376 temperature: 6.649999999999994\n",
      "mean_logits tensor([-1.1204, -1.0010, -1.0892, -1.0492, -1.0442], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1077, -1.1085, -1.0729, -1.0690, -1.0994])\n",
      "mean: tensor(-1.0915)\n",
      "iter_dt 6391.72ms; iter 34: train loss 0.02570 temperature: 6.699999999999994\n",
      "mean_logits tensor([-1.0904, -0.9981, -1.1894, -1.2353, -1.1855], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0647, -1.0851, -1.0363, -1.1020, -1.0513])\n",
      "mean: tensor(-1.0679)\n",
      "iter_dt 6388.43ms; iter 35: train loss 0.12746 temperature: 6.749999999999994\n",
      "mean_logits tensor([-1.0468, -1.0375, -1.0688, -1.1656, -1.2084], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0910, -1.0478, -0.9576, -1.0884, -1.1193])\n",
      "mean: tensor(-1.0608)\n",
      "iter_dt 6390.47ms; iter 36: train loss 0.04992 temperature: 6.799999999999994\n",
      "mean_logits tensor([-1.1474, -1.0417, -1.0430, -1.0725, -1.2357], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0752, -1.1190, -1.1016, -1.0964, -1.0635])\n",
      "mean: tensor(-1.0912)\n",
      "iter_dt 6391.94ms; iter 37: train loss 0.08609 temperature: 6.849999999999993\n",
      "mean_logits tensor([-1.1350, -1.1705, -1.1197, -1.1970, -1.0571], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9120, -1.0943, -1.1075, -1.0408, -1.0886])\n",
      "mean: tensor(-1.0486)\n",
      "iter_dt 6398.15ms; iter 38: train loss 0.13632 temperature: 6.899999999999993\n",
      "mean_logits tensor([-0.9754, -1.0753, -1.0464, -1.1427, -1.2057], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0435, -1.0877, -1.1178, -1.0279, -0.9609])\n",
      "mean: tensor(-1.0476)\n",
      "iter_dt 6391.56ms; iter 39: train loss 0.14440 temperature: 6.949999999999993\n",
      "mean_logits tensor([-1.0979, -1.0841, -1.1325, -1.0548, -1.1371], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0960, -1.0889, -1.0783, -1.0800, -1.0800])\n",
      "mean: tensor(-1.0847)\n",
      "iter_dt 6388.56ms; iter 40: train loss 0.01247 temperature: 6.999999999999993\n",
      "mean_logits tensor([-1.0244, -1.0335, -1.0740, -1.0792, -1.1000], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1134, -1.1083, -1.1016, -1.0705, -1.0884])\n",
      "mean: tensor(-1.0964)\n",
      "iter_dt 6384.41ms; iter 41: train loss 0.02467 temperature: 7.049999999999993\n",
      "mean_logits tensor([-1.0634, -1.1082, -1.0491, -1.0456, -1.0834], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0882, -1.1091, -1.1170, -1.0743, -0.9680])\n",
      "mean: tensor(-1.0713)\n",
      "iter_dt 6417.16ms; iter 42: train loss 0.03120 temperature: 7.0999999999999925\n",
      "mean_logits tensor([-1.1187, -1.1725, -1.0832, -1.0873, -0.9117], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1078, -0.9873, -0.9730, -1.0794, -1.0390])\n",
      "mean: tensor(-1.0373)\n",
      "iter_dt 6398.60ms; iter 43: train loss 0.10182 temperature: 7.149999999999992\n",
      "mean_logits tensor([-1.2469, -0.9821, -1.0786, -1.1826, -1.1065], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0985, -1.0899, -1.0647, -1.0839, -1.1050])\n",
      "mean: tensor(-1.0884)\n",
      "iter_dt 6406.10ms; iter 44: train loss 0.08363 temperature: 7.199999999999992\n",
      "mean_logits tensor([-1.1576, -0.9751, -1.1800, -1.1052, -1.0710], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0822, -1.1124, -1.1150, -1.0728, -1.0972])\n",
      "mean: tensor(-1.0959)\n",
      "iter_dt 6384.63ms; iter 45: train loss 0.05254 temperature: 7.249999999999992\n",
      "mean_logits tensor([-0.9152, -1.1060, -1.0625, -0.9819, -1.1326], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0754, -1.0766, -1.0359, -1.1080, -1.0795])\n",
      "mean: tensor(-1.0751)\n",
      "iter_dt 6394.23ms; iter 46: train loss 0.07120 temperature: 7.299999999999992\n",
      "mean_logits tensor([-0.9795, -0.9365, -1.1023, -1.0056, -1.0432], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0954, -1.0776, -1.1177, -1.0368, -1.1091])\n",
      "mean: tensor(-1.0873)\n",
      "iter_dt 6413.04ms; iter 47: train loss 0.06072 temperature: 7.349999999999992\n",
      "mean_logits tensor([-1.0952, -1.1251, -1.0865, -1.1569, -1.0831], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0321, -1.0745, -0.8965, -1.0816, -1.0316])\n",
      "mean: tensor(-1.0233)\n",
      "iter_dt 6418.26ms; iter 48: train loss 0.07892 temperature: 7.3999999999999915\n",
      "mean_logits tensor([-1.1009, -1.0734, -1.1016, -1.0895, -1.0696], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0546, -1.0651, -0.7898, -1.1045, -1.0816])\n",
      "mean: tensor(-1.0191)\n",
      "iter_dt 6391.39ms; iter 49: train loss 0.13437 temperature: 7.449999999999991\n",
      "mean_logits tensor([-0.9949, -1.1604, -1.1497, -1.0144, -1.0807], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0638, -0.7908, -1.0748, -1.0569, -1.0093])\n",
      "mean: tensor(-0.9991)\n",
      "iter_dt 6387.44ms; iter 50: train loss 0.22339 temperature: 7.499999999999991\n",
      "mean_logits tensor([-1.0366, -1.0564, -1.1554, -1.2023, -1.0022], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1074, -0.7877, -1.0334, -1.1059, -1.1075])\n",
      "mean: tensor(-1.0284)\n",
      "iter_dt 6408.83ms; iter 51: train loss 0.16409 temperature: 7.549999999999991\n",
      "mean_logits tensor([-1.0235, -1.2626, -0.9993, -1.0024, -1.1267], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0454, -1.1056, -1.1073, -1.0849, -1.0532])\n",
      "mean: tensor(-1.0793)\n",
      "iter_dt 6392.75ms; iter 52: train loss 0.09324 temperature: 7.599999999999991\n",
      "mean_logits tensor([-1.1037, -1.0956, -1.1558, -1.0035, -1.1052], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0727, -1.0826, -1.0417, -0.9060, -1.0636])\n",
      "mean: tensor(-1.0333)\n",
      "iter_dt 6416.96ms; iter 53: train loss 0.04135 temperature: 7.649999999999991\n",
      "mean_logits tensor([-1.1940, -0.9887, -0.9602, -0.9901, -0.9031], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0400, -1.0816, -1.1110, -1.0741, -1.0972])\n",
      "mean: tensor(-1.0808)\n",
      "iter_dt 6387.45ms; iter 54: train loss 0.16119 temperature: 7.69999999999999\n",
      "mean_logits tensor([-1.1299, -1.1461, -0.9058, -1.0730, -1.2212], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0849, -1.0371, -1.0562, -1.1024, -1.1107])\n",
      "mean: tensor(-1.0783)\n",
      "iter_dt 6393.41ms; iter 55: train loss 0.08379 temperature: 7.74999999999999\n",
      "mean_logits tensor([-1.0857, -0.9967, -1.0737, -1.0922, -0.9657], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0889, -1.0543, -1.1020, -1.0950, -1.0544])\n",
      "mean: tensor(-1.0789)\n",
      "iter_dt 6393.23ms; iter 56: train loss 0.01846 temperature: 7.79999999999999\n",
      "mean_logits tensor([-1.1678, -0.8386, -1.0722, -1.0386, -1.0441], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1157, -1.0948, -1.0829, -0.9216, -1.0756])\n",
      "mean: tensor(-1.0581)\n",
      "iter_dt 6393.07ms; iter 57: train loss 0.11783 temperature: 7.84999999999999\n",
      "mean_logits tensor([-1.0637, -1.0432, -1.1167, -0.9683, -1.0166], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0109, -1.1063, -1.0637, -1.0979, -1.1155])\n",
      "mean: tensor(-1.0789)\n",
      "iter_dt 6394.97ms; iter 58: train loss 0.05934 temperature: 7.89999999999999\n",
      "mean_logits tensor([-1.0934, -1.1533, -1.0174, -0.9930, -1.0564], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0282, -1.1078, -1.0888, -1.1118, -1.0626])\n",
      "mean: tensor(-1.0798)\n",
      "iter_dt 6382.34ms; iter 59: train loss 0.04271 temperature: 7.9499999999999895\n",
      "mean_logits tensor([-1.1033, -1.1192, -1.0016, -1.0634, -1.1399], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0102, -1.0225, -1.0746, -1.0909, -1.0931])\n",
      "mean: tensor(-1.0583)\n",
      "iter_dt 6389.03ms; iter 60: train loss 0.04418 temperature: 7.999999999999989\n",
      "mean_logits tensor([-0.9609, -0.9601, -0.9744, -1.1771, -1.0000], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1163, -1.1085, -1.0990, -1.1036, -1.1064])\n",
      "mean: tensor(-1.1068)\n",
      "iter_dt 6393.16ms; iter 61: train loss 0.12749 temperature: 8.04999999999999\n",
      "mean_logits tensor([-1.1453, -1.0394, -1.0475, -1.0991, -1.0465], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0927, -1.1023, -1.0684, -1.1097, -1.0495])\n",
      "mean: tensor(-1.0845)\n",
      "iter_dt 6418.97ms; iter 62: train loss 0.01287 temperature: 8.09999999999999\n",
      "mean_logits tensor([-1.0638, -1.1646, -1.1263, -1.1272, -1.0752], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0978, -1.0979, -1.0996, -1.1107, -0.9652])\n",
      "mean: tensor(-1.0743)\n",
      "iter_dt 6391.45ms; iter 63: train loss 0.03102 temperature: 8.149999999999991\n",
      "mean_logits tensor([-1.0672, -1.0490, -1.1607, -1.0876, -1.0419], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1019, -1.0943, -1.1039, -1.1001, -1.0422])\n",
      "mean: tensor(-1.0885)\n",
      "iter_dt 6388.33ms; iter 64: train loss 0.01210 temperature: 8.199999999999992\n",
      "mean_logits tensor([-1.1327, -1.1296, -0.9807, -1.0273, -1.0722], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0849, -1.1078, -1.1136, -1.0523, -1.1038])\n",
      "mean: tensor(-1.0925)\n",
      "iter_dt 6387.43ms; iter 65: train loss 0.03656 temperature: 8.249999999999993\n",
      "mean_logits tensor([-1.2179, -1.1559, -1.1082, -1.1102, -1.1906], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1171, -1.1194, -1.1168, -0.9289, -1.1196])\n",
      "mean: tensor(-1.0804)\n",
      "iter_dt 6402.07ms; iter 66: train loss 0.08461 temperature: 8.299999999999994\n",
      "mean_logits tensor([-1.2139, -1.0967, -1.0160, -1.1177, -1.1363], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1119, -1.1182, -1.0943, -1.1069, -1.0695])\n",
      "mean: tensor(-1.1002)\n",
      "iter_dt 6438.03ms; iter 67: train loss 0.04059 temperature: 8.349999999999994\n",
      "mean_logits tensor([-1.1249, -1.1404, -1.1082, -1.0545, -1.1775], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0890, -1.1125, -1.1025, -1.1091, -1.1157])\n",
      "mean: tensor(-1.1058)\n",
      "iter_dt 6428.67ms; iter 68: train loss 0.01666 temperature: 8.399999999999995\n",
      "mean_logits tensor([-1.1286, -1.0999, -0.9312, -1.2254, -1.2358], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0786, -1.0848, -1.0816, -1.0912, -1.1038])\n",
      "mean: tensor(-1.0880)\n",
      "iter_dt 6396.33ms; iter 69: train loss 0.11172 temperature: 8.449999999999996\n",
      "mean_logits tensor([-1.1502, -1.1314, -0.9664, -1.0779, -1.0280], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0841, -1.0875, -1.1072, -1.0796, -1.1082])\n",
      "mean: tensor(-1.0933)\n",
      "iter_dt 6379.25ms; iter 70: train loss 0.05418 temperature: 8.499999999999996\n",
      "mean_logits tensor([-1.0953, -1.1111, -1.1158, -1.1135, -1.0315], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1158, -1.0935, -1.0618, -1.1059, -1.0078])\n",
      "mean: tensor(-1.0770)\n",
      "iter_dt 6393.77ms; iter 71: train loss 0.00744 temperature: 8.549999999999997\n",
      "mean_logits tensor([-1.0013, -1.0317, -1.0923, -1.0344, -1.0704], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1063, -1.1027, -1.0895, -1.0942, -1.1136])\n",
      "mean: tensor(-1.1013)\n",
      "iter_dt 6389.58ms; iter 72: train loss 0.03603 temperature: 8.599999999999998\n",
      "mean_logits tensor([-1.0744, -1.0900, -1.0900, -1.0147, -1.0962], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1089, -1.0682, -1.1088, -1.0362, -0.9468])\n",
      "mean: tensor(-1.0538)\n",
      "iter_dt 6389.84ms; iter 73: train loss 0.03879 temperature: 8.649999999999999\n",
      "mean_logits tensor([-1.0881, -1.0317, -1.0620, -1.1251, -1.1234], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0803, -1.1088, -1.0571, -1.0962, -1.1164])\n",
      "mean: tensor(-1.0918)\n",
      "iter_dt 6383.60ms; iter 74: train loss 0.01190 temperature: 8.7\n",
      "mean_logits tensor([-1.0800, -1.1327, -1.0499, -1.0264, -1.1089], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1189, -1.0982, -1.0137, -1.0926, -1.1035])\n",
      "mean: tensor(-1.0854)\n",
      "iter_dt 6396.82ms; iter 75: train loss 0.01437 temperature: 8.75\n",
      "mean_logits tensor([-1.1092, -1.0924, -1.0260, -1.0588, -1.0081], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1009, -1.1175, -1.1143, -1.1100, -1.0564])\n",
      "mean: tensor(-1.0998)\n",
      "iter_dt 6399.32ms; iter 76: train loss 0.02283 temperature: 8.8\n",
      "mean_logits tensor([-1.1723, -1.0662, -1.0139, -1.0900, -1.0520], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1168, -1.1134, -1.1120, -1.0739, -1.0913])\n",
      "mean: tensor(-1.1015)\n",
      "iter_dt 6412.64ms; iter 77: train loss 0.02928 temperature: 8.850000000000001\n",
      "mean_logits tensor([-1.1840, -1.1862, -1.1128, -1.1310, -1.1041], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0953, -1.0916, -1.0924, -1.0759, -1.1027])\n",
      "mean: tensor(-1.0916)\n",
      "iter_dt 6387.14ms; iter 78: train loss 0.03912 temperature: 8.900000000000002\n",
      "mean_logits tensor([-1.1355, -1.0770, -1.0471, -1.2084, -1.1623], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1044, -1.0810, -1.1170, -1.0835, -1.0711])\n",
      "mean: tensor(-1.0914)\n",
      "iter_dt 6392.77ms; iter 79: train loss 0.05676 temperature: 8.950000000000003\n",
      "mean_logits tensor([-1.1451, -1.1172, -1.0182, -1.1421, -1.1081], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1133, -1.0289, -1.1069, -1.0389, -1.0744])\n",
      "mean: tensor(-1.0725)\n",
      "iter_dt 6393.32ms; iter 80: train loss 0.04930 temperature: 9.000000000000004\n",
      "mean_logits tensor([-0.9954, -1.1150, -0.9674, -1.0749, -1.1456], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0948, -1.1124, -1.0771, -1.0939, -1.1034])\n",
      "mean: tensor(-1.0963)\n",
      "iter_dt 6389.74ms; iter 81: train loss 0.03863 temperature: 9.050000000000004\n",
      "mean_logits tensor([-1.1538, -1.0978, -1.1101, -1.1190, -1.0461], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0969, -1.1130, -1.0358, -1.0720, -1.1091])\n",
      "mean: tensor(-1.0854)\n",
      "iter_dt 6388.96ms; iter 82: train loss 0.02682 temperature: 9.100000000000005\n",
      "mean_logits tensor([-1.0533, -1.0931, -1.1921, -1.1125, -1.0850], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1047, -1.0839, -1.0560, -1.1171, -1.0878])\n",
      "mean: tensor(-1.0899)\n",
      "iter_dt 6388.12ms; iter 83: train loss 0.03997 temperature: 9.150000000000006\n",
      "mean_logits tensor([-1.0571, -1.1710, -1.1082, -1.1336, -1.0705], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1155, -1.1134, -1.1118, -1.0899, -1.0894])\n",
      "mean: tensor(-1.1040)\n",
      "iter_dt 6387.93ms; iter 84: train loss 0.01670 temperature: 9.200000000000006\n",
      "mean_logits tensor([-1.0667, -1.0863, -0.9157, -1.1898, -1.0734], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1145, -1.0830, -1.0876, -1.1100, -1.0901])\n",
      "mean: tensor(-1.0971)\n",
      "iter_dt 6392.59ms; iter 85: train loss 0.06114 temperature: 9.250000000000007\n",
      "mean_logits tensor([-1.0726, -0.9878, -1.0779, -0.9999, -1.0414], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1158, -1.0746, -1.0871, -1.0751, -1.0976])\n",
      "mean: tensor(-1.0900)\n",
      "iter_dt 6392.51ms; iter 86: train loss 0.02971 temperature: 9.300000000000008\n",
      "mean_logits tensor([-1.1191, -1.1067, -1.0890, -1.1480, -0.9669], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1010, -1.1089, -1.0556, -1.1105, -1.1169])\n",
      "mean: tensor(-1.0986)\n",
      "iter_dt 6389.36ms; iter 87: train loss 0.04140 temperature: 9.350000000000009\n",
      "mean_logits tensor([-1.0857, -1.1335, -1.0242, -1.1175, -1.1526], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0936, -1.0593, -1.0892, -1.0650, -1.1129])\n",
      "mean: tensor(-1.0840)\n",
      "iter_dt 6488.42ms; iter 88: train loss 0.02490 temperature: 9.40000000000001\n",
      "mean_logits tensor([-1.0860, -1.1288, -1.0542, -1.1255, -1.0850], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1010, -1.0180, -1.0833, -1.1033, -1.0975])\n",
      "mean: tensor(-1.0806)\n",
      "iter_dt 6502.36ms; iter 89: train loss 0.02410 temperature: 9.45000000000001\n",
      "mean_logits tensor([-1.0143, -1.0462, -1.0369, -1.1147, -1.1584], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1063, -1.1152, -1.0724, -1.0828, -1.0876])\n",
      "mean: tensor(-1.0929)\n",
      "iter_dt 6485.49ms; iter 90: train loss 0.03577 temperature: 9.50000000000001\n",
      "mean_logits tensor([-1.1176, -1.0613, -1.0622, -1.0338, -1.0837], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1117, -1.1136, -1.1012, -1.0438, -1.1133])\n",
      "mean: tensor(-1.0967)\n",
      "iter_dt 6486.33ms; iter 91: train loss 0.00926 temperature: 9.550000000000011\n",
      "mean_logits tensor([-1.1147, -1.0819, -1.0224, -1.1248, -1.0896], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1085, -1.0741, -1.0625, -1.1072, -1.1014])\n",
      "mean: tensor(-1.0907)\n",
      "iter_dt 6463.77ms; iter 92: train loss 0.00359 temperature: 9.600000000000012\n",
      "mean_logits tensor([-1.0386, -1.1515, -1.1560, -1.1197, -1.0667], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0725, -1.1045, -1.0890, -1.0894, -1.1154])\n",
      "mean: tensor(-1.0942)\n",
      "iter_dt 6508.71ms; iter 93: train loss 0.02047 temperature: 9.650000000000013\n",
      "mean_logits tensor([-1.1148, -1.1355, -1.1064, -1.0856, -1.0904], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1122, -0.9854, -1.0999, -1.0303, -1.0721])\n",
      "mean: tensor(-1.0600)\n",
      "iter_dt 6531.81ms; iter 94: train loss 0.04340 temperature: 9.700000000000014\n",
      "mean_logits tensor([-1.1450, -0.9909, -1.1305, -1.0823, -1.0781], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1149, -1.0999, -1.0495, -1.0984, -1.0868])\n",
      "mean: tensor(-1.0899)\n",
      "iter_dt 6494.77ms; iter 95: train loss 0.03319 temperature: 9.750000000000014\n",
      "mean_logits tensor([-1.0872, -1.0481, -1.0852, -1.1205, -1.0978], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0704, -1.1181, -1.0218, -1.1033, -1.0436])\n",
      "mean: tensor(-1.0715)\n",
      "iter_dt 6540.58ms; iter 96: train loss 0.02119 temperature: 9.800000000000015\n",
      "mean_logits tensor([-1.0139, -1.0465, -1.0089, -1.0431, -1.1036], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1126, -1.0802, -1.0911, -1.0739, -1.1070])\n",
      "mean: tensor(-1.0929)\n",
      "iter_dt 6549.06ms; iter 97: train loss 0.03088 temperature: 9.850000000000016\n",
      "mean_logits tensor([-1.1674, -1.0801, -1.0576, -1.0374, -1.0957], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1039, -1.0642, -1.0859, -1.0368, -1.0297])\n",
      "mean: tensor(-1.0641)\n",
      "iter_dt 6552.38ms; iter 98: train loss 0.01690 temperature: 9.900000000000016\n",
      "mean_logits tensor([-1.1405, -1.0071, -1.1042, -0.9893, -1.0972], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0682, -0.9935, -1.1119, -1.0810, -1.0853])\n",
      "mean: tensor(-1.0680)\n",
      "iter_dt 6475.92ms; iter 99: train loss 0.02349 temperature: 9.950000000000017\n",
      "converged SCF energy = -1.06610864931794\n",
      "Starting to parse FermionOperator using 4 qubits...\n",
      "\n",
      "Operator t:  -0.32760818967480904 [] +\n",
      "-0.04919764587136756 [X0 X1 Y2 Y3] +\n",
      "0.04919764587136756 [X0 Y1 Y2 X3] +\n",
      "0.04919764587136756 [Y0 X1 X2 Y3] +\n",
      "-0.04919764587136756 [Y0 Y1 X2 X3] +\n",
      "0.13716572937099508 [Z0] +\n",
      "0.1566006248823795 [Z0 Z1] +\n",
      "0.10622904490856076 [Z0 Z2] +\n",
      "0.15542669077992832 [Z0 Z3] +\n",
      "0.13716572937099514 [Z1] +\n",
      "0.15542669077992832 [Z1 Z2] +\n",
      "0.10622904490856076 [Z1 Z3] +\n",
      "-0.13036292057109117 [Z2] +\n",
      "0.1632676867356435 [Z2 Z3] +\n",
      "-0.13036292057109117 [Z3]\n",
      "Term, coeff:  () -0.32760818967480904\n",
      "Term, coeff:  ((0, 'Z'),) 0.13716572937099508\n",
      "Index, p_char:  0 Z\n",
      "Term, coeff:  ((1, 'Z'),) 0.13716572937099514\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((2, 'Z'),) -0.13036292057109117\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((3, 'Z'),) -0.13036292057109117\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((0, 'Z'), (1, 'Z')) 0.1566006248823795\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((0, 'Y'), (1, 'X'), (2, 'X'), (3, 'Y')) 0.04919764587136756\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'Y'), (1, 'Y'), (2, 'X'), (3, 'X')) -0.04919764587136756\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'X'), (1, 'X'), (2, 'Y'), (3, 'Y')) -0.04919764587136756\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'X'), (1, 'Y'), (2, 'Y'), (3, 'X')) 0.04919764587136756\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'Z'), (2, 'Z')) 0.10622904490856076\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((0, 'Z'), (3, 'Z')) 0.15542669077992832\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((1, 'Z'), (2, 'Z')) 0.15542669077992832\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((1, 'Z'), (3, 'Z')) 0.10622904490856076\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((2, 'Z'), (3, 'Z')) 0.1632676867356435\n",
      "Index, p_char:  2 Z\n",
      "Index, p_char:  3 Z\n",
      "ground state: -1.101150330232619\n",
      "hf state: -1.0661086493179368\n",
      "number of parameters: 85.29M\n",
      "running on device cpu\n",
      "mean_logits tensor([-1.1676, -0.9894, -1.0338, -1.0321, -1.2728], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0998, -1.0653, -1.0816, -1.0938, -0.9252])\n",
      "mean: tensor(-1.0531)\n",
      "iter_dt 0.00ms; iter 0: train loss 0.24792 temperature: 5\n",
      "mean_logits tensor([-0.9104, -1.4444, -0.9677, -1.0280, -0.9422], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0078, -1.0945, -1.0731, -1.0167, -1.0473])\n",
      "mean: tensor(-1.0479)\n",
      "iter_dt 1694972039945.53ms; iter 1: train loss 0.35964 temperature: 5.05\n",
      "mean_logits tensor([-0.9972, -0.9897, -1.1477, -1.2339, -1.0142], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0487, -1.0200, -1.0856, -0.7110, -1.0785])\n",
      "mean: tensor(-0.9888)\n",
      "iter_dt 6601.26ms; iter 2: train loss 0.41064 temperature: 5.1\n",
      "mean_logits tensor([-1.0063, -0.9289, -1.0437, -1.0842, -1.0195], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0248, -1.0902, -1.0745, -1.0833, -1.0895])\n",
      "mean: tensor(-1.0725)\n",
      "iter_dt 6575.76ms; iter 3: train loss 0.04945 temperature: 5.1499999999999995\n",
      "mean_logits tensor([-0.8848, -1.1142, -1.0693, -1.1674, -1.1538], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0800, -1.0238, -1.0543, -1.0909, -0.8679])\n",
      "mean: tensor(-1.0234)\n",
      "iter_dt 6584.01ms; iter 4: train loss 0.20417 temperature: 5.199999999999999\n",
      "mean_logits tensor([-1.0590, -0.9346, -1.0639, -0.9772, -0.9450], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0630, -1.0690, -1.0877, -1.0934, -1.0400])\n",
      "mean: tensor(-1.0706)\n",
      "iter_dt 6559.73ms; iter 5: train loss 0.06240 temperature: 5.249999999999999\n",
      "mean_logits tensor([-0.9681, -0.9828, -1.0532, -1.0225, -0.9236], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0660, -1.0903, -1.0539, -0.9611, -1.0837])\n",
      "mean: tensor(-1.0510)\n",
      "iter_dt 6559.62ms; iter 6: train loss 0.07678 temperature: 5.299999999999999\n",
      "mean_logits tensor([-1.1151, -1.1266, -1.0055, -1.0422, -0.9758], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0654, -1.0754, -1.0660, -1.0926, -1.0598])\n",
      "mean: tensor(-1.0719)\n",
      "iter_dt 6530.33ms; iter 7: train loss 0.03002 temperature: 5.349999999999999\n",
      "mean_logits tensor([-1.1565, -1.1866, -0.9697, -1.0637, -1.2689], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0826, -1.0973, -1.0683, -1.0719, -0.9843])\n",
      "mean: tensor(-1.0609)\n",
      "iter_dt 6457.94ms; iter 8: train loss 0.19615 temperature: 5.399999999999999\n",
      "mean_logits tensor([-1.0531, -1.1005, -1.0619, -1.0192, -1.0481], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0280, -1.0032, -0.9834, -1.0992, -1.0461])\n",
      "mean: tensor(-1.0320)\n",
      "iter_dt 6521.42ms; iter 9: train loss 0.03673 temperature: 5.449999999999998\n",
      "mean_logits tensor([-0.9925, -1.1705, -1.1439, -1.0221, -1.0893], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9173, -1.0912, -0.9632, -1.0874, -1.0533])\n",
      "mean: tensor(-1.0225)\n",
      "iter_dt 6597.01ms; iter 10: train loss 0.08277 temperature: 5.499999999999998\n",
      "mean_logits tensor([-1.0938, -1.0901, -1.1069, -0.9582, -1.1744], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9981, -1.0483, -0.7072, -1.0852, -1.0180])\n",
      "mean: tensor(-0.9714)\n",
      "iter_dt 6506.31ms; iter 11: train loss 0.28529 temperature: 5.549999999999998\n",
      "mean_logits tensor([-1.0957, -1.0759, -1.0295, -1.2642, -1.0693], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9913, -1.0801, -1.0842, -1.0817, -1.0847])\n",
      "mean: tensor(-1.0644)\n",
      "iter_dt 6464.22ms; iter 12: train loss 0.09273 temperature: 5.599999999999998\n",
      "mean_logits tensor([-1.0333, -0.8447, -1.0292, -1.0207, -1.0890], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0551, -1.0570, -1.0948, -1.0743, -1.0526])\n",
      "mean: tensor(-1.0668)\n",
      "iter_dt 6445.54ms; iter 13: train loss 0.07550 temperature: 5.649999999999998\n",
      "mean_logits tensor([-0.9957, -1.0041, -1.0626, -0.7784, -1.0114], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0715, -1.0189, -1.0940, -1.0211, -1.0809])\n",
      "mean: tensor(-1.0573)\n",
      "iter_dt 6500.89ms; iter 14: train loss 0.09046 temperature: 5.6999999999999975\n",
      "mean_logits tensor([-1.0715, -1.1535, -0.9794, -1.0594, -1.0742], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0773, -1.0493, -1.0722, -1.0874, -1.1007])\n",
      "mean: tensor(-1.0774)\n",
      "iter_dt 6514.79ms; iter 15: train loss 0.03574 temperature: 5.749999999999997\n",
      "mean_logits tensor([-1.1245, -1.0575, -0.9671, -1.0485, -1.0024], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0325, -1.0980, -1.0661, -1.0130, -0.9881])\n",
      "mean: tensor(-1.0395)\n",
      "iter_dt 6548.90ms; iter 16: train loss 0.03476 temperature: 5.799999999999997\n",
      "mean_logits tensor([-1.0772, -1.1066, -0.9752, -1.1783, -1.0006], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0867, -1.0989, -1.0230, -0.9839, -1.0684])\n",
      "mean: tensor(-1.0522)\n",
      "iter_dt 6476.13ms; iter 17: train loss 0.07679 temperature: 5.849999999999997\n",
      "mean_logits tensor([-1.0620, -1.1903, -1.0554, -1.2782, -1.1228], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0158, -1.0705, -0.9280, -1.0719, -1.0819])\n",
      "mean: tensor(-1.0336)\n",
      "iter_dt 6462.70ms; iter 18: train loss 0.14718 temperature: 5.899999999999997\n",
      "mean_logits tensor([-0.9928, -0.9986, -0.9909, -1.1992, -1.0045], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0349, -1.0830, -1.0058, -1.0905, -1.0705])\n",
      "mean: tensor(-1.0570)\n",
      "iter_dt 6440.20ms; iter 19: train loss 0.04479 temperature: 5.949999999999997\n",
      "mean_logits tensor([-1.1210, -1.1360, -1.1429, -1.0780, -1.0329], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0868, -0.9728, -1.0589, -1.0647, -1.0632])\n",
      "mean: tensor(-1.0493)\n",
      "iter_dt 6443.73ms; iter 20: train loss 0.06068 temperature: 5.9999999999999964\n",
      "mean_logits tensor([-1.0013, -1.1151, -1.0954, -1.0315, -1.0079], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0844, -1.0860, -1.0815, -1.0979, -1.0812])\n",
      "mean: tensor(-1.0862)\n",
      "iter_dt 6435.09ms; iter 21: train loss 0.02909 temperature: 6.049999999999996\n",
      "mean_logits tensor([-0.9477, -1.1439, -1.0710, -1.1159, -1.1908], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0926, -1.0680, -1.0936, -1.0877, -1.0895])\n",
      "mean: tensor(-1.0863)\n",
      "iter_dt 6493.15ms; iter 22: train loss 0.06534 temperature: 6.099999999999996\n",
      "mean_logits tensor([-1.0784, -1.0263, -1.0266, -1.1257, -1.0346], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0950, -0.9493, -1.0617, -0.9973, -1.0808])\n",
      "mean: tensor(-1.0368)\n",
      "iter_dt 6466.68ms; iter 23: train loss 0.04218 temperature: 6.149999999999996\n",
      "mean_logits tensor([-0.9827, -1.0595, -1.0458, -1.0513, -1.0069], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0389, -0.9409, -1.0830, -1.0484, -1.0912])\n",
      "mean: tensor(-1.0405)\n",
      "iter_dt 6522.81ms; iter 24: train loss 0.03950 temperature: 6.199999999999996\n",
      "mean_logits tensor([-1.0665, -0.9332, -0.9852, -1.2876, -1.1056], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0059, -1.0990, -1.0949, -1.0928, -1.0667])\n",
      "mean: tensor(-1.0719)\n",
      "iter_dt 6592.20ms; iter 25: train loss 0.15207 temperature: 6.249999999999996\n",
      "mean_logits tensor([-1.1019, -1.0006, -1.0447, -1.0940, -1.0570], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0586, -1.0281, -1.0935, -0.9528, -1.0885])\n",
      "mean: tensor(-1.0443)\n",
      "iter_dt 6523.88ms; iter 26: train loss 0.04108 temperature: 6.299999999999995\n",
      "mean_logits tensor([-1.1380, -0.8848, -1.1189, -1.1437, -1.1491], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0391, -1.0881, -1.0467, -1.0774, -1.0242])\n",
      "mean: tensor(-1.0551)\n",
      "iter_dt 6523.58ms; iter 27: train loss 0.12159 temperature: 6.349999999999995\n",
      "mean_logits tensor([-0.9628, -0.9933, -1.1439, -0.9902, -1.1497], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0826, -1.0682, -1.0929, -1.0715, -1.0782])\n",
      "mean: tensor(-1.0787)\n",
      "iter_dt 6608.91ms; iter 28: train loss 0.05581 temperature: 6.399999999999995\n",
      "mean_logits tensor([-1.0232, -1.0734, -0.8807, -1.0390, -0.9297], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0718, -1.0556, -1.0926, -1.0588, -1.0921])\n",
      "mean: tensor(-1.0742)\n",
      "iter_dt 6578.48ms; iter 29: train loss 0.10982 temperature: 6.449999999999995\n",
      "mean_logits tensor([-1.1119, -1.0387, -1.0446, -1.0564, -1.1214], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0802, -1.0924, -1.0719, -1.0947, -1.0964])\n",
      "mean: tensor(-1.0871)\n",
      "iter_dt 6511.55ms; iter 30: train loss 0.01157 temperature: 6.499999999999995\n",
      "mean_logits tensor([-1.0854, -0.9152, -1.0016, -1.1233, -1.0443], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0804, -1.0902, -1.0933, -0.9842, -1.0815])\n",
      "mean: tensor(-1.0659)\n",
      "iter_dt 6480.08ms; iter 31: train loss 0.09358 temperature: 6.5499999999999945\n",
      "mean_logits tensor([-1.1080, -1.1103, -1.1183, -1.1161, -1.0553], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0636, -1.0901, -1.0737, -1.0586, -1.0860])\n",
      "mean: tensor(-1.0744)\n",
      "iter_dt 6605.43ms; iter 32: train loss 0.01521 temperature: 6.599999999999994\n",
      "mean_logits tensor([-0.9664, -1.0902, -1.0919, -1.0692, -0.9081], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0726, -1.0877, -1.0993, -1.0409, -1.0629])\n",
      "mean: tensor(-1.0727)\n",
      "iter_dt 6566.69ms; iter 33: train loss 0.05327 temperature: 6.649999999999994\n",
      "mean_logits tensor([-1.0940, -1.0687, -1.1172, -1.0362, -1.0851], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0773, -1.0745, -1.0920, -1.0601, -1.0858])\n",
      "mean: tensor(-1.0779)\n",
      "iter_dt 6591.03ms; iter 34: train loss 0.00263 temperature: 6.699999999999994\n",
      "mean_logits tensor([-1.0743, -1.0593, -1.1629, -1.0871, -1.2272], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0774, -1.0424, -1.0479, -1.0959, -1.0622])\n",
      "mean: tensor(-1.0652)\n",
      "iter_dt 6511.19ms; iter 35: train loss 0.07860 temperature: 6.749999999999994\n",
      "mean_logits tensor([-1.0680, -0.9848, -1.0119, -1.1519, -1.0891], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0562, -1.0970, -1.0982, -1.0052, -1.0946])\n",
      "mean: tensor(-1.0703)\n",
      "iter_dt 6487.99ms; iter 36: train loss 0.07006 temperature: 6.799999999999994\n",
      "mean_logits tensor([-1.0109, -1.0786, -0.9645, -1.0906, -1.1572], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0648, -1.0833, -1.0953, -1.0827, -1.0747])\n",
      "mean: tensor(-1.0802)\n",
      "iter_dt 6720.06ms; iter 37: train loss 0.04435 temperature: 6.849999999999993\n",
      "mean_logits tensor([-1.0817, -1.1868, -1.0710, -1.0778, -1.0454], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0909, -1.0954, -1.0864, -1.0870, -0.9243])\n",
      "mean: tensor(-1.0568)\n",
      "iter_dt 6552.27ms; iter 38: train loss 0.03813 temperature: 6.899999999999993\n",
      "mean_logits tensor([-0.9751, -1.0210, -1.0655, -1.1524, -1.1785], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0966, -1.0637, -1.0853, -1.0850, -1.0282])\n",
      "mean: tensor(-1.0718)\n",
      "iter_dt 6464.21ms; iter 39: train loss 0.07668 temperature: 6.949999999999993\n",
      "mean_logits tensor([-1.0815, -1.1140, -1.1455, -1.0283, -1.0108], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0890, -1.1009, -0.9921, -1.0844, -1.0950])\n",
      "mean: tensor(-1.0723)\n",
      "iter_dt 6514.32ms; iter 40: train loss 0.05729 temperature: 6.999999999999993\n",
      "mean_logits tensor([-1.0903, -1.0877, -1.0988, -1.0273, -1.1838], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0929, -1.0516, -0.9118, -1.0844, -1.0798])\n",
      "mean: tensor(-1.0441)\n",
      "iter_dt 6593.58ms; iter 41: train loss 0.08082 temperature: 7.049999999999993\n",
      "mean_logits tensor([-1.0681, -1.1434, -1.0599, -1.1950, -1.0714], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0858, -1.0919, -1.0908, -1.0964, -1.0729])\n",
      "mean: tensor(-1.0876)\n",
      "iter_dt 6615.57ms; iter 42: train loss 0.02637 temperature: 7.0999999999999925\n",
      "mean_logits tensor([-1.1019, -1.1528, -1.1218, -0.9634, -1.0202], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0735, -0.9303, -1.0787, -1.0751, -1.0786])\n",
      "mean: tensor(-1.0473)\n",
      "iter_dt 6443.43ms; iter 43: train loss 0.10937 temperature: 7.149999999999992\n",
      "mean_logits tensor([-1.1382, -1.0904, -0.9823, -1.1518, -1.0811], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0755, -1.0838, -1.0868, -0.9723, -1.0689])\n",
      "mean: tensor(-1.0575)\n",
      "iter_dt 6479.71ms; iter 44: train loss 0.07886 temperature: 7.199999999999992\n",
      "mean_logits tensor([-1.0814, -0.9471, -1.1238, -1.0839, -1.0948], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0232, -1.0697, -1.0835, -1.0210, -0.9868])\n",
      "mean: tensor(-1.0369)\n",
      "iter_dt 6614.31ms; iter 45: train loss 0.05634 temperature: 7.249999999999992\n",
      "mean_logits tensor([-1.0037, -1.1002, -1.0012, -1.1068, -1.0417], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0527, -1.1003, -1.0994, -1.0966, -1.0916])\n",
      "mean: tensor(-1.0881)\n",
      "iter_dt 6854.29ms; iter 46: train loss 0.02394 temperature: 7.299999999999992\n",
      "mean_logits tensor([-1.0015, -0.8984, -1.0484, -1.0819, -0.8988], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0735, -1.0842, -1.0133, -1.0604, -1.0969])\n",
      "mean: tensor(-1.0657)\n",
      "iter_dt 6584.46ms; iter 47: train loss 0.11922 temperature: 7.349999999999992\n",
      "mean_logits tensor([-1.0261, -0.9445, -0.9610, -1.0875, -0.9466], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0753, -1.0923, -1.0576, -1.0730, -1.0662])\n",
      "mean: tensor(-1.0729)\n",
      "iter_dt 6429.78ms; iter 48: train loss 0.07338 temperature: 7.3999999999999915\n",
      "mean_logits tensor([-1.1108, -1.0716, -1.0225, -0.9749, -1.2151], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0785, -1.1008, -0.9688, -1.0894, -1.0835])\n",
      "mean: tensor(-1.0642)\n",
      "iter_dt 6548.87ms; iter 49: train loss 0.06285 temperature: 7.449999999999991\n",
      "mean_logits tensor([-1.1743, -1.1718, -1.0453, -0.9481, -0.9939], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0656, -1.0525, -1.0547, -1.0859, -1.0253])\n",
      "mean: tensor(-1.0568)\n",
      "iter_dt 6474.06ms; iter 50: train loss 0.07920 temperature: 7.499999999999991\n",
      "mean_logits tensor([-1.1024, -1.0668, -1.0028, -1.0664, -1.0170], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0718, -1.0954, -1.0812, -1.0982, -1.0858])\n",
      "mean: tensor(-1.0865)\n",
      "iter_dt 6425.08ms; iter 51: train loss 0.02247 temperature: 7.549999999999991\n",
      "mean_logits tensor([-1.0994, -1.0568, -1.0422, -1.0736, -1.0232], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0689, -1.0889, -1.0025, -1.0645, -1.0638])\n",
      "mean: tensor(-1.0577)\n",
      "iter_dt 6450.61ms; iter 52: train loss 0.00862 temperature: 7.599999999999991\n",
      "mean_logits tensor([-1.0004, -1.0355, -1.1643, -1.1433, -1.0100], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0634, -1.0770, -0.8646, -1.0741, -1.0302])\n",
      "mean: tensor(-1.0219)\n",
      "iter_dt 6413.19ms; iter 53: train loss 0.15620 temperature: 7.649999999999991\n",
      "mean_logits tensor([-1.0919, -1.0992, -1.0899, -1.2171, -1.0867], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.1008, -1.0963, -1.0622, -1.0256, -1.0718])\n",
      "mean: tensor(-1.0713)\n",
      "iter_dt 6430.74ms; iter 54: train loss 0.07117 temperature: 7.69999999999999\n",
      "mean_logits tensor([-1.0849, -1.0223, -0.9876, -1.0668, -0.9758], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0870, -1.0790, -1.0925, -1.0972, -0.7519])\n",
      "mean: tensor(-1.0215)\n",
      "iter_dt 6544.98ms; iter 55: train loss 0.08119 temperature: 7.74999999999999\n",
      "mean_logits tensor([-1.1735, -1.0517, -1.0885, -1.1161, -0.9566], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0808, -1.0679, -1.0727, -1.0414, -1.0951])\n",
      "mean: tensor(-1.0716)\n",
      "iter_dt 6463.53ms; iter 56: train loss 0.05686 temperature: 7.79999999999999\n",
      "mean_logits tensor([-1.0299, -0.9532, -1.0007, -0.9925, -1.1267], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0529, -1.0799, -1.0878, -1.0583, -1.0710])\n",
      "mean: tensor(-1.0700)\n",
      "iter_dt 6495.37ms; iter 57: train loss 0.04998 temperature: 7.84999999999999\n",
      "mean_logits tensor([-1.0273, -1.0539, -1.0106, -1.0512, -1.0577], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0958, -1.0763, -1.0853, -1.0749, -1.0949])\n",
      "mean: tensor(-1.0854)\n",
      "iter_dt 6394.59ms; iter 58: train loss 0.02107 temperature: 7.89999999999999\n",
      "mean_logits tensor([-1.0466, -1.0869, -0.9878, -1.0932, -1.1764], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0972, -1.0760, -1.0089, -1.0950, -1.0447])\n",
      "mean: tensor(-1.0643)\n",
      "iter_dt 6422.83ms; iter 59: train loss 0.03723 temperature: 7.9499999999999895\n",
      "mean_logits tensor([-0.9768, -1.2172, -1.0278, -1.0890, -1.0010], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0632, -1.0853, -0.8697, -1.0656, -1.0543])\n",
      "mean: tensor(-1.0276)\n",
      "iter_dt 6469.61ms; iter 60: train loss 0.08513 temperature: 7.999999999999989\n",
      "mean_logits tensor([-1.1115, -1.0708, -1.0808, -1.1312, -1.0759], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0890, -1.0447, -1.0946, -1.0946, -1.0878])\n",
      "mean: tensor(-1.0821)\n",
      "iter_dt 6403.86ms; iter 61: train loss 0.00511 temperature: 8.04999999999999\n",
      "mean_logits tensor([-1.0258, -1.0451, -1.1209, -1.0308, -1.1230], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0642, -1.0831, -1.0744, -1.0898, -1.0920])\n",
      "mean: tensor(-1.0807)\n",
      "iter_dt 6436.72ms; iter 62: train loss 0.01625 temperature: 8.09999999999999\n",
      "mean_logits tensor([-1.0972, -1.1297, -1.0560, -1.0759, -1.0690], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0445, -1.0972, -1.0607, -1.0692, -1.0852])\n",
      "mean: tensor(-1.0714)\n",
      "iter_dt 6428.80ms; iter 63: train loss 0.00726 temperature: 8.149999999999991\n",
      "mean_logits tensor([-1.1054, -1.0038, -1.1174, -1.0309, -0.9524], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0982, -1.0789, -1.0664, -1.0880, -1.0561])\n",
      "mean: tensor(-1.0775)\n",
      "iter_dt 6420.68ms; iter 64: train loss 0.03524 temperature: 8.199999999999992\n",
      "mean_logits tensor([-1.1066, -1.0969, -1.0278, -1.0675, -1.0282], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0863, -1.0833, -1.0494, -1.0936, -1.0784])\n",
      "mean: tensor(-1.0782)\n",
      "iter_dt 6412.66ms; iter 65: train loss 0.00715 temperature: 8.249999999999993\n",
      "mean_logits tensor([-1.0049, -1.1111, -1.0116, -1.1639, -1.0517], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0829, -1.0938, -1.0492, -1.0770, -1.0938])\n",
      "mean: tensor(-1.0793)\n",
      "iter_dt 6400.26ms; iter 66: train loss 0.02983 temperature: 8.299999999999994\n",
      "mean_logits tensor([-1.0519, -1.0226, -1.0978, -1.1124, -1.0682], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0753, -1.0653, -1.0571, -1.0893, -1.0942])\n",
      "mean: tensor(-1.0762)\n",
      "iter_dt 6423.60ms; iter 67: train loss 0.00885 temperature: 8.349999999999994\n",
      "mean_logits tensor([-1.0472, -1.1872, -1.0799, -1.2077, -1.1668], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0933, -1.0846, -1.0994, -1.0859, -1.0993])\n",
      "mean: tensor(-1.0925)\n",
      "iter_dt 6431.59ms; iter 68: train loss 0.06292 temperature: 8.399999999999995\n",
      "mean_logits tensor([-1.0710, -1.1560, -1.0669, -1.1896, -1.2010], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0285, -1.0589, -1.1010, -1.0784, -1.0440])\n",
      "mean: tensor(-1.0622)\n",
      "iter_dt 6397.64ms; iter 69: train loss 0.09279 temperature: 8.449999999999996\n",
      "mean_logits tensor([-1.0555, -1.0481, -1.0755, -1.1601, -1.1082], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0670, -1.1002, -1.0383, -1.0829, -1.1001])\n",
      "mean: tensor(-1.0777)\n",
      "iter_dt 6601.90ms; iter 70: train loss 0.01851 temperature: 8.499999999999996\n",
      "mean_logits tensor([-1.0898, -1.0674, -1.0789, -1.1681, -1.0269], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0727, -1.0989, -1.0848, -1.0936, -1.0948])\n",
      "mean: tensor(-1.0889)\n",
      "iter_dt 6465.93ms; iter 71: train loss 0.02065 temperature: 8.549999999999997\n",
      "mean_logits tensor([-1.0897, -1.0624, -0.9741, -1.0258, -0.9776], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0952, -1.0832, -1.0607, -1.0577, -1.0959])\n",
      "mean: tensor(-1.0785)\n",
      "iter_dt 6470.80ms; iter 72: train loss 0.03618 temperature: 8.599999999999998\n",
      "mean_logits tensor([-1.0397, -1.0350, -1.0644, -1.0503, -1.0191], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0947, -1.0297, -1.0958, -1.0738, -1.0756])\n",
      "mean: tensor(-1.0739)\n",
      "iter_dt 6413.48ms; iter 73: train loss 0.01301 temperature: 8.649999999999999\n",
      "mean_logits tensor([-1.0417, -1.1031, -1.1299, -1.2662, -1.1306], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0087, -1.0890, -1.0780, -1.0714, -1.0297])\n",
      "mean: tensor(-1.0554)\n",
      "iter_dt 6410.03ms; iter 74: train loss 0.10343 temperature: 8.7\n",
      "mean_logits tensor([-1.1372, -1.0260, -1.0787, -1.0628, -1.1391], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0618, -1.0569, -1.0473, -1.0786, -1.1005])\n",
      "mean: tensor(-1.0690)\n",
      "iter_dt 6476.19ms; iter 75: train loss 0.01666 temperature: 8.75\n",
      "mean_logits tensor([-1.0479, -1.1816, -1.0776, -1.0190, -1.2453], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9180, -1.0646, -1.0325, -1.0865, -1.0825])\n",
      "mean: tensor(-1.0368)\n",
      "iter_dt 6421.52ms; iter 76: train loss 0.11543 temperature: 8.8\n",
      "mean_logits tensor([-1.1121, -1.0977, -1.1163, -1.0210, -0.9699], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0952, -1.0085, -1.0866, -1.0872, -0.8357])\n",
      "mean: tensor(-1.0226)\n",
      "iter_dt 6448.59ms; iter 77: train loss 0.04436 temperature: 8.850000000000001\n",
      "mean_logits tensor([-1.0605, -1.0570, -1.0598, -1.0094, -1.0933], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0253, -1.0647, -1.0292, -1.0572, -1.0988])\n",
      "mean: tensor(-1.0551)\n",
      "iter_dt 6405.41ms; iter 78: train loss 0.00728 temperature: 8.900000000000002\n",
      "mean_logits tensor([-1.0355, -1.0946, -1.0461, -0.9303, -1.1060], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0909, -1.0041, -1.0783, -1.0466, -1.0892])\n",
      "mean: tensor(-1.0618)\n",
      "iter_dt 6422.97ms; iter 79: train loss 0.04030 temperature: 8.950000000000003\n",
      "mean_logits tensor([-0.9981, -1.0366, -1.0359, -1.0388, -1.0304], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0922, -1.0858, -1.0412, -1.0838, -1.0342])\n",
      "mean: tensor(-1.0674)\n",
      "iter_dt 6531.29ms; iter 80: train loss 0.02181 temperature: 9.000000000000004\n",
      "mean_logits tensor([-1.1230, -1.0774, -1.1545, -1.0073, -1.0710], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0974, -1.0733, -1.0112, -1.0713, -1.0992])\n",
      "mean: tensor(-1.0705)\n",
      "iter_dt 6437.25ms; iter 81: train loss 0.04504 temperature: 9.050000000000004\n",
      "mean_logits tensor([-1.1244, -1.0969, -1.1249, -1.0292, -1.0547], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0264, -1.0736, -1.0196, -1.0873, -1.0221])\n",
      "mean: tensor(-1.0458)\n",
      "iter_dt 6517.65ms; iter 82: train loss 0.04373 temperature: 9.100000000000005\n",
      "mean_logits tensor([-0.9778, -1.1034, -1.1109, -1.0662, -1.0939], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0059, -1.0479, -1.0522, -1.0790, -1.0612])\n",
      "mean: tensor(-1.0492)\n",
      "iter_dt 6470.76ms; iter 83: train loss 0.01457 temperature: 9.150000000000006\n",
      "mean_logits tensor([-1.0248, -1.0491, -0.9923, -1.0799, -1.0659], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0154, -1.0473, -1.1002, -1.0038, -1.0920])\n",
      "mean: tensor(-1.0518)\n",
      "iter_dt 6486.69ms; iter 84: train loss 0.02953 temperature: 9.200000000000006\n",
      "mean_logits tensor([-1.0512, -1.1307, -1.1040, -1.0408, -1.0945], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0832, -1.0055, -1.0257, -1.0932, -1.0924])\n",
      "mean: tensor(-1.0600)\n",
      "iter_dt 6527.86ms; iter 85: train loss 0.04330 temperature: 9.250000000000007\n",
      "mean_logits tensor([-0.9801, -1.0248, -1.1287, -1.0039, -1.1381], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0867, -1.0905, -1.0486, -1.0833, -1.0802])\n",
      "mean: tensor(-1.0779)\n",
      "iter_dt 6470.63ms; iter 86: train loss 0.05283 temperature: 9.300000000000008\n",
      "mean_logits tensor([-1.1055, -1.1183, -1.0718, -1.1852, -1.0658], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0763, -1.0536, -1.0753, -1.0483, -1.0763])\n",
      "mean: tensor(-1.0659)\n",
      "iter_dt 6575.70ms; iter 87: train loss 0.04413 temperature: 9.350000000000009\n",
      "mean_logits tensor([-1.0686, -1.0818, -1.0681, -1.0846, -1.0747], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0297, -1.0925, -1.0919, -1.0802, -1.0943])\n",
      "mean: tensor(-1.0777)\n",
      "iter_dt 6429.80ms; iter 88: train loss 0.00436 temperature: 9.40000000000001\n",
      "mean_logits tensor([-1.0283, -1.0311, -1.1120, -1.1092, -1.1222], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0537, -1.0749, -1.0392, -1.0901, -1.0319])\n",
      "mean: tensor(-1.0580)\n",
      "iter_dt 6648.89ms; iter 89: train loss 0.02804 temperature: 9.45000000000001\n",
      "mean_logits tensor([-1.0176, -1.0922, -1.0899, -0.9881, -1.0608], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9988, -1.0060, -1.0125, -1.0866, -1.0874])\n",
      "mean: tensor(-1.0383)\n",
      "iter_dt 6686.08ms; iter 90: train loss 0.03915 temperature: 9.50000000000001\n",
      "mean_logits tensor([-1.0685, -1.0300, -0.9770, -1.1108, -1.0187], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0606, -1.0742, -1.0912, -1.0958, -0.9716])\n",
      "mean: tensor(-1.0587)\n",
      "iter_dt 6451.59ms; iter 91: train loss 0.02764 temperature: 9.550000000000011\n",
      "mean_logits tensor([-0.9718, -1.0351, -0.9772, -1.0046, -1.0905], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0601, -1.0947, -1.0838, -1.0819, -1.0883])\n",
      "mean: tensor(-1.0818)\n",
      "iter_dt 6440.78ms; iter 92: train loss 0.04542 temperature: 9.600000000000012\n",
      "mean_logits tensor([-1.0883, -1.0615, -0.9721, -1.0320, -1.0792], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0950, -1.0963, -1.0877, -1.0991, -1.0157])\n",
      "mean: tensor(-1.0787)\n",
      "iter_dt 6529.05ms; iter 93: train loss 0.03730 temperature: 9.650000000000013\n",
      "mean_logits tensor([-1.0150, -1.0923, -1.1346, -1.0036, -1.0725], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0842, -1.0932, -1.0285, -1.0748, -1.0443])\n",
      "mean: tensor(-1.0650)\n",
      "iter_dt 6475.13ms; iter 94: train loss 0.03686 temperature: 9.700000000000014\n",
      "mean_logits tensor([-1.1095, -0.9558, -0.9526, -1.0539, -1.0033], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9461, -1.0636, -1.0922, -1.0738, -1.0309])\n",
      "mean: tensor(-1.0413)\n",
      "iter_dt 6445.21ms; iter 95: train loss 0.09134 temperature: 9.750000000000014\n",
      "mean_logits tensor([-1.0148, -1.1127, -1.0823, -1.0090, -1.0545], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0949, -1.0881, -1.0931, -1.0178, -1.0970])\n",
      "mean: tensor(-1.0782)\n",
      "iter_dt 6487.50ms; iter 96: train loss 0.01509 temperature: 9.800000000000015\n",
      "mean_logits tensor([-1.0229, -1.1723, -1.0359, -1.0432, -1.0835], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0583, -1.0922, -1.0670, -1.0933, -1.0883])\n",
      "mean: tensor(-1.0798)\n",
      "iter_dt 6392.24ms; iter 97: train loss 0.02027 temperature: 9.850000000000016\n",
      "mean_logits tensor([-1.0873, -0.9707, -1.0700, -1.0929, -1.0696], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0885, -1.0828, -1.0855, -1.0880, -1.0974])\n",
      "mean: tensor(-1.0884)\n",
      "iter_dt 6408.84ms; iter 98: train loss 0.02143 temperature: 9.900000000000016\n",
      "mean_logits tensor([-1.0996, -1.1217, -1.1051, -1.0582, -1.1238], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-1.0282, -1.1006, -1.0607, -1.1003, -1.0350])\n",
      "mean: tensor(-1.0650)\n",
      "iter_dt 6509.80ms; iter 99: train loss 0.02955 temperature: 9.950000000000017\n",
      "converged SCF energy = -0.910873554594387\n",
      "Starting to parse FermionOperator using 4 qubits...\n",
      "\n",
      "Operator t:  -0.49178577730353756 [] +\n",
      "-0.0573839840149255 [X0 X1 Y2 Y3] +\n",
      "0.0573839840149255 [X0 Y1 Y2 X3] +\n",
      "0.0573839840149255 [Y0 X1 X2 Y3] +\n",
      "-0.0573839840149255 [Y0 Y1 X2 X3] +\n",
      "0.09345649667701589 [Z0] +\n",
      "0.13817584576560335 [Z0 Z1] +\n",
      "0.08253705488832763 [Z0 Z2] +\n",
      "0.13992103890325314 [Z0 Z3] +\n",
      "0.09345649667701589 [Z1] +\n",
      "0.13992103890325314 [Z1 Z2] +\n",
      "0.08253705488832763 [Z1 Z3] +\n",
      "-0.03564481621009516 [Z2] +\n",
      "0.1458551903009311 [Z2 Z3] +\n",
      "-0.035644816210095145 [Z3]\n",
      "Term, coeff:  () -0.49178577730353756\n",
      "Term, coeff:  ((0, 'Z'),) 0.09345649667701589\n",
      "Index, p_char:  0 Z\n",
      "Term, coeff:  ((1, 'Z'),) 0.09345649667701589\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((2, 'Z'),) -0.03564481621009516\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((3, 'Z'),) -0.035644816210095145\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((0, 'Z'), (1, 'Z')) 0.13817584576560335\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((0, 'Y'), (1, 'X'), (2, 'X'), (3, 'Y')) 0.0573839840149255\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'Y'), (1, 'Y'), (2, 'X'), (3, 'X')) -0.0573839840149255\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'X'), (1, 'X'), (2, 'Y'), (3, 'Y')) -0.0573839840149255\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'X'), (1, 'Y'), (2, 'Y'), (3, 'X')) 0.0573839840149255\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'Z'), (2, 'Z')) 0.08253705488832763\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((0, 'Z'), (3, 'Z')) 0.13992103890325314\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((1, 'Z'), (2, 'Z')) 0.13992103890325314\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((1, 'Z'), (3, 'Z')) 0.08253705488832763\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((2, 'Z'), (3, 'Z')) 0.1458551903009311\n",
      "Index, p_char:  2 Z\n",
      "Index, p_char:  3 Z\n",
      "ground state: -0.9981493534714105\n",
      "hf state: -0.9108735545943868\n",
      "number of parameters: 85.29M\n",
      "running on device cpu\n",
      "mean_logits tensor([-1.1676, -0.9894, -1.0338, -1.0321, -1.2728], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9705, -0.9155, -0.9787, -0.9882, -0.7849])\n",
      "mean: tensor(-0.9276)\n",
      "iter_dt 0.00ms; iter 0: train loss 0.46103 temperature: 5\n",
      "mean_logits tensor([-0.9160, -1.4367, -0.9551, -0.9196, -0.9355], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9953, -0.9514, -0.9225, -0.9676, -0.8948])\n",
      "mean: tensor(-0.9463)\n",
      "iter_dt 1694972702099.37ms; iter 1: train loss 0.53831 temperature: 5.05\n",
      "mean_logits tensor([-0.9963, -0.9618, -1.1439, -1.2338, -0.9703], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9714, -0.9926, -0.9680, -0.9647, -0.9787])\n",
      "mean: tensor(-0.9751)\n",
      "iter_dt 6565.43ms; iter 2: train loss 0.18491 temperature: 5.1\n",
      "mean_logits tensor([-1.0861, -0.9041, -1.0123, -0.8850, -1.0435], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9582, -0.9781, -0.9954, -0.8903, -0.9950])\n",
      "mean: tensor(-0.9634)\n",
      "iter_dt 6536.64ms; iter 3: train loss 0.03658 temperature: 5.1499999999999995\n",
      "mean_logits tensor([-0.8009, -1.0625, -1.0927, -1.0446, -1.1445], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9978, -0.9765, -0.9893, -0.9741, -0.9401])\n",
      "mean: tensor(-0.9756)\n",
      "iter_dt 6540.13ms; iter 4: train loss 0.15046 temperature: 5.199999999999999\n",
      "mean_logits tensor([-0.8638, -0.8316, -0.9356, -1.0447, -0.8478], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9493, -0.9587, -0.9506, -0.9942, -0.9775])\n",
      "mean: tensor(-0.9661)\n",
      "iter_dt 6511.31ms; iter 5: train loss 0.05347 temperature: 5.249999999999999\n",
      "mean_logits tensor([-1.0351, -0.8563, -0.8912, -0.9115, -0.8645], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8933, -0.9973, -0.9807, -0.7431, -0.9777])\n",
      "mean: tensor(-0.9184)\n",
      "iter_dt 6463.65ms; iter 6: train loss 0.10948 temperature: 5.299999999999999\n",
      "mean_logits tensor([-0.9648, -0.8821, -0.9912, -0.9509, -0.9157], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9582, -0.9893, -0.9794, -0.9371, -0.9054])\n",
      "mean: tensor(-0.9539)\n",
      "iter_dt 6420.68ms; iter 7: train loss 0.01557 temperature: 5.349999999999999\n",
      "mean_logits tensor([-0.9751, -0.9458, -0.9659, -0.8346, -0.9420], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9921, -0.9021, -0.9848, -0.9499, -0.9647])\n",
      "mean: tensor(-0.9587)\n",
      "iter_dt 6410.95ms; iter 8: train loss 0.01991 temperature: 5.399999999999999\n",
      "mean_logits tensor([-0.8724, -0.9160, -0.9386, -0.9014, -0.8773], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9385, -0.9281, -0.9728, -0.8443, -0.8981])\n",
      "mean: tensor(-0.9163)\n",
      "iter_dt 6406.93ms; iter 9: train loss 0.01136 temperature: 5.449999999999998\n",
      "mean_logits tensor([-0.8552, -1.0391, -0.8522, -0.8548, -0.9645], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.7426, -0.9969, -0.9361, -0.6676, -0.9198])\n",
      "mean: tensor(-0.8526)\n",
      "iter_dt 6496.03ms; iter 10: train loss 0.05855 temperature: 5.499999999999998\n",
      "mean_logits tensor([-0.9441, -1.0277, -1.0629, -0.8648, -0.8275], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9896, -0.9748, -0.9727, -0.9828, -0.9042])\n",
      "mean: tensor(-0.9648)\n",
      "iter_dt 6629.81ms; iter 11: train loss 0.04383 temperature: 5.549999999999998\n",
      "mean_logits tensor([-0.8872, -0.9510, -0.9665, -0.9448, -0.9381], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9457, -0.9807, -0.9743, -0.9964, -0.9830])\n",
      "mean: tensor(-0.9760)\n",
      "iter_dt 6739.46ms; iter 12: train loss 0.01205 temperature: 5.599999999999998\n",
      "mean_logits tensor([-0.9490, -0.7968, -0.9585, -1.0270, -0.9663], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9304, -0.9576, -0.9600, -0.9923, -0.9530])\n",
      "mean: tensor(-0.9587)\n",
      "iter_dt 6561.89ms; iter 13: train loss 0.03246 temperature: 5.649999999999998\n",
      "mean_logits tensor([-1.0809, -1.0197, -1.0357, -0.9502, -0.9227], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9653, -0.9732, -0.9906, -0.9917, -0.9904])\n",
      "mean: tensor(-0.9822)\n",
      "iter_dt 6553.22ms; iter 14: train loss 0.03559 temperature: 5.6999999999999975\n",
      "mean_logits tensor([-0.9560, -1.0213, -0.9474, -0.9877, -1.0733], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9638, -0.9858, -0.9488, -0.9911, -0.9387])\n",
      "mean: tensor(-0.9656)\n",
      "iter_dt 6652.02ms; iter 15: train loss 0.02911 temperature: 5.749999999999997\n",
      "mean_logits tensor([-0.9884, -1.0230, -1.0514, -0.9545, -0.9335], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9383, -0.9784, -0.9144, -0.9937, -0.9754])\n",
      "mean: tensor(-0.9600)\n",
      "iter_dt 6478.83ms; iter 16: train loss 0.03779 temperature: 5.799999999999997\n",
      "mean_logits tensor([-0.8906, -0.8608, -1.0801, -1.0571, -1.0114], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9934, -0.9784, -0.9341, -0.9245, -0.9941])\n",
      "mean: tensor(-0.9649)\n",
      "iter_dt 6424.32ms; iter 17: train loss 0.08937 temperature: 5.849999999999997\n",
      "mean_logits tensor([-1.0092, -1.1228, -1.2306, -0.9563, -0.9198], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9095, -0.9832, -0.9231, -0.9045, -0.9511])\n",
      "mean: tensor(-0.9343)\n",
      "iter_dt 6428.43ms; iter 18: train loss 0.21458 temperature: 5.899999999999997\n",
      "mean_logits tensor([-1.0340, -0.9151, -0.8521, -1.0635, -0.9717], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9547, -0.9109, -0.8265, -0.9646, -0.9466])\n",
      "mean: tensor(-0.9207)\n",
      "iter_dt 6428.05ms; iter 19: train loss 0.02565 temperature: 5.949999999999997\n",
      "mean_logits tensor([-1.1339, -0.7728, -1.0257, -1.0371, -1.0483], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9886, -0.9134, -0.8564, -0.9773, -0.9658])\n",
      "mean: tensor(-0.9403)\n",
      "iter_dt 6447.51ms; iter 20: train loss 0.10998 temperature: 5.9999999999999964\n",
      "mean_logits tensor([-1.0873, -0.9093, -0.8679, -0.8882, -0.9088], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9171, -0.9848, -0.8595, -0.8412, -0.9833])\n",
      "mean: tensor(-0.9172)\n",
      "iter_dt 6526.96ms; iter 21: train loss 0.06059 temperature: 6.049999999999996\n",
      "mean_logits tensor([-0.8943, -0.9312, -0.9423, -0.9441, -1.0126], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9469, -0.9975, -0.9487, -0.9856, -0.9971])\n",
      "mean: tensor(-0.9751)\n",
      "iter_dt 6565.41ms; iter 22: train loss 0.01231 temperature: 6.099999999999996\n",
      "mean_logits tensor([-0.9910, -0.9004, -0.9449, -1.0004, -0.9756], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9058, -0.8817, -0.9558, -0.9816, -0.9237])\n",
      "mean: tensor(-0.9297)\n",
      "iter_dt 6521.48ms; iter 23: train loss 0.01434 temperature: 6.149999999999996\n",
      "mean_logits tensor([-0.9351, -1.0245, -0.8282, -0.9976, -0.9443], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9979, -0.9769, -0.9545, -0.9394, -0.9956])\n",
      "mean: tensor(-0.9728)\n",
      "iter_dt 6479.52ms; iter 24: train loss 0.03617 temperature: 6.199999999999996\n",
      "mean_logits tensor([-0.9104, -0.7848, -0.8973, -0.8993, -0.9254], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9700, -0.9114, -0.9503, -0.9243, -0.9544])\n",
      "mean: tensor(-0.9421)\n",
      "iter_dt 6472.13ms; iter 25: train loss 0.02762 temperature: 6.249999999999996\n",
      "mean_logits tensor([-0.9145, -0.8478, -0.8849, -0.8757, -0.9302], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9698, -0.9082, -0.9905, -0.9641, -0.9272])\n",
      "mean: tensor(-0.9519)\n",
      "iter_dt 6543.14ms; iter 26: train loss 0.03265 temperature: 6.299999999999995\n",
      "mean_logits tensor([-1.0654, -0.9700, -1.0355, -0.9057, -1.0876], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9103, -0.9464, -0.9620, -0.9932, -0.9841])\n",
      "mean: tensor(-0.9592)\n",
      "iter_dt 6494.75ms; iter 27: train loss 0.07073 temperature: 6.349999999999995\n",
      "mean_logits tensor([-0.9229, -0.9512, -0.9918, -1.0046, -1.0473], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8449, -0.9139, -0.9832, -0.9849, -0.9789])\n",
      "mean: tensor(-0.9412)\n",
      "iter_dt 6471.70ms; iter 28: train loss 0.01669 temperature: 6.399999999999995\n",
      "mean_logits tensor([-0.8241, -1.0126, -0.8851, -1.0561, -0.8415], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9314, -0.8422, -0.9799, -0.9468, -0.9555])\n",
      "mean: tensor(-0.9312)\n",
      "iter_dt 6462.48ms; iter 29: train loss 0.09553 temperature: 6.449999999999995\n",
      "mean_logits tensor([-0.9480, -0.9599, -0.9626, -0.9262, -1.0747], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9624, -0.8187, -0.9941, -0.9766, -0.9699])\n",
      "mean: tensor(-0.9444)\n",
      "iter_dt 6461.57ms; iter 30: train loss 0.04575 temperature: 6.499999999999995\n",
      "mean_logits tensor([-0.9797, -0.9587, -0.9395, -0.9347, -1.0704], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9802, -0.9635, -0.9866, -0.9582, -0.9289])\n",
      "mean: tensor(-0.9635)\n",
      "iter_dt 6439.97ms; iter 31: train loss 0.03345 temperature: 6.5499999999999945\n",
      "mean_logits tensor([-0.9057, -0.9243, -1.0077, -0.8338, -1.0170], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9609, -0.9870, -0.9495, -0.9760, -0.9730])\n",
      "mean: tensor(-0.9693)\n",
      "iter_dt 6591.70ms; iter 32: train loss 0.04164 temperature: 6.599999999999994\n",
      "mean_logits tensor([-1.0616, -1.0188, -0.9552, -0.9390, -0.9093], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9449, -0.9532, -0.9309, -0.9970, -0.9308])\n",
      "mean: tensor(-0.9513)\n",
      "iter_dt 6563.76ms; iter 33: train loss 0.03249 temperature: 6.649999999999994\n",
      "mean_logits tensor([-0.8805, -1.0109, -0.9024, -1.0724, -0.8374], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9777, -0.9403, -0.9871, -0.9808, -0.9874])\n",
      "mean: tensor(-0.9747)\n",
      "iter_dt 6507.66ms; iter 34: train loss 0.06970 temperature: 6.699999999999994\n",
      "mean_logits tensor([-0.9253, -0.9636, -0.8717, -0.9611, -1.0167], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9856, -0.9948, -0.9820, -0.9732, -0.8937])\n",
      "mean: tensor(-0.9659)\n",
      "iter_dt 6531.89ms; iter 35: train loss 0.04248 temperature: 6.749999999999994\n",
      "mean_logits tensor([-0.8655, -0.9003, -0.8398, -0.9719, -0.8979], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9944, -0.9704, -0.9515, -0.9814, -0.9675])\n",
      "mean: tensor(-0.9730)\n",
      "iter_dt 6512.41ms; iter 36: train loss 0.04913 temperature: 6.799999999999994\n",
      "mean_logits tensor([-0.9107, -0.9106, -1.0382, -1.0701, -1.0261], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9421, -0.8903, -0.9658, -0.9426, -0.9157])\n",
      "mean: tensor(-0.9313)\n",
      "iter_dt 6590.90ms; iter 37: train loss 0.05089 temperature: 6.849999999999993\n",
      "mean_logits tensor([-1.0634, -0.9332, -0.8630, -0.8020, -0.9677], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9346, -0.9779, -0.9955, -0.9311, -0.9803])\n",
      "mean: tensor(-0.9639)\n",
      "iter_dt 6575.58ms; iter 38: train loss 0.06882 temperature: 6.899999999999993\n",
      "mean_logits tensor([-0.7774, -0.8076, -0.8578, -1.0134, -0.9997], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9389, -0.9521, -0.9563, -0.9508, -0.9859])\n",
      "mean: tensor(-0.9568)\n",
      "iter_dt 6524.46ms; iter 39: train loss 0.07114 temperature: 6.949999999999993\n",
      "mean_logits tensor([-1.0069, -0.9716, -0.9429, -0.8659, -0.9028], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9317, -0.9767, -0.8842, -0.9240, -0.9523])\n",
      "mean: tensor(-0.9338)\n",
      "iter_dt 6449.16ms; iter 40: train loss 0.01936 temperature: 6.999999999999993\n",
      "mean_logits tensor([-0.9612, -1.0047, -0.8569, -0.9071, -1.0557], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8575, -0.8355, -0.9952, -0.8777, -0.9854])\n",
      "mean: tensor(-0.9103)\n",
      "iter_dt 6521.81ms; iter 41: train loss 0.08251 temperature: 7.049999999999993\n",
      "mean_logits tensor([-0.9628, -0.9721, -0.9456, -0.9395, -0.9552], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9907, -0.9819, -0.9643, -0.9525, -0.9883])\n",
      "mean: tensor(-0.9755)\n",
      "iter_dt 7187.15ms; iter 42: train loss 0.00346 temperature: 7.0999999999999925\n",
      "mean_logits tensor([-0.9736, -0.9924, -1.0581, -0.9208, -0.9204], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9821, -0.9974, -0.9614, -0.9950, -0.9463])\n",
      "mean: tensor(-0.9764)\n",
      "iter_dt 6570.24ms; iter 43: train loss 0.02261 temperature: 7.149999999999992\n",
      "mean_logits tensor([-1.0593, -0.9567, -0.9928, -1.0057, -1.0073], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9792, -0.9896, -0.9973, -0.9948, -0.9731])\n",
      "mean: tensor(-0.9868)\n",
      "iter_dt 6473.62ms; iter 44: train loss 0.01327 temperature: 7.199999999999992\n",
      "mean_logits tensor([-0.9525, -0.9560, -1.0625, -0.9830, -0.9412], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9884, -0.9798, -0.9113, -0.9581, -0.9920])\n",
      "mean: tensor(-0.9659)\n",
      "iter_dt 6440.83ms; iter 45: train loss 0.03996 temperature: 7.249999999999992\n",
      "mean_logits tensor([-1.0395, -1.1456, -1.0618, -0.9833, -1.1422], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9751, -0.9525, -0.9771, -0.9590, -0.8706])\n",
      "mean: tensor(-0.9469)\n",
      "iter_dt 6464.27ms; iter 46: train loss 0.19019 temperature: 7.299999999999992\n",
      "mean_logits tensor([-0.9933, -1.1422, -0.9940, -0.9252, -0.9156], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9933, -0.9127, -0.9780, -0.9566, -0.9919])\n",
      "mean: tensor(-0.9665)\n",
      "iter_dt 6450.98ms; iter 47: train loss 0.09206 temperature: 7.349999999999992\n",
      "mean_logits tensor([-0.9377, -0.9407, -1.0550, -0.9716, -0.9421], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9391, -0.9834, -0.8871, -0.9940, -0.9709])\n",
      "mean: tensor(-0.9549)\n",
      "iter_dt 6465.24ms; iter 48: train loss 0.04374 temperature: 7.3999999999999915\n",
      "mean_logits tensor([-1.1017, -0.9950, -1.0158, -0.9828, -0.9062], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8060, -0.9537, -0.9780, -0.9693, -0.9561])\n",
      "mean: tensor(-0.9326)\n",
      "iter_dt 6473.71ms; iter 49: train loss 0.12660 temperature: 7.449999999999991\n",
      "mean_logits tensor([-1.0412, -0.9681, -1.0166, -1.0054, -0.9881], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9542, -0.9762, -0.9163, -0.9885, -0.9609])\n",
      "mean: tensor(-0.9592)\n",
      "iter_dt 6397.05ms; iter 50: train loss 0.02661 temperature: 7.499999999999991\n",
      "mean_logits tensor([-0.9491, -1.0294, -0.9326, -0.9557, -0.8692], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9857, -0.9649, -0.9809, -0.9898, -0.9730])\n",
      "mean: tensor(-0.9789)\n",
      "iter_dt 6434.14ms; iter 51: train loss 0.02635 temperature: 7.549999999999991\n",
      "mean_logits tensor([-0.9919, -0.8176, -0.9176, -1.0866, -0.9610], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9367, -0.9904, -0.9912, -0.9828, -0.9873])\n",
      "mean: tensor(-0.9777)\n",
      "iter_dt 6495.60ms; iter 52: train loss 0.06605 temperature: 7.599999999999991\n",
      "mean_logits tensor([-0.9241, -0.9280, -0.9114, -0.8854, -1.0382], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9933, -0.9882, -0.9512, -0.9213, -0.9978])\n",
      "mean: tensor(-0.9704)\n",
      "iter_dt 6485.77ms; iter 53: train loss 0.01755 temperature: 7.649999999999991\n",
      "mean_logits tensor([-0.9478, -0.9985, -0.9802, -0.9957, -0.8688], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9755, -0.9729, -0.9674, -0.9538, -0.9759])\n",
      "mean: tensor(-0.9691)\n",
      "iter_dt 6501.36ms; iter 54: train loss 0.01922 temperature: 7.69999999999999\n",
      "mean_logits tensor([-1.0365, -0.8911, -0.8573, -0.9944, -1.0215], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9384, -0.9768, -0.9941, -0.9940, -0.9695])\n",
      "mean: tensor(-0.9746)\n",
      "iter_dt 6474.71ms; iter 55: train loss 0.05126 temperature: 7.74999999999999\n",
      "mean_logits tensor([-1.0186, -0.9986, -1.0579, -1.0386, -0.9487], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9372, -0.9406, -0.9559, -0.9381, -0.9872])\n",
      "mean: tensor(-0.9518)\n",
      "iter_dt 6497.38ms; iter 56: train loss 0.04629 temperature: 7.79999999999999\n",
      "mean_logits tensor([-0.9174, -0.9164, -0.8739, -0.9424, -1.0588], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8728, -0.9952, -0.9515, -0.9132, -0.9518])\n",
      "mean: tensor(-0.9369)\n",
      "iter_dt 6436.84ms; iter 57: train loss 0.03649 temperature: 7.84999999999999\n",
      "mean_logits tensor([-0.9000, -1.0243, -1.0237, -0.9673, -0.9078], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9513, -0.9787, -0.9923, -0.9921, -0.9848])\n",
      "mean: tensor(-0.9798)\n",
      "iter_dt 6431.13ms; iter 58: train loss 0.01666 temperature: 7.89999999999999\n",
      "mean_logits tensor([-0.9914, -0.9345, -0.9274, -0.9256, -1.0057], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9391, -0.9821, -0.9673, -0.9142, -0.9160])\n",
      "mean: tensor(-0.9437)\n",
      "iter_dt 6460.73ms; iter 59: train loss 0.02017 temperature: 7.9499999999999895\n",
      "mean_logits tensor([-1.0912, -1.0455, -0.9986, -0.9760, -1.0044], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9828, -0.9665, -0.9473, -0.9919, -0.9422])\n",
      "mean: tensor(-0.9661)\n",
      "iter_dt 6532.27ms; iter 60: train loss 0.03750 temperature: 7.999999999999989\n",
      "mean_logits tensor([-0.9400, -1.0009, -0.9729, -0.9500, -0.9052], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9708, -0.9669, -0.9908, -0.9895, -0.9842])\n",
      "mean: tensor(-0.9804)\n",
      "iter_dt 6608.15ms; iter 61: train loss 0.01383 temperature: 8.04999999999999\n",
      "mean_logits tensor([-0.8815, -1.0385, -0.9249, -0.9304, -0.9364], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9904, -0.9944, -0.9735, -0.9519, -0.9785])\n",
      "mean: tensor(-0.9777)\n",
      "iter_dt 6491.83ms; iter 62: train loss 0.02456 temperature: 8.09999999999999\n",
      "mean_logits tensor([-0.9818, -1.0819, -0.8622, -0.9370, -0.9675], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9884, -0.9498, -0.9137, -0.9658, -0.9876])\n",
      "mean: tensor(-0.9610)\n",
      "iter_dt 6403.76ms; iter 63: train loss 0.03153 temperature: 8.149999999999991\n",
      "mean_logits tensor([-0.9701, -0.9225, -1.0788, -0.8582, -0.8956], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9436, -0.9913, -0.9548, -0.9890, -0.9434])\n",
      "mean: tensor(-0.9644)\n",
      "iter_dt 6497.68ms; iter 64: train loss 0.05550 temperature: 8.199999999999992\n",
      "mean_logits tensor([-0.9341, -0.9744, -1.0534, -0.9008, -0.8494], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9598, -0.9880, -0.9891, -0.9395, -0.9851])\n",
      "mean: tensor(-0.9723)\n",
      "iter_dt 6418.02ms; iter 65: train loss 0.03250 temperature: 8.249999999999993\n",
      "mean_logits tensor([-0.9112, -0.9116, -1.0462, -1.0244, -0.9976], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9851, -0.9515, -0.8644, -0.9599, -0.9841])\n",
      "mean: tensor(-0.9490)\n",
      "iter_dt 6398.92ms; iter 66: train loss 0.06042 temperature: 8.299999999999994\n",
      "mean_logits tensor([-1.0220, -1.0198, -0.8008, -1.0289, -0.9660], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9362, -0.9446, -0.9839, -0.9940, -0.9084])\n",
      "mean: tensor(-0.9534)\n",
      "iter_dt 6496.57ms; iter 67: train loss 0.06472 temperature: 8.349999999999994\n",
      "mean_logits tensor([-0.9085, -1.0301, -1.0234, -0.9287, -0.9614], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9933, -0.9223, -0.9931, -0.9659, -0.9678])\n",
      "mean: tensor(-0.9685)\n",
      "iter_dt 6490.42ms; iter 68: train loss 0.02932 temperature: 8.399999999999995\n",
      "mean_logits tensor([-1.0597, -0.9872, -0.9142, -0.9945, -0.9693], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9877, -0.9269, -0.9828, -0.9428, -0.9924])\n",
      "mean: tensor(-0.9665)\n",
      "iter_dt 6414.96ms; iter 69: train loss 0.02370 temperature: 8.449999999999996\n",
      "mean_logits tensor([-0.9730, -1.0519, -0.8647, -0.9726, -0.9885], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9777, -0.9880, -0.9758, -0.9903, -0.9662])\n",
      "mean: tensor(-0.9796)\n",
      "iter_dt 6444.07ms; iter 70: train loss 0.02302 temperature: 8.499999999999996\n",
      "mean_logits tensor([-0.9733, -0.9625, -0.9440, -0.9146, -0.9586], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9622, -0.9820, -0.9904, -0.9525, -0.9867])\n",
      "mean: tensor(-0.9748)\n",
      "iter_dt 6436.46ms; iter 71: train loss 0.00664 temperature: 8.549999999999997\n",
      "mean_logits tensor([-0.9475, -0.9337, -0.9597, -0.9332, -0.9888], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9912, -0.9745, -0.9781, -0.9923, -0.9348])\n",
      "mean: tensor(-0.9742)\n",
      "iter_dt 6465.72ms; iter 72: train loss 0.01415 temperature: 8.599999999999998\n",
      "mean_logits tensor([-1.0234, -1.0352, -0.9732, -0.9326, -1.0514], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9354, -0.9424, -0.9546, -0.9316, -0.9153])\n",
      "mean: tensor(-0.9359)\n",
      "iter_dt 6502.59ms; iter 73: train loss 0.05043 temperature: 8.649999999999999\n",
      "mean_logits tensor([-1.0063, -1.0382, -0.9340, -0.9309, -0.9649], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9948, -0.8813, -0.9549, -0.9873, -0.9164])\n",
      "mean: tensor(-0.9469)\n",
      "iter_dt 6544.71ms; iter 74: train loss 0.04184 temperature: 8.7\n",
      "mean_logits tensor([-1.0200, -0.9474, -0.9970, -0.8745, -0.9932], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9820, -0.8538, -0.9739, -0.9614, -0.9719])\n",
      "mean: tensor(-0.9486)\n",
      "iter_dt 6458.88ms; iter 75: train loss 0.02365 temperature: 8.75\n",
      "mean_logits tensor([-0.9808, -0.9389, -1.0455, -0.9603, -0.9778], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9910, -0.9912, -0.9591, -0.9666, -0.9974])\n",
      "mean: tensor(-0.9811)\n",
      "iter_dt 6551.11ms; iter 76: train loss 0.01562 temperature: 8.8\n",
      "mean_logits tensor([-0.9570, -1.0601, -1.0371, -1.0076, -0.9901], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9919, -0.9822, -0.9331, -0.9460, -0.9180])\n",
      "mean: tensor(-0.9542)\n",
      "iter_dt 6563.56ms; iter 77: train loss 0.03899 temperature: 8.850000000000001\n",
      "mean_logits tensor([-0.9537, -0.9578, -1.0126, -0.9041, -0.8970], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9953, -0.9850, -0.9693, -0.9458, -0.9883])\n",
      "mean: tensor(-0.9768)\n",
      "iter_dt 6481.55ms; iter 78: train loss 0.01940 temperature: 8.900000000000002\n",
      "mean_logits tensor([-0.9733, -0.9815, -0.9828, -0.9140, -0.9103], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9380, -0.9537, -0.9210, -0.9913, -0.9867])\n",
      "mean: tensor(-0.9582)\n",
      "iter_dt 6550.94ms; iter 79: train loss 0.02372 temperature: 8.950000000000003\n",
      "mean_logits tensor([-0.9419, -0.8749, -0.8482, -1.0511, -0.9341], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9514, -0.9883, -0.9544, -0.9436, -0.9883])\n",
      "mean: tensor(-0.9652)\n",
      "iter_dt 6749.32ms; iter 80: train loss 0.05146 temperature: 9.000000000000004\n",
      "mean_logits tensor([-0.9106, -0.9049, -0.9211, -0.9287, -0.9834], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9660, -0.9577, -0.9754, -0.9426, -0.9657])\n",
      "mean: tensor(-0.9615)\n",
      "iter_dt 6610.06ms; iter 81: train loss 0.01222 temperature: 9.050000000000004\n",
      "mean_logits tensor([-0.9207, -0.9411, -0.8818, -0.9831, -0.9849], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9509, -0.9849, -0.9601, -0.9907, -0.9582])\n",
      "mean: tensor(-0.9689)\n",
      "iter_dt 6506.59ms; iter 82: train loss 0.01264 temperature: 9.100000000000005\n",
      "mean_logits tensor([-0.8986, -1.0259, -0.9694, -0.9994, -0.9449], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9712, -0.9779, -0.9492, -0.9536, -0.9868])\n",
      "mean: tensor(-0.9677)\n",
      "iter_dt 6490.94ms; iter 83: train loss 0.01620 temperature: 9.150000000000006\n",
      "mean_logits tensor([-0.9987, -1.0469, -0.9827, -0.9527, -1.0075], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9533, -0.9940, -0.9767, -0.9289, -0.9412])\n",
      "mean: tensor(-0.9588)\n",
      "iter_dt 6554.71ms; iter 84: train loss 0.01417 temperature: 9.200000000000006\n",
      "mean_logits tensor([-1.0279, -0.9845, -0.8931, -0.8994, -0.9759], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9709, -0.9897, -0.9932, -0.9529, -0.9445])\n",
      "mean: tensor(-0.9702)\n",
      "iter_dt 6500.82ms; iter 85: train loss 0.02306 temperature: 9.250000000000007\n",
      "mean_logits tensor([-1.0836, -1.0526, -0.9114, -0.9780, -0.9809], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8323, -0.9963, -0.9884, -0.9579, -0.9855])\n",
      "mean: tensor(-0.9521)\n",
      "iter_dt 6500.66ms; iter 86: train loss 0.09971 temperature: 9.300000000000008\n",
      "mean_logits tensor([-0.9880, -0.9328, -1.0650, -1.0041, -0.9468], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9747, -0.9395, -0.9814, -0.9701, -0.9583])\n",
      "mean: tensor(-0.9648)\n",
      "iter_dt 6494.31ms; iter 87: train loss 0.01296 temperature: 9.350000000000009\n",
      "mean_logits tensor([-1.0168, -1.0462, -0.9625, -0.9384, -0.9702], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8412, -0.9506, -0.8536, -0.9581, -0.9909])\n",
      "mean: tensor(-0.9189)\n",
      "iter_dt 6497.22ms; iter 88: train loss 0.06882 temperature: 9.40000000000001\n",
      "mean_logits tensor([-0.9825, -0.8803, -0.9985, -1.0243, -1.0310], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9587, -0.9286, -0.9847, -0.9201, -0.9727])\n",
      "mean: tensor(-0.9530)\n",
      "iter_dt 6495.98ms; iter 89: train loss 0.02416 temperature: 9.45000000000001\n",
      "mean_logits tensor([-0.9610, -0.8723, -1.0105, -0.9298, -0.8507], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9854, -0.8526, -0.9848, -0.9795, -0.9969])\n",
      "mean: tensor(-0.9599)\n",
      "iter_dt 6443.89ms; iter 90: train loss 0.03274 temperature: 9.50000000000001\n",
      "mean_logits tensor([-0.9239, -0.9802, -0.9382, -0.9470, -0.9749], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9067, -0.9775, -0.9181, -0.9233, -0.9903])\n",
      "mean: tensor(-0.9432)\n",
      "iter_dt 6461.07ms; iter 91: train loss 0.00196 temperature: 9.550000000000011\n",
      "mean_logits tensor([-0.9395, -1.0150, -0.9955, -0.9584, -0.9326], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9770, -0.9968, -0.9725, -0.9398, -0.9910])\n",
      "mean: tensor(-0.9754)\n",
      "iter_dt 6659.38ms; iter 92: train loss 0.00829 temperature: 9.600000000000012\n",
      "mean_logits tensor([-0.9987, -0.9913, -1.0697, -1.0240, -0.9742], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9744, -0.9713, -0.9498, -0.9888, -0.9820])\n",
      "mean: tensor(-0.9733)\n",
      "iter_dt 6571.69ms; iter 93: train loss 0.02507 temperature: 9.650000000000013\n",
      "mean_logits tensor([-1.0108, -1.0120, -0.9946, -0.9562, -0.9258], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9889, -0.9821, -0.9937, -0.8872, -0.9847])\n",
      "mean: tensor(-0.9673)\n",
      "iter_dt 6420.69ms; iter 94: train loss 0.01272 temperature: 9.700000000000014\n",
      "mean_logits tensor([-0.9528, -0.8414, -0.8989, -0.9956, -0.9660], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9902, -0.9650, -0.9881, -0.9709, -0.9761])\n",
      "mean: tensor(-0.9780)\n",
      "iter_dt 6468.96ms; iter 95: train loss 0.03209 temperature: 9.750000000000014\n",
      "mean_logits tensor([-0.9793, -0.9780, -0.9220, -0.9944, -0.9977], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9905, -0.9474, -0.9831, -0.9459, -0.9933])\n",
      "mean: tensor(-0.9720)\n",
      "iter_dt 6391.05ms; iter 96: train loss 0.00977 temperature: 9.800000000000015\n",
      "mean_logits tensor([-0.9278, -1.0362, -0.9959, -1.0110, -0.9476], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9783, -0.9556, -0.9196, -0.9913, -0.9900])\n",
      "mean: tensor(-0.9670)\n",
      "iter_dt 6458.38ms; iter 97: train loss 0.02395 temperature: 9.850000000000016\n",
      "mean_logits tensor([-0.9382, -1.0030, -0.9349, -1.0030, -0.9402], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9327, -0.9648, -0.9849, -0.9963, -0.9726])\n",
      "mean: tensor(-0.9703)\n",
      "iter_dt 6377.08ms; iter 98: train loss 0.00702 temperature: 9.900000000000016\n",
      "mean_logits tensor([-0.9325, -0.9666, -0.9183, -1.0673, -0.9957], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9760, -0.9234, -0.9664, -0.9967, -0.9911])\n",
      "mean: tensor(-0.9707)\n",
      "iter_dt 6379.84ms; iter 99: train loss 0.01594 temperature: 9.950000000000017\n",
      "converged SCF energy = -0.783792654277353\n",
      "Starting to parse FermionOperator using 4 qubits...\n",
      "\n",
      "Operator t:  -0.5339363487727404 [] +\n",
      "-0.0647846187202642 [X0 X1 Y2 Y3] +\n",
      "0.0647846187202642 [X0 Y1 Y2 X3] +\n",
      "0.0647846187202642 [Y0 X1 X2 Y3] +\n",
      "-0.0647846187202642 [Y0 Y1 X2 X3] +\n",
      "0.06727930458983414 [Z0] +\n",
      "0.12736570310657463 [Z0 Z1] +\n",
      "0.06501569581211997 [Z0 Z2] +\n",
      "0.12980031453238416 [Z0 Z3] +\n",
      "0.06727930458983411 [Z1] +\n",
      "0.12980031453238416 [Z1 Z2] +\n",
      "0.06501569581211997 [Z1 Z3] +\n",
      "0.006651295687574332 [Z2] +\n",
      "0.13366602988233994 [Z2 Z3] +\n",
      "0.006651295687574332 [Z3]\n",
      "Term, coeff:  () -0.5339363487727404\n",
      "Term, coeff:  ((0, 'Z'),) 0.06727930458983414\n",
      "Index, p_char:  0 Z\n",
      "Term, coeff:  ((1, 'Z'),) 0.06727930458983411\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((2, 'Z'),) 0.006651295687574332\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((3, 'Z'),) 0.006651295687574332\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((0, 'Z'), (1, 'Z')) 0.12736570310657463\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  1 Z\n",
      "Term, coeff:  ((0, 'Y'), (1, 'X'), (2, 'X'), (3, 'Y')) 0.0647846187202642\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'Y'), (1, 'Y'), (2, 'X'), (3, 'X')) -0.0647846187202642\n",
      "Index, p_char:  0 Y\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 X\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'X'), (1, 'X'), (2, 'Y'), (3, 'Y')) -0.0647846187202642\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 X\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 Y\n",
      "Term, coeff:  ((0, 'X'), (1, 'Y'), (2, 'Y'), (3, 'X')) 0.0647846187202642\n",
      "Index, p_char:  0 X\n",
      "Index, p_char:  1 Y\n",
      "Index, p_char:  2 Y\n",
      "Index, p_char:  3 X\n",
      "Term, coeff:  ((0, 'Z'), (2, 'Z')) 0.06501569581211997\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((0, 'Z'), (3, 'Z')) 0.12980031453238416\n",
      "Index, p_char:  0 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((1, 'Z'), (2, 'Z')) 0.12980031453238416\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  2 Z\n",
      "Term, coeff:  ((1, 'Z'), (3, 'Z')) 0.06501569581211997\n",
      "Index, p_char:  1 Z\n",
      "Index, p_char:  3 Z\n",
      "Term, coeff:  ((2, 'Z'), (3, 'Z')) 0.13366602988233994\n",
      "Index, p_char:  2 Z\n",
      "Index, p_char:  3 Z\n",
      "ground state: -0.9486411121761863\n",
      "hf state: -0.7837926542773537\n",
      "number of parameters: 85.29M\n",
      "running on device cpu\n",
      "mean_logits tensor([-1.1676, -0.9894, -1.0338, -1.0321, -1.2728], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8606, -0.7916, -0.8837, -0.8921, -0.6595])\n",
      "mean: tensor(-0.8175)\n",
      "iter_dt 0.00ms; iter 0: train loss 0.78464 temperature: 5\n",
      "mean_logits tensor([-0.9121, -1.4357, -1.0989, -0.9236, -0.9343], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9142, -0.8330, -0.8570, -0.8790, -0.7688])\n",
      "mean: tensor(-0.8504)\n",
      "iter_dt 1694973364514.61ms; iter 1: train loss 0.83948 temperature: 5.05\n",
      "mean_logits tensor([-0.9895, -0.9313, -1.1402, -1.2263, -0.9595], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9421, -0.9422, -0.8595, -0.9381, -0.8740])\n",
      "mean: tensor(-0.9112)\n",
      "iter_dt 6633.43ms; iter 2: train loss 0.27522 temperature: 5.1\n",
      "mean_logits tensor([-1.0765, -0.8967, -1.0768, -1.0464, -0.9755], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9427, -0.8782, -0.9279, -0.8193, -0.8879])\n",
      "mean: tensor(-0.8912)\n",
      "iter_dt 6641.43ms; iter 3: train loss 0.13724 temperature: 5.1499999999999995\n",
      "mean_logits tensor([-0.7921, -0.9792, -1.0733, -1.0447, -1.1075], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9220, -0.9275, -0.9234, -0.8816, -0.9285])\n",
      "mean: tensor(-0.9166)\n",
      "iter_dt 6537.25ms; iter 4: train loss 0.14140 temperature: 5.199999999999999\n",
      "mean_logits tensor([-0.8513, -0.7303, -0.8811, -1.0251, -0.8275], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8321, -0.8206, -0.8506, -0.9143, -0.9404])\n",
      "mean: tensor(-0.8716)\n",
      "iter_dt 6590.01ms; iter 5: train loss 0.04121 temperature: 5.249999999999999\n",
      "mean_logits tensor([-0.9436, -0.8011, -0.8131, -0.8118, -0.8608], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8422, -0.9366, -0.7716, -0.8402, -0.9013])\n",
      "mean: tensor(-0.8584)\n",
      "iter_dt 6487.52ms; iter 6: train loss 0.03761 temperature: 5.299999999999999\n",
      "mean_logits tensor([-0.8237, -0.8965, -0.9173, -0.9411, -0.9788], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8433, -0.8530, -0.9426, -0.8453, -0.8363])\n",
      "mean: tensor(-0.8641)\n",
      "iter_dt 6581.87ms; iter 7: train loss 0.03934 temperature: 5.349999999999999\n",
      "mean_logits tensor([-0.7547, -0.8501, -0.6400, -0.8316, -0.8816], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9155, -0.8356, -0.8971, -0.8402, -0.6902])\n",
      "mean: tensor(-0.8357)\n",
      "iter_dt 6562.53ms; iter 8: train loss 0.12506 temperature: 5.399999999999999\n",
      "mean_logits tensor([-0.7053, -0.7599, -0.8197, -0.9055, -0.8598], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8262, -0.9351, -0.7016, -0.8271, -0.8868])\n",
      "mean: tensor(-0.8354)\n",
      "iter_dt 6432.79ms; iter 9: train loss 0.06763 temperature: 5.449999999999998\n",
      "mean_logits tensor([-0.8588, -0.8685, -0.7415, -0.8381, -0.8466], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8740, -0.8526, -0.7693, -0.8040, -0.8648])\n",
      "mean: tensor(-0.8329)\n",
      "iter_dt 6409.17ms; iter 10: train loss 0.00281 temperature: 5.499999999999998\n",
      "mean_logits tensor([-0.9188, -0.8646, -0.7858, -0.8521, -1.0318], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9223, -0.8680, -0.9138, -0.7755, -0.8894])\n",
      "mean: tensor(-0.8738)\n",
      "iter_dt 6368.25ms; iter 11: train loss 0.05174 temperature: 5.549999999999998\n",
      "mean_logits tensor([-0.9445, -0.7568, -0.8267, -0.9205, -0.9190], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9026, -0.7519, -0.9361, -0.8594, -0.8689])\n",
      "mean: tensor(-0.8638)\n",
      "iter_dt 6401.81ms; iter 12: train loss 0.02363 temperature: 5.599999999999998\n",
      "mean_logits tensor([-0.8075, -0.7083, -0.8421, -0.8845, -0.8690], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.7967, -0.8196, -0.9452, -0.6405, -0.8875])\n",
      "mean: tensor(-0.8179)\n",
      "iter_dt 6410.89ms; iter 13: train loss 0.07960 temperature: 5.649999999999998\n",
      "mean_logits tensor([-0.8701, -0.8372, -0.7065, -0.7094, -0.9042], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9239, -0.8417, -0.8772, -0.8821, -0.8926])\n",
      "mean: tensor(-0.8835)\n",
      "iter_dt 6429.65ms; iter 14: train loss 0.06147 temperature: 5.6999999999999975\n",
      "mean_logits tensor([-0.7902, -0.9147, -0.8082, -0.8412, -0.7194], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8352, -0.8567, -0.8028, -0.8816, -0.9422])\n",
      "mean: tensor(-0.8637)\n",
      "iter_dt 6390.73ms; iter 15: train loss 0.06042 temperature: 5.749999999999997\n",
      "mean_logits tensor([-0.8477, -0.8028, -0.7411, -0.7066, -0.6701], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9271, -0.9109, -0.7663, -0.8878, -0.9358])\n",
      "mean: tensor(-0.8856)\n",
      "iter_dt 6413.12ms; iter 16: train loss 0.12415 temperature: 5.799999999999997\n",
      "mean_logits tensor([-0.8316, -0.8118, -0.7989, -0.9254, -0.8391], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.7349, -0.9231, -0.9287, -0.7941, -0.9404])\n",
      "mean: tensor(-0.8643)\n",
      "iter_dt 6393.15ms; iter 17: train loss 0.07348 temperature: 5.849999999999997\n",
      "mean_logits tensor([-0.9373, -0.9510, -0.9609, -0.9168, -0.9580], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.7913, -0.9365, -0.9258, -0.8821, -0.8862])\n",
      "mean: tensor(-0.8844)\n",
      "iter_dt 6415.95ms; iter 18: train loss 0.03396 temperature: 5.899999999999997\n",
      "mean_logits tensor([-0.9900, -0.9491, -0.8664, -0.9352, -0.9206], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9448, -0.9445, -0.8833, -0.9474, -0.7561])\n",
      "mean: tensor(-0.8952)\n",
      "iter_dt 6421.30ms; iter 19: train loss 0.03241 temperature: 5.949999999999997\n",
      "mean_logits tensor([-1.0034, -0.9134, -0.9624, -0.8962, -1.0337], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9099, -0.8984, -0.9182, -0.9426, -0.9318])\n",
      "mean: tensor(-0.9202)\n",
      "iter_dt 6440.88ms; iter 20: train loss 0.03222 temperature: 5.9999999999999964\n",
      "mean_logits tensor([-0.9917, -0.8897, -0.8376, -0.9256, -0.9612], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9132, -0.9248, -0.9438, -0.8084, -0.9328])\n",
      "mean: tensor(-0.9046)\n",
      "iter_dt 6405.10ms; iter 21: train loss 0.03984 temperature: 6.049999999999996\n",
      "mean_logits tensor([-0.8020, -1.0235, -0.8522, -0.8478, -0.9944], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9106, -0.7982, -0.9345, -0.9349, -0.9094])\n",
      "mean: tensor(-0.8975)\n",
      "iter_dt 6386.76ms; iter 22: train loss 0.10290 temperature: 6.099999999999996\n",
      "mean_logits tensor([-0.9449, -0.9250, -0.8969, -0.8924, -0.8941], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.6959, -0.9226, -0.9091, -0.9000, -0.9027])\n",
      "mean: tensor(-0.8660)\n",
      "iter_dt 6391.77ms; iter 23: train loss 0.06465 temperature: 6.149999999999996\n",
      "mean_logits tensor([-1.1044, -0.9483, -0.8820, -0.9332, -0.8540], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9373, -0.8596, -0.9323, -0.9278, -0.8992])\n",
      "mean: tensor(-0.9112)\n",
      "iter_dt 6370.99ms; iter 24: train loss 0.05821 temperature: 6.199999999999996\n",
      "mean_logits tensor([-0.8525, -0.7512, -0.8934, -0.8200, -0.8524], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8242, -0.9391, -0.8770, -0.9352, -0.9079])\n",
      "mean: tensor(-0.8967)\n",
      "iter_dt 6466.49ms; iter 25: train loss 0.05854 temperature: 6.249999999999996\n",
      "mean_logits tensor([-1.0006, -0.8086, -0.8894, -0.9669, -0.8548], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9426, -0.8896, -0.8993, -0.9432, -0.8918])\n",
      "mean: tensor(-0.9133)\n",
      "iter_dt 6339.83ms; iter 26: train loss 0.01431 temperature: 6.299999999999995\n",
      "mean_logits tensor([-0.8187, -0.7845, -0.9650, -0.8971, -0.8927], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8729, -0.9238, -0.9034, -0.9030, -0.8839])\n",
      "mean: tensor(-0.8974)\n",
      "iter_dt 6421.91ms; iter 27: train loss 0.02968 temperature: 6.349999999999995\n",
      "mean_logits tensor([-0.7363, -0.8507, -0.9495, -0.9300, -0.8764], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9233, -0.9167, -0.9438, -0.9310, -0.9182])\n",
      "mean: tensor(-0.9266)\n",
      "iter_dt 6479.65ms; iter 28: train loss 0.04412 temperature: 6.399999999999995\n",
      "mean_logits tensor([-0.8634, -0.9327, -0.7469, -1.0077, -0.8911], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8471, -0.9389, -0.9428, -0.9452, -0.9420])\n",
      "mean: tensor(-0.9232)\n",
      "iter_dt 6441.02ms; iter 29: train loss 0.05083 temperature: 6.449999999999995\n",
      "mean_logits tensor([-0.9759, -0.9062, -0.8069, -0.9407, -0.8264], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9315, -0.8812, -0.9323, -0.9100, -0.9356])\n",
      "mean: tensor(-0.9181)\n",
      "iter_dt 6439.74ms; iter 30: train loss 0.03644 temperature: 6.499999999999995\n",
      "mean_logits tensor([-0.9209, -1.0542, -1.0195, -0.9491, -0.9938], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9037, -0.6350, -0.9336, -0.9213, -0.9175])\n",
      "mean: tensor(-0.8622)\n",
      "iter_dt 6442.24ms; iter 31: train loss 0.21285 temperature: 6.5499999999999945\n",
      "mean_logits tensor([-1.0384, -0.9320, -0.9960, -1.1429, -1.0092], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8464, -0.7512, -0.9281, -0.8335, -0.9203])\n",
      "mean: tensor(-0.8559)\n",
      "iter_dt 6434.03ms; iter 32: train loss 0.24051 temperature: 6.599999999999994\n",
      "mean_logits tensor([-0.9376, -0.9410, -1.2254, -1.0928, -0.9577], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8378, -0.9397, -0.8822, -0.9272, -0.9440])\n",
      "mean: tensor(-0.9062)\n",
      "iter_dt 6420.54ms; iter 33: train loss 0.24914 temperature: 6.649999999999994\n",
      "mean_logits tensor([-1.0544, -0.8584, -0.9131, -0.9755, -0.9587], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9460, -0.9344, -0.9118, -0.9391, -0.9433])\n",
      "mean: tensor(-0.9349)\n",
      "iter_dt 6472.28ms; iter 34: train loss 0.02644 temperature: 6.699999999999994\n",
      "mean_logits tensor([-0.9334, -0.8814, -0.7373, -0.9773, -0.9900], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9431, -0.9462, -0.9194, -0.9455, -0.9478])\n",
      "mean: tensor(-0.9404)\n",
      "iter_dt 6483.77ms; iter 35: train loss 0.04405 temperature: 6.749999999999994\n",
      "mean_logits tensor([-0.9030, -0.9492, -0.8736, -0.9525, -1.0585], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9288, -0.9068, -0.9427, -0.9229, -0.9362])\n",
      "mean: tensor(-0.9275)\n",
      "iter_dt 6486.03ms; iter 36: train loss 0.03215 temperature: 6.799999999999994\n",
      "mean_logits tensor([-1.0472, -0.9604, -0.8814, -0.8858, -0.9323], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8744, -0.9238, -0.8878, -0.9063, -0.9176])\n",
      "mean: tensor(-0.9020)\n",
      "iter_dt 6402.91ms; iter 37: train loss 0.04352 temperature: 6.849999999999993\n",
      "mean_logits tensor([-0.9254, -1.0075, -0.9188, -0.9136, -0.9394], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9362, -0.8634, -0.9148, -0.8729, -0.9152])\n",
      "mean: tensor(-0.9005)\n",
      "iter_dt 6463.90ms; iter 38: train loss 0.02991 temperature: 6.899999999999993\n",
      "mean_logits tensor([-0.6712, -0.9477, -0.8779, -0.8822, -0.9258], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.7636, -0.8845, -0.9293, -0.8775, -0.9239])\n",
      "mean: tensor(-0.8758)\n",
      "iter_dt 6453.27ms; iter 39: train loss 0.01542 temperature: 6.949999999999993\n",
      "mean_logits tensor([-0.9835, -0.9448, -1.0101, -0.8225, -0.9997], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9390, -0.8771, -0.8808, -0.9188, -0.9431])\n",
      "mean: tensor(-0.9118)\n",
      "iter_dt 6408.62ms; iter 40: train loss 0.04559 temperature: 6.999999999999993\n",
      "mean_logits tensor([-0.8346, -0.9467, -0.7287, -0.8914, -0.9763], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9160, -0.9476, -0.9431, -0.9371, -0.9343])\n",
      "mean: tensor(-0.9356)\n",
      "iter_dt 6407.09ms; iter 41: train loss 0.06173 temperature: 7.049999999999993\n",
      "mean_logits tensor([-0.9119, -0.9096, -0.9476, -0.9227, -0.8369], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8983, -0.9393, -0.9074, -0.9248, -0.9346])\n",
      "mean: tensor(-0.9209)\n",
      "iter_dt 6382.45ms; iter 42: train loss 0.01465 temperature: 7.0999999999999925\n",
      "mean_logits tensor([-0.9473, -0.9412, -0.8334, -0.7914, -0.8362], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8791, -0.9206, -0.9019, -0.8924, -0.8381])\n",
      "mean: tensor(-0.8864)\n",
      "iter_dt 6400.69ms; iter 43: train loss 0.02266 temperature: 7.149999999999992\n",
      "mean_logits tensor([-0.8150, -0.8690, -0.8906, -0.9284, -0.8833], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9060, -0.9423, -0.9052, -0.9410, -0.9073])\n",
      "mean: tensor(-0.9203)\n",
      "iter_dt 6390.86ms; iter 44: train loss 0.01697 temperature: 7.199999999999992\n",
      "mean_logits tensor([-0.9482, -0.9023, -0.8788, -1.0528, -0.9250], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9117, -0.9295, -0.9022, -0.8547, -0.9178])\n",
      "mean: tensor(-0.9032)\n",
      "iter_dt 6401.94ms; iter 45: train loss 0.05642 temperature: 7.249999999999992\n",
      "mean_logits tensor([-0.8656, -0.9816, -0.8693, -1.0120, -0.9826], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9377, -0.8932, -0.9108, -0.8494, -0.9443])\n",
      "mean: tensor(-0.9071)\n",
      "iter_dt 6383.31ms; iter 46: train loss 0.05463 temperature: 7.299999999999992\n",
      "mean_logits tensor([-0.8748, -0.9830, -0.9252, -0.8665, -0.9472], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9219, -0.8936, -0.9019, -0.8476, -0.9137])\n",
      "mean: tensor(-0.8958)\n",
      "iter_dt 6408.64ms; iter 47: train loss 0.01563 temperature: 7.349999999999992\n",
      "mean_logits tensor([-0.9644, -0.8849, -0.8958, -0.8955, -0.7473], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.7958, -0.9467, -0.8394, -0.9452, -0.9341])\n",
      "mean: tensor(-0.8923)\n",
      "iter_dt 6416.49ms; iter 48: train loss 0.08222 temperature: 7.3999999999999915\n",
      "mean_logits tensor([-0.9105, -0.8456, -0.8842, -0.8513, -0.9075], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8815, -0.8355, -0.8842, -0.8743, -0.9282])\n",
      "mean: tensor(-0.8807)\n",
      "iter_dt 6401.26ms; iter 49: train loss 0.00225 temperature: 7.449999999999991\n",
      "mean_logits tensor([-0.8629, -0.7972, -0.9601, -1.1058, -0.9071], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8907, -0.7705, -0.9317, -0.8772, -0.9022])\n",
      "mean: tensor(-0.8745)\n",
      "iter_dt 6365.98ms; iter 50: train loss 0.07890 temperature: 7.499999999999991\n",
      "mean_logits tensor([-0.9728, -0.9272, -0.9617, -0.9381, -0.9686], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8828, -0.9194, -0.9354, -0.9237, -0.9328])\n",
      "mean: tensor(-0.9188)\n",
      "iter_dt 6428.33ms; iter 51: train loss 0.01336 temperature: 7.549999999999991\n",
      "mean_logits tensor([-0.9778, -0.9807, -0.8754, -1.0114, -0.9831], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8219, -0.9376, -0.9380, -0.9199, -0.8436])\n",
      "mean: tensor(-0.8922)\n",
      "iter_dt 6410.49ms; iter 52: train loss 0.07259 temperature: 7.599999999999991\n",
      "mean_logits tensor([-0.9752, -0.8438, -0.8405, -0.8786, -0.8636], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9289, -0.9224, -0.9005, -0.9282, -0.9364])\n",
      "mean: tensor(-0.9233)\n",
      "iter_dt 6405.75ms; iter 53: train loss 0.02364 temperature: 7.649999999999991\n",
      "mean_logits tensor([-0.8506, -1.0347, -0.9532, -0.9292, -0.8858], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9448, -0.8626, -0.9449, -0.9398, -0.8574])\n",
      "mean: tensor(-0.9099)\n",
      "iter_dt 6511.08ms; iter 54: train loss 0.05146 temperature: 7.69999999999999\n",
      "mean_logits tensor([-0.8675, -0.8697, -0.9092, -0.9280, -0.8878], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8720, -0.9302, -0.9138, -0.9427, -0.9400])\n",
      "mean: tensor(-0.9197)\n",
      "iter_dt 6411.90ms; iter 55: train loss 0.00813 temperature: 7.74999999999999\n",
      "mean_logits tensor([-0.9058, -0.9818, -0.9927, -0.9502, -0.8999], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9320, -0.8776, -0.8519, -0.9411, -0.9321])\n",
      "mean: tensor(-0.9069)\n",
      "iter_dt 6466.04ms; iter 56: train loss 0.04135 temperature: 7.79999999999999\n",
      "mean_logits tensor([-0.8647, -0.8822, -0.8655, -0.8974, -0.8339], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9137, -0.9396, -0.8958, -0.8878, -0.9301])\n",
      "mean: tensor(-0.9134)\n",
      "iter_dt 6404.66ms; iter 57: train loss 0.01893 temperature: 7.84999999999999\n",
      "mean_logits tensor([-0.8573, -0.9307, -0.9536, -0.8345, -0.8914], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9349, -0.9221, -0.9396, -0.8662, -0.9209])\n",
      "mean: tensor(-0.9167)\n",
      "iter_dt 6404.79ms; iter 58: train loss 0.00975 temperature: 7.89999999999999\n",
      "mean_logits tensor([-1.0436, -0.9824, -0.9391, -0.9893, -0.9915], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8264, -0.8747, -0.8631, -0.9317, -0.9215])\n",
      "mean: tensor(-0.8835)\n",
      "iter_dt 6377.23ms; iter 59: train loss 0.09451 temperature: 7.9499999999999895\n",
      "mean_logits tensor([-1.0630, -0.9492, -0.9231, -0.9483, -0.8808], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9270, -0.8907, -0.8856, -0.9007, -0.9363])\n",
      "mean: tensor(-0.9080)\n",
      "iter_dt 6395.13ms; iter 60: train loss 0.03983 temperature: 7.999999999999989\n",
      "mean_logits tensor([-0.8596, -0.8486, -0.8682, -0.9939, -1.0106], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8729, -0.9441, -0.9449, -0.9172, -0.9169])\n",
      "mean: tensor(-0.9192)\n",
      "iter_dt 6419.00ms; iter 61: train loss 0.03840 temperature: 8.04999999999999\n",
      "mean_logits tensor([-0.9414, -0.8377, -0.9067, -0.9123, -0.9999], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9282, -0.8201, -0.9443, -0.9030, -0.8882])\n",
      "mean: tensor(-0.8967)\n",
      "iter_dt 6528.06ms; iter 62: train loss 0.01896 temperature: 8.09999999999999\n",
      "mean_logits tensor([-0.9370, -0.9851, -0.9888, -0.8290, -0.9841], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9235, -0.8915, -0.8695, -0.8760, -0.9332])\n",
      "mean: tensor(-0.8987)\n",
      "iter_dt 6404.00ms; iter 63: train loss 0.03594 temperature: 8.149999999999991\n",
      "mean_logits tensor([-0.8961, -0.9087, -0.9965, -0.9487, -0.9101], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8532, -0.9261, -0.9087, -0.9393, -0.9325])\n",
      "mean: tensor(-0.9120)\n",
      "iter_dt 6395.48ms; iter 64: train loss 0.01363 temperature: 8.199999999999992\n",
      "mean_logits tensor([-0.9120, -0.9247, -0.9944, -0.8476, -0.9035], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8327, -0.8704, -0.9349, -0.8864, -0.8974])\n",
      "mean: tensor(-0.8844)\n",
      "iter_dt 6441.14ms; iter 65: train loss 0.01739 temperature: 8.249999999999993\n",
      "mean_logits tensor([-0.8301, -0.8815, -0.8601, -0.8099, -0.8284], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9430, -0.8510, -0.8361, -0.7293, -0.8850])\n",
      "mean: tensor(-0.8489)\n",
      "iter_dt 6419.64ms; iter 66: train loss 0.02632 temperature: 8.299999999999994\n",
      "mean_logits tensor([-0.8365, -0.9692, -0.9248, -0.8653, -0.9387], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9127, -0.8835, -0.8959, -0.9385, -0.8987])\n",
      "mean: tensor(-0.9058)\n",
      "iter_dt 6419.79ms; iter 67: train loss 0.02563 temperature: 8.349999999999994\n",
      "mean_logits tensor([-0.8566, -0.8613, -0.8358, -0.7800, -0.8584], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9340, -0.8470, -0.9352, -0.8834, -0.9226])\n",
      "mean: tensor(-0.9044)\n",
      "iter_dt 6394.28ms; iter 68: train loss 0.03523 temperature: 8.399999999999995\n",
      "mean_logits tensor([-0.8997, -0.7979, -0.8435, -0.9641, -0.9321], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9401, -0.9023, -0.7427, -0.9061, -0.9340])\n",
      "mean: tensor(-0.8850)\n",
      "iter_dt 6408.82ms; iter 69: train loss 0.02831 temperature: 8.449999999999996\n",
      "mean_logits tensor([-0.9716, -0.8585, -0.9119, -0.9613, -0.9801], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9049, -0.9452, -0.8930, -0.9105, -0.9318])\n",
      "mean: tensor(-0.9171)\n",
      "iter_dt 6403.96ms; iter 70: train loss 0.02190 temperature: 8.499999999999996\n",
      "mean_logits tensor([-0.8709, -0.9104, -0.7512, -0.9296, -0.9366], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9367, -0.9254, -0.9149, -0.9105, -0.8760])\n",
      "mean: tensor(-0.9127)\n",
      "iter_dt 6398.96ms; iter 71: train loss 0.03896 temperature: 8.549999999999997\n",
      "mean_logits tensor([-0.8611, -0.8716, -0.8397, -0.7182, -0.9191], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9139, -0.8827, -0.7666, -0.9120, -0.9466])\n",
      "mean: tensor(-0.8844)\n",
      "iter_dt 6406.25ms; iter 72: train loss 0.04819 temperature: 8.599999999999998\n",
      "mean_logits tensor([-0.9617, -0.9233, -0.8063, -0.9189, -0.9182], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9371, -0.9278, -0.9277, -0.8442, -0.9239])\n",
      "mean: tensor(-0.9121)\n",
      "iter_dt 6391.28ms; iter 73: train loss 0.02410 temperature: 8.649999999999999\n",
      "mean_logits tensor([-0.8454, -0.8296, -0.8615, -0.9819, -0.8659], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9420, -0.8767, -0.9100, -0.9401, -0.8630])\n",
      "mean: tensor(-0.9064)\n",
      "iter_dt 6396.53ms; iter 74: train loss 0.01878 temperature: 8.7\n",
      "mean_logits tensor([-1.0985, -0.9520, -1.0182, -0.9738, -0.8272], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9319, -0.9303, -0.9314, -0.8866, -0.8393])\n",
      "mean: tensor(-0.9039)\n",
      "iter_dt 6423.52ms; iter 75: train loss 0.06352 temperature: 8.75\n",
      "mean_logits tensor([-1.0215, -0.9402, -0.9256, -0.7982, -1.0538], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9360, -0.9237, -0.9139, -0.9383, -0.9263])\n",
      "mean: tensor(-0.9276)\n",
      "iter_dt 6409.37ms; iter 76: train loss 0.05681 temperature: 8.8\n",
      "mean_logits tensor([-0.9480, -0.9622, -0.9720, -0.9725, -0.8299], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8342, -0.9449, -0.9006, -0.9417, -0.8296])\n",
      "mean: tensor(-0.8902)\n",
      "iter_dt 6400.85ms; iter 77: train loss 0.02372 temperature: 8.850000000000001\n",
      "mean_logits tensor([-0.8838, -0.8312, -0.9491, -0.9798, -1.0265], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9232, -0.8864, -0.9481, -0.9197, -0.9447])\n",
      "mean: tensor(-0.9244)\n",
      "iter_dt 6424.71ms; iter 78: train loss 0.01972 temperature: 8.900000000000002\n",
      "mean_logits tensor([-0.8675, -1.0269, -1.0346, -0.9797, -0.9504], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9311, -0.9426, -0.8727, -0.8969, -0.9319])\n",
      "mean: tensor(-0.9151)\n",
      "iter_dt 6451.77ms; iter 79: train loss 0.05983 temperature: 8.950000000000003\n",
      "mean_logits tensor([-0.8703, -0.9722, -0.8964, -0.9991, -0.8284], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9307, -0.9486, -0.9248, -0.8713, -0.8949])\n",
      "mean: tensor(-0.9141)\n",
      "iter_dt 6625.64ms; iter 80: train loss 0.03236 temperature: 9.000000000000004\n",
      "mean_logits tensor([-0.9389, -0.9680, -0.8719, -0.8858, -0.9032], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9314, -0.9304, -0.9159, -0.9385, -0.9409])\n",
      "mean: tensor(-0.9314)\n",
      "iter_dt 6545.55ms; iter 81: train loss 0.00953 temperature: 9.050000000000004\n",
      "mean_logits tensor([-0.9648, -0.8992, -0.8937, -0.9725, -0.8518], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9320, -0.9477, -0.8923, -0.9252, -0.9368])\n",
      "mean: tensor(-0.9268)\n",
      "iter_dt 6518.48ms; iter 82: train loss 0.01605 temperature: 9.100000000000005\n",
      "mean_logits tensor([-0.8450, -1.0551, -0.8828, -0.9380, -0.8984], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9172, -0.9051, -0.9238, -0.9398, -0.9384])\n",
      "mean: tensor(-0.9249)\n",
      "iter_dt 6575.01ms; iter 83: train loss 0.04216 temperature: 9.150000000000006\n",
      "mean_logits tensor([-0.9937, -0.9003, -0.7883, -0.9254, -0.9292], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9484, -0.8782, -0.9330, -0.8798, -0.9418])\n",
      "mean: tensor(-0.9162)\n",
      "iter_dt 6502.71ms; iter 84: train loss 0.02962 temperature: 9.200000000000006\n",
      "mean_logits tensor([-0.9171, -0.9166, -0.8919, -0.8505, -0.9733], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9410, -0.8859, -0.9383, -0.9472, -0.9191])\n",
      "mean: tensor(-0.9263)\n",
      "iter_dt 6416.34ms; iter 85: train loss 0.01977 temperature: 9.250000000000007\n",
      "mean_logits tensor([-0.8870, -0.8676, -0.9217, -0.8495, -0.8533], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9167, -0.9161, -0.8629, -0.9295, -0.9419])\n",
      "mean: tensor(-0.9134)\n",
      "iter_dt 6419.15ms; iter 86: train loss 0.02504 temperature: 9.300000000000008\n",
      "mean_logits tensor([-0.9199, -0.9717, -0.8357, -0.9352, -0.8297], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8396, -0.9225, -0.8965, -0.9312, -0.9202])\n",
      "mean: tensor(-0.9020)\n",
      "iter_dt 6420.78ms; iter 87: train loss 0.02435 temperature: 9.350000000000009\n",
      "mean_logits tensor([-0.9979, -1.1175, -0.9421, -0.9504, -0.9333], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9391, -0.5946, -0.9134, -0.9073, -0.9455])\n",
      "mean: tensor(-0.8600)\n",
      "iter_dt 6407.28ms; iter 88: train loss 0.31847 temperature: 9.40000000000001\n",
      "mean_logits tensor([-1.0097, -1.0102, -0.9735, -0.9399, -0.9714], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9328, -0.6563, -0.9248, -0.8516, -0.8903])\n",
      "mean: tensor(-0.8512)\n",
      "iter_dt 6402.51ms; iter 89: train loss 0.16321 temperature: 9.45000000000001\n",
      "mean_logits tensor([-0.8841, -0.8984, -0.9349, -0.9346, -0.9746], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9297, -0.9172, -0.7339, -0.8857, -0.6607])\n",
      "mean: tensor(-0.8254)\n",
      "iter_dt 6414.24ms; iter 90: train loss 0.15089 temperature: 9.50000000000001\n",
      "mean_logits tensor([-0.8434, -0.9850, -0.9400, -0.9267, -0.8925], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.8698, -0.9358, -0.9252, -0.9216, -0.9279])\n",
      "mean: tensor(-0.9161)\n",
      "iter_dt 6403.70ms; iter 91: train loss 0.00595 temperature: 9.550000000000011\n",
      "mean_logits tensor([-1.0036, -0.9641, -0.9369, -0.9462, -0.9708], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9213, -0.9349, -0.9249, -0.9277, -0.8924])\n",
      "mean: tensor(-0.9202)\n",
      "iter_dt 6394.24ms; iter 92: train loss 0.01900 temperature: 9.600000000000012\n",
      "mean_logits tensor([-0.9604, -0.8855, -0.9026, -0.9353, -1.0403], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9419, -0.9398, -0.9245, -0.8530, -0.9447])\n",
      "mean: tensor(-0.9208)\n",
      "iter_dt 6351.91ms; iter 93: train loss 0.02613 temperature: 9.650000000000013\n",
      "mean_logits tensor([-0.9964, -0.9748, -0.9392, -0.9411, -1.0037], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9438, -0.9347, -0.9438, -0.9270, -0.9188])\n",
      "mean: tensor(-0.9336)\n",
      "iter_dt 6398.36ms; iter 94: train loss 0.01617 temperature: 9.700000000000014\n",
      "mean_logits tensor([-0.8342, -0.9312, -0.9250, -0.9717, -0.8667], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9371, -0.9479, -0.9210, -0.9277, -0.9129])\n",
      "mean: tensor(-0.9293)\n",
      "iter_dt 6406.88ms; iter 95: train loss 0.01796 temperature: 9.750000000000014\n",
      "mean_logits tensor([-0.8176, -0.9233, -0.8262, -0.9277, -0.8596], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9065, -0.9200, -0.9360, -0.9360, -0.8996])\n",
      "mean: tensor(-0.9196)\n",
      "iter_dt 6398.12ms; iter 96: train loss 0.02492 temperature: 9.800000000000015\n",
      "mean_logits tensor([-0.9555, -0.9981, -0.9073, -0.9440, -0.8842], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9478, -0.9453, -0.9316, -0.9257, -0.9377])\n",
      "mean: tensor(-0.9376)\n",
      "iter_dt 6393.69ms; iter 97: train loss 0.00870 temperature: 9.850000000000016\n",
      "mean_logits tensor([-0.9393, -0.9561, -0.8605, -0.9913, -0.9226], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9313, -0.9258, -0.8816, -0.9468, -0.9434])\n",
      "mean: tensor(-0.9258)\n",
      "iter_dt 6433.71ms; iter 98: train loss 0.00510 temperature: 9.900000000000016\n",
      "mean_logits tensor([-0.9010, -0.8715, -0.9409, -0.7560, -0.9102], grad_fn=<MeanBackward1>)\n",
      "energies: tensor([-0.9215, -0.9473, -0.9440, -0.9450, -0.9298])\n",
      "mean: tensor(-0.9375)\n",
      "iter_dt 6401.71ms; iter 99: train loss 0.04738 temperature: 9.950000000000017\n"
     ]
    }
   ],
   "source": [
    "for d in distances:\n",
    "    find_ground_state_energy(d, seed, ignore_cache=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T18:06:51.808544Z",
     "start_time": "2023-09-17T16:28:19.095730Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Figure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -0.90436139416354\n",
      "converged SCF energy = -0.936718704219097\n",
      "converged SCF energy = -0.964945015121394\n",
      "converged SCF energy = -0.989550091635695\n",
      "converged SCF energy = -1.01096729731079\n",
      "converged SCF energy = -1.02956727247755\n",
      "converged SCF energy = -1.04566888041386\n",
      "converged SCF energy = -1.05954801683135\n",
      "converged SCF energy = -1.07144473111346\n",
      "converged SCF energy = -1.08156900253999\n",
      "converged SCF energy = -1.09010543818623\n",
      "converged SCF energy = -1.09721710258265\n",
      "converged SCF energy = -1.10304864664113\n",
      "converged SCF energy = -1.10772887072515\n",
      "converged SCF energy = -1.11137283126101\n",
      "converged SCF energy = -1.11408358001426\n",
      "converged SCF energy = -1.11595360874732\n",
      "converged SCF energy = -1.1170660584989\n",
      "converged SCF energy = -1.11749574153842\n",
      "converged SCF energy = -1.11731001469769\n",
      "converged SCF energy = -1.11656953494796\n",
      "converged SCF energy = -1.1153289215404\n",
      "converged SCF energy = -1.11363734358685\n",
      "converged SCF energy = -1.11153904748242\n",
      "converged SCF energy = -1.10907383494504\n",
      "converged SCF energy = -1.10627749955942\n",
      "converged SCF energy = -1.10318222747019\n",
      "converged SCF energy = -1.09981696617325\n",
      "converged SCF energy = -1.09620776412366\n",
      "converged SCF energy = -1.09237808302647\n",
      "converged SCF energy = -1.08834908413144\n",
      "converged SCF energy = -1.08413988954369\n",
      "converged SCF energy = -1.07976781942899\n",
      "converged SCF energy = -1.07524860598415\n",
      "converged SCF energy = -1.07059658511255\n",
      "converged SCF energy = -1.06582486686076\n",
      "converged SCF energy = -1.06094548580128\n",
      "converged SCF energy = -1.05596953267198\n",
      "converged SCF energy = -1.05090726868925\n",
      "converged SCF energy = -1.04576822402693\n",
      "converged SCF energy = -1.04056128199716\n",
      "converged SCF energy = -1.0352947504774\n",
      "converged SCF energy = -1.02997642210328\n",
      "converged SCF energy = -1.02461362469239\n",
      "converged SCF energy = -1.01921326328575\n",
      "converged SCF energy = -1.0137818550948\n",
      "converged SCF energy = -1.00832555852993\n",
      "converged SCF energy = -1.00285019736543\n",
      "converged SCF energy = -0.997361280971559\n",
      "converged SCF energy = -0.991864021420451\n",
      "converged SCF energy = -0.986363348152976\n",
      "converged SCF energy = -0.980863920780542\n",
      "converged SCF energy = -0.975370140492084\n",
      "converged SCF energy = -0.969886160442651\n",
      "converged SCF energy = -0.964415895417479\n",
      "converged SCF energy = -0.958963030993083\n",
      "converged SCF energy = -0.953531032357414\n",
      "converged SCF energy = -0.948123152899673\n",
      "converged SCF energy = -0.942742442640776\n",
      "converged SCF energy = -0.937391756542829\n",
      "converged SCF energy = -0.932073762712097\n",
      "converged SCF energy = -0.926790950492223\n",
      "converged SCF energy = -0.921545638432415\n",
      "converged SCF energy = -0.916339982107811\n",
      "converged SCF energy = -0.91117598176558\n",
      "converged SCF energy = -0.906055489769334\n",
      "converged SCF energy = -0.900980217815608\n",
      "converged SCF energy = -0.895951743899412\n",
      "converged SCF energy = -0.890971519008912\n",
      "converged SCF energy = -0.886040873534479\n",
      "converged SCF energy = -0.881161023381738\n",
      "converged SCF energy = -0.876333075782618\n",
      "converged SCF energy = -0.871558034803417\n",
      "converged SCF energy = -0.866836806552646\n",
      "converged SCF energy = -0.862170204095175\n",
      "converged SCF energy = -0.857558952082343\n",
      "converged SCF energy = -0.853003691110415\n",
      "converged SCF energy = -0.848504981821767\n",
      "converged SCF energy = -0.844063308764772\n",
      "converged SCF energy = -0.839679084029517\n",
      "converged SCF energy = -0.835352650676989\n",
      "converged SCF energy = -0.831084285979668\n",
      "converged SCF energy = -0.826874204491295\n",
      "converged SCF energy = -0.822722560963121\n",
      "converged SCF energy = -0.818629453123181\n",
      "converged SCF energy = -0.814594924334159\n",
      "converged SCF energy = -0.810618966144509\n",
      "converged SCF energy = -0.806701520745856\n",
      "converged SCF energy = -0.802842483348611\n",
      "converged SCF energy = -0.79904170448647\n",
      "converged SCF energy = -0.795298992258595\n",
      "converged SCF energy = -0.791614114517261\n",
      "converged SCF energy = -0.787986801007249\n",
      "converged SCF energy = -0.784416745461881\n",
      "converged SCF energy = -0.780903607659415\n",
      "converged SCF energy = -0.777447015442297\n",
      "converged SCF energy = -0.774046566700748\n",
      "converged SCF energy = -0.770701831321248\n",
      "converged SCF energy = -0.767412353099479\n",
      "converged SCF energy = -0.764177651616797\n"
     ]
    }
   ],
   "source": [
    "min_d = distances[0] - 0.1\n",
    "max_d = distances[len(distances) - 1] + 0.1\n",
    "n_bin = 100\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "ys3 = []\n",
    "initializer = HFStateInitializer(n_electrons=2)\n",
    "for j in range(n_bin):\n",
    "    d = min_d + (max_d - min_d) / (n_bin - 1) * j\n",
    "    molecule = generate_molecule(\"H\", \"H\", d, \"sto-3g\", bravyi_kitaev=False)\n",
    "    hamiltonian = DiatomicMolecularHamiltonian(nqubit, molecule, bravyi_kitaev=False)\n",
    "    ge = compute_ground_state(hamiltonian)\n",
    "    scf = hamiltonian.exact_value(initializer.init_circuit(4, [], \"qulacs\"))\n",
    "    xs.append(d)\n",
    "    ys.append(ge)\n",
    "    ys3.append(scf)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T18:40:42.849247Z",
     "start_time": "2023-09-25T18:40:29.048104Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs2 = []\n",
    "ys2 = []\n",
    "\n",
    "for d in distances:\n",
    "    xs2.append(d)\n",
    "    with open(OTHER_FILEBASE.format(model_type, d, transformation, seed)) as f:\n",
    "        ys2.append(json.loads(f.readline())['computed_energy'])\n",
    "\n",
    "import matplotlib.pyplot as p\n",
    "\n",
    "# p.grid('-')\n",
    "p.plot(xs, ys, label='exact', linewidth=1, color='blue')\n",
    "p.plot(xs2, ys2, label='computed', marker='x', linewidth=0, color='green')\n",
    "p.plot(xs, ys3, label='hf', linewidth=1, color='gray')\n",
    "p.xlabel('bond length (angstrom)')\n",
    "p.ylabel('energy value (Hartree)')\n",
    "p.title('GPT-QE result with H2 Hamiltonian (sto-3g basis)')\n",
    "p.legend()\n",
    "p.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T18:41:26.897788Z",
     "start_time": "2023-09-25T18:41:26.756952Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
